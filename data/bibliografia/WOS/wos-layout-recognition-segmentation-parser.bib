
@inproceedings{neudecker_making_2016,
	title = {Making {Europe}'s {Historical} {Newspapers} {Searchable}},
	isbn = {9781509017928 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979556437&doi=10.1109%2fDAS.2016.83&partnerID=40&md5=8f07aa6473046bc026f95a822340502c},
	doi = {10.1109/DAS.2016.83},
	abstract = {This paper provides a rare glimpse into the overall approach for the refinement, i.e. the enrichment of scanned historical newspapers with text and layout recognition, in the Europeana Newspapers project. Within three years, the project processed more than 10 million pages of historical newspapers from 12 national and major libraries to produce the largest open access and fully searchable text collection of digital historical newspapers in Europe. In this, a wide variety of legal, logistical, technical and other challenges were encountered. After introducing the background issues in newspaper digitization in Europe, the paper discusses the technical aspects of refinement in greater detail. It explains what decisions were taken in the design of the large-scale processing workflow to address these challenges, what were the results produced and what were identified as best practices. © 2016 IEEE.},
	language = {English},
	urldate = {2016-04-11},
	booktitle = {Proc. - {IAPR} {Int}. {Workshop} {Doc}. {Anal}. {Syst}., {DAS}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Neudecker, C. and Antonacopoulos, A.},
	year = {2016},
	note = {Journal Abbreviation: Proc. - IAPR Int. Workshop Doc. Anal. Syst., DAS},
	keywords = {Best practices, Character recognition, digital libraries, Digital libraries, Digitisation, Electronic publishing, europeana, HEMDIG - PROJETOS SIMILARES, HEMDIG - SCOPUS, HEMDIG - WOS, HEMDIG FRAMEWORK, historical newspapers, Historical newspapers, large-scale digitisation, Large-scale processing, layout analysis, Layout analysis, newspapers, Newsprint, OCR, OLR, Open Access, optical character recognition, Optical character recognition, SCOPUS, Technical aspects, Text collection},
	pages = {405--410},
	file = {neudecker_antonacopoulos_2016_making europe's historical newspapers searchable.pdf:/home/ebn/pCloudDrive/zot_library/IEEE/2016/neudecker_antonacopoulos_2016_making europe's historical newspapers searchable.pdf:application/pdf;Snapshot:/home/ebn/Zotero/storage/L8533WTD/7490152.html:text/html},
}

@article{watanabe_layout_1995,
	title = {Layout {Recognition} of {Multi}-{Kinds} of {Table}-{Form} {Documents}},
	volume = {17},
	issn = {01628828 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029293805&doi=10.1109%2f34.385976&partnerID=40&md5=257ade263f09b1c8421e776680bbb8bc},
	doi = {10.1109/34.385976},
	abstract = {Many approaches have reported that knowledge-based layout recognition methods are very successful to classify the meaningful data from document images automatically. However, these approaches are applicable to only the same kind of documents because they are based on the paradigm that specifies the structure definition information in advance so as to be able to analyze a particular class of documents intelligently. In this paper, we propose a method to recognize the layout structures of multi-kinds of table-form document images. For this purpose, we introduce a classification tree to manage the relationships among different classes of layout structures. Our recognition system has two modes: layout knowledge acquisition and layout structure recognition. In the layout knowledge acquisition mode, table-form document images are distinguished according to this classification tree and then the structure description trees which specify the logical structures of table-form documents are generated automatically. While, in the layout structure recognition mode, individual item fields in the table-form document images are extracted and classified successfully by searching the classification tree and interpreting the structure description tree. © 1995 IEEE},
	language = {English},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Watanabe, T. and Luo, Q. and Sugie, N.},
	year = {1995},
	keywords = {automatic acquisition of layout knowledge, AUTOMATIC ACQUISITION OF LAYOUT KNOWLEDGE, Automation, classification tree, Classification tree, CLASSIFICATION TREE, HEMDIG - SCOPUS, HEMDIG - WOS, Image understanding, Knowledge acquisition, Knowledge based systems, Layout knowledge, Layout recognition, Layout structures, Pattern recognition, recognition of document classes, RECOGNITION OF DOCUMENT CLASSES, recognition of layout structures, RECOGNITION OF LAYOUT STRUCTURES, Recognition paradigm for multi-kinds of table-form documents, RECOGNITION PARADIGM FOR MULTI-KINDS OF TABLE-FORM DOCUMENTS, SCOPUS, structure description tree, Structure description tree, STRUCTURE DESCRIPTION TREE, Table form documents, Trees (mathematics)},
	pages = {432--445},
}

@inproceedings{ratha_fpga-based_1996,
	address = {Los Alamitos, CA, United States},
	title = {{FPGA}-based high performance page layout segmentation},
	isbn = {10661395 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029720295&partnerID=40&md5=645f1050465b1f83193aa0c7aa4732c1},
	abstract = {A page layout segmentation algorithm for locating text, background and halftone areas is presented. The algorithm has been implemented on Splash 2 - an FPGA-based array processor. The speed as determined by the Xilinx synthesis tools projects an application speed of 5 MHz. For documents of size 1.024 × 1.024 pixels, a significant speedup of two orders of magnitude compared to a SparcStation 20 has been achieved.},
	language = {English},
	urldate = {1996-03-22},
	booktitle = {Proc {IEEE} {Great} {Lakes} {Symp} {VLSI}},
	publisher = {IEEE},
	author = {Ratha, N.K. and Jain, A.K. and Rover, D.T.},
	collaborator = {{IEEE}},
	year = {1996},
	note = {Journal Abbreviation: Proc IEEE Great Lakes Symp VLSI},
	keywords = {Algorithms, Character recognition, Codes (symbols), Computer graphics, Design, HEMDIG - SCOPUS, HEMDIG - WOS, Image segmentation, Page layout segmentation, Program processors, SCOPUS},
	pages = {29--34},
}

@article{watanabe_multilayer_1996,
	title = {A multilayer recognition method for understanding table-form documents},
	volume = {7},
	issn = {08999457 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030392123&doi=10.1002%2f%28SICI%291098-1098%28199624%297%3a4%3c279%3a%3aAID-IMA3%3e3.0.CO%3b2-5&partnerID=40&md5=8b89af04d86c1fd0653fb2258cc000cc},
	doi = {10.1002/(SICI)1098-1098(199624)7:4<279::AID-IMA3>3.0.CO;2-5},
	abstract = {The items in documents are composed under the mutual relationship between geometrical layout structure and logical structure. Thus, currently used documents may be categorized roughly on the basis of the geometrical layout structure and logical structure. Table-form documents are strictly defined by the geometrical layout structure in comparison with the other documents. Since the item fields are, in general, surrounded with vertical and horizontal line segments, with a view to understanding table-form documents it is better to recognize these line segments first and then identify individual item fields on the basis of the recognition results. At least, in table-form documents the vertical and horizontal line segments take important roles in recognizing the layout structures. In this article, we address a multilayer recognition method for understanding table-form documents. Our recognition layers are composed hierarchically of four different recognition processes: document class recognition, layout recognition, item recognition, and character recognition. In addition, our recognition method is organized on the basis of the interaction paradigm that the lower-layer recognition process verifies objects interpreted in the upper-layer recognition process with its own knowledge and then decomposes the objects into more elementary and meaningful objects. Moreover, we discuss a knowledge representation method from the viewpoints of physical and logical representations, and syntactic and semantic information. We also show the knowledge that is useful in understanding table-form documents with respect to such a representation method. © 1996 John Wiley \& Sons, inc.},
	language = {English},
	number = {4},
	journal = {International Journal of Imaging Systems and Technology},
	author = {Watanabe, T. and Luo, Q.},
	year = {1996},
	note = {Publisher: John Wiley and Sons Inc.},
	keywords = {Character recognition, Data processing, Document class recognition, HEMDIG - SCOPUS, HEMDIG - WOS, Image understanding, Item recognition, Layout recognition, Multilayer recognition methods, Object recognition, Pattern recognition systems, SCOPUS, Table form documents},
	pages = {279--288},
}

@inproceedings{tian_layout_2002,
	address = {Beijing},
	title = {The layout recognition and reconstruction of {Chinese} documents based on gabor filter},
	volume = {4},
	isbn = {0780375084 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036929026&partnerID=40&md5=a6e02bfbe938e2dc41dca3235434b2e6},
	abstract = {The layout recognition of Chinese documents is one of the most difficult pattern recognition problems, because it concerns a great number of classes and many variations. In this paper, an intelligent layout recognition and reconstruction system is put forward on the basis of the analysis of the present ways. The algorithms of the pivotal processing steps such as layout analysis and character font recognition are discussed. Especially, a 2-D Gabor filter is used in character font recognition to improve the adaptability to the Chinese document. Experiments have been made and some promising results have been drawn.},
	language = {English},
	urldate = {2002-11-04},
	booktitle = {Proc. of 2002 {Internat}. {Conf}. on {Machine} {Learning} and {Cybernetics}},
	author = {Tian, X.-D. and Guo, B.-L.},
	collaborator = {Hebei University; IEEE systems, Man {and} Cybernetics technical Comm. on Cybernetics},
	year = {2002},
	note = {Journal Abbreviation: Proc. of 2002 Internat. Conf. on Machine Learning and Cybernetics},
	keywords = {Algorithms, Artificial intelligence, Character font recognition, Character recognition, Chinese documents, Digital filters, digitalization of document, Digitalization of document, File organization, font recognition, Font recognition, Gabor filter, HEMDIG - SCOPUS, HEMDIG - WOS, IDENTIFICATION, Image compression, Image reconstruction, layout analysis, Layout analysis, layout recognition, Layout recognition, SCOPUS, Two dimensional},
	pages = {1707--1710},
}

@article{diwadkar_viewpoint_1997,
	title = {Viewpoint dependence in scene recognition},
	volume = {8},
	issn = {09567976 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347351067&doi=10.1111%2fj.1467-9280.1997.tb00442.x&partnerID=40&md5=f5c905d8dafacbe15c3f36c641c5214f},
	doi = {10.1111/j.1467-9280.1997.tb00442.x},
	abstract = {Two experiments investigated the viewpoint dependence of spatial memories. In Experiment 1, participants learned the locations of objects on a desktop from a single perspective and then took part in a recognition test; test scenes included familiar and novel views of the layout. Recognition latency was a linear function of the angular distance between a test view and the study view. In Experiment 2, participants studied a layout from a single view and then learned to recognize the layout from three additional training views. A final recognition test showed that the study view and the training views were represented in memory, and that latency was a linear function of the angular distance to the nearest study or training view. These results indicate that interobject spatial relations are encoded in a viewpoint-dependent manner, and that recognition of novel views requires normalization to the most similar representation in memory. These findings parallel recent results in visual object recognition.},
	language = {English},
	number = {4},
	journal = {Psychological Science},
	author = {Diwadkar, V.A. and McNamara, T.P.},
	year = {1997},
	note = {Publisher: Blackwell Publishing Ltd},
	keywords = {HEMDIG - SCOPUS, HEMDIG - WOS, MEMORY, MENTAL ROTATION, OBJECT RECOGNITION, REPRESENTATION, ROTATED OBJECTS, SCOPUS, VIEWS},
	pages = {302--307},
}

@inproceedings{mighlani_intelligent_1997,
	address = {Piscataway, NJ, United States},
	title = {Intelligent hierarchical layout segmentation of document images on the basis of colour content},
	volume = {1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031345878&partnerID=40&md5=7c771103f731fb57dda1fbbb72f158e9},
	abstract = {This paper proposes a general methodology for automatic layout segmentation of documents. We first use colour histograms for extracting dominant colours of an image. This information is then used to hierarchically segment documents into regions of interest represented as polygons. If a region of interest is a picture the algorithm intelligently refrains from segmenting it further, while coloured regions that contain text are sub-segmented. The method has been tested on 50 real life documents, such as office letters, brochures, and technical papers, scanned at 100×100 dpi resolution. Regions are detected with about 68\% reliability. A critical analysis of the results is presented.},
	language = {English},
	urldate = {1997-12-02},
	booktitle = {{IEEE} {Reg} 10 {Annu} {Int} {Conf} {Proc} {TENCON}},
	publisher = {IEEE},
	author = {Mighlani, D. and Hennig, A. and Sherkat, N. and Whitrow, R.J.},
	editor = {{Deriche M.} and {Moody M.} and {Bennamoun M.}},
	collaborator = {{IEEE}},
	year = {1997},
	note = {Journal Abbreviation: IEEE Reg 10 Annu Int Conf Proc TENCON},
	keywords = {Adaptive algorithms, Automatic layout segmentation, Color image processing, Feature extraction, HEMDIG - SCOPUS, HEMDIG - WOS, Image segmentation, Intelligent hierarchical layout segmentation, SCOPUS},
	pages = {191--194},
}

@inproceedings{watanabe_guideline_1999,
	address = {Bellingham},
	title = {Guideline for specifying layout knowledge},
	volume = {3651},
	isbn = {0277786X (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032629301&doi=10.1117%2f12.335813&partnerID=40&md5=dc21383b9e23e0c596cf3f5573201fa4},
	doi = {10.1117/12.335813},
	abstract = {Until today, many layout recognition/analysis methods have been proposed, but the guideline for knowledge representation applicable to documents which should be analyzed newly is not always discussed directly. This paper addresses such a subject. Generally, the documents can be categorized into the appropriate document types on the basis of the features of layout structures. Then, the processing mechanisms are assessed with a view to establishing the criteria for selecting the knowledge representation means appropriate to the document types. First, we define the physical layout structure and logical layout structure in addition to the traditional concepts of layout structure and logical structure. Second, we define the document type on the basis of the relationship between the physical layout structure and logical layout structure. Third, we make the knowledge representation means and the processing mechanisms clear under the document types. Finally, we show a criterion or guideline for knowledge representation means and processing mechanisms with respect to the logical layout structure and physical layout structure. For this discussion, our basic view is derived from our document understanding methods which we have developed for several different documents.},
	language = {English},
	urldate = {1999-01-27},
	booktitle = {Proc {SPIE} {Int} {Soc} {Opt} {Eng}},
	publisher = {Society of Photo-Optical Instrumentation Engineers},
	author = {Watanabe, Toyohide},
	collaborator = {{IS and T; SPIE}},
	year = {1999},
	note = {Journal Abbreviation: Proc SPIE Int Soc Opt Eng},
	keywords = {bottom-up, Data structures, document class, document model, document type, HEMDIG - SCOPUS, HEMDIG - WOS, Image analysis, Knowledge representation, Layout knowledge representation, Layout structure, logical layout structure, Mathematical models, model-driven, Optical character recognition, physical layout structure, RECOGNITION, representation of layout knowledge, SCOPUS, TABLE-FORM DOCUMENTS, top-down},
	pages = {162--172},
}

@article{watanabe_document_1999,
	title = {Document analysis and recognition},
	volume = {E82-D},
	issn = {09168532 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033350174&partnerID=40&md5=0fe0408d0f13fb6fea99c186ea7c00c2},
	abstract = {SUMMARY The subject about document image understanding is to extract and classify individual data meaningfully from paper-based documents. Until today, many methods/approaches have been proposed with regard to recognition of various kinds of documents, various technical problems for extensions of OCR, and requirements for practical usages. Of course, though the technical research issues in the early stage are looked upon as complementary attacks for the traditional OCR which is dependent on character recognition techniques, the application ranges or related issues are widely investigated or should be established progressively. This paper addresses current topics about document image understanding from a technical point of view as a survey.},
	language = {English},
	number = {3},
	journal = {IEICE Transactions on Information and Systems},
	author = {Watanabe, T.},
	year = {1999},
	note = {Publisher: Institute of Electronics, Information and Communication, Engineers, IEICE},
	keywords = {bottom-up, Bottom-up, Character recognition techniques, Data structures, Database systems, Document image understanding, document model, Document model, document types, Document types, Feature extraction, HEMDIG - SCOPUS, HEMDIG - WOS, Image analysis, Image understanding, Knowledge representation, layout recognition, Layout recognition, layout structure, Layout structure, logical structure, Logical structure, Optical character recognition, Paper based documents, SCOPUS, TABLE-FORM DOCUMENTS, top-down, Top-down},
	pages = {601--610},
}

@inproceedings{minguillon_progressive_1999,
	address = {Bellingham, WA, United States},
	title = {Progressive classification scheme for document layout recognition},
	volume = {3816},
	isbn = {0277786X (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033359405&partnerID=40&md5=094c3898ca3859bac8af0866375ef8ed},
	abstract = {In this paper we present a progressive classification scheme for a document layout recognition system (background, text and images) using three stages. The first stage, preprocessing, extracts statistical information that may be used for background detection and removal. The second stage, a tree based classifier, uses a variable block size and a set of probabilistic rules to classify segmented blocks. When a block cannot be classified at a given block size because contains more than one class, it is split in four sub-blocks that are independently classified. The third stage, postprocessing, uses the label map generated in the second stage with a set of context rules to label unclassified blocks, trying also to solve some of the misclassification errors that may have been generated during the previous stage. The progressive scheme used in the second and third stages allows the user to stop the classification process at any block size, depending on his requirements. Experiments show that a progressive scheme combined with a set of postprocessing rules increases the percentage of correctly classified blocks and reduces the number of block computations.},
	language = {English},
	urldate = {1999-07-21},
	booktitle = {Proc {SPIE} {Int} {Soc} {Opt} {Eng}},
	publisher = {Society of Photo-Optical Instrumentation Engineers},
	author = {Minguillon, Julia and Pujol, Jaume and Zeger, Kenneth},
	collaborator = {{SPIE}},
	year = {1999},
	note = {Journal Abbreviation: Proc SPIE Int Soc Opt Eng},
	keywords = {COMPRESSION, document analysis, Document analysis, Document layout recognition, HEMDIG - SCOPUS, HEMDIG - WOS, Image coding, Image compression, Image quality, Information analysis, Information retrieval systems, Pattern recognition systems, Probability, Progressive classification tree, progressive classification trees, SCOPUS, wavelet packets, Wavelet packets},
	pages = {241--251},
}

@inproceedings{cinque_system_2003,
	address = {Mantova},
	title = {A system for the automatic layout segmentation and classification of digital documents},
	isbn = {0769519482 (ISBN); 9780769519487 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649152439&doi=10.1109%2fICIAP.2003.1234050&partnerID=40&md5=d82804346f8d2452921f12efafdabc82},
	doi = {10.1109/ICIAP.2003.1234050},
	abstract = {Paper document recognition is fundamental for office automation becoming every day a more powerful tool in those fields where information is still on paper. Document recognition follows from data acquisition, from both journals and entire books, in order to transform them into digital objects. We present a new system for document recognition that follows the open source methodologies, XML description for document segmentation and classification, which turns out to be beneficial in terms of classification precision, and general-purpose availability. © 2003 IEEE.},
	language = {English},
	urldate = {2003-09-17},
	booktitle = {Proc. - {Int}. {Conf}. {Image} {Anal}. {Process}., {ICIAP}},
	author = {Cinque, L. and Levialdi, S. and Malizia, A.},
	year = {2003},
	note = {Journal Abbreviation: Proc. - Int. Conf. Image Anal. Process., ICIAP},
	keywords = {Automatic layout, Classification precision, Digital Documents, Digital Objects, document analysis, Document analysis, Document Analysis, Document recognition, Document segmentation, HEMDIG - SCOPUS, HEMDIG - WOS, Image analysis, image processing and segmentation, Image processing and segmentation, Information retrieval systems, Office automation, Open systems, Paper documents, pattern recognition, Pattern recognition, Pattern Recognition, SCOPUS},
	pages = {201--206},
}

@inproceedings{takiguchi_fundamental_2005,
	address = {Seoul},
	title = {A fundamental study of output translation from layout recognition and semantic understanding system for mathematical formulae},
	volume = {2005},
	isbn = {15205363 (ISSN); 0769524206 (ISBN); 9780769524207 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947360285&doi=10.1109%2fICDAR.2005.10&partnerID=40&md5=b469479340d8a26a56f00a6bd8986866},
	doi = {10.1109/ICDAR.2005.10},
	abstract = {In this paper we propose an implementation method for an off-line layout recognition and semantic understanding system for mathematical formulae. This off-line system aims at higher order coding of mathematical formulae in scientific articles as an application in document analysis. The system has two intermediate output codes: a layout tree holding information of geometrical structure of the formula and character recognized code of the symbols, and a semantic tree holding information of semantics of symbols. From the structure tree and the semantic tree after layout recognition and semantic understanding, various useful output can be generated at the translating part. This paper mainly describes implementation techniques for LATEX source output for high quality typesetting and gnuplot script output for drawing a function as a method for visual representation. © 2005 IEEE.},
	language = {English},
	urldate = {2005-08-31},
	booktitle = {Proc. {Int}. {Conf}. {Doc}. {Anal}. {Recognit}.},
	author = {Takiguchi, Y. and Okada, M. and Miyake, Y.},
	collaborator = {{TC10 (Graph. Recog.) and TC11 (Read. Syst.) of the (IAPR)}},
	year = {2005},
	note = {Journal Abbreviation: Proc. Int. Conf. Doc. Anal. Recognit.},
	keywords = {CHARACTER-RECOGNITION, Computational geometry, Encoding (symbols), Geometrical structure, HEMDIG - SCOPUS, HEMDIG - WOS, Layout recognition, Mathematical formulae, Pattern recognition, SCOPUS, Semantics, Translation (languages), Visual representation},
	pages = {745--749},
}

@inproceedings{esposito_intelligent_2005,
	address = {Seoul},
	title = {Intelligent document processing},
	volume = {2005},
	isbn = {15205363 (ISSN); 0769524206 (ISBN); 9780769524207 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947413581&doi=10.1109%2fICDAR.2005.144&partnerID=40&md5=2593cd8304f09ed0c5cc62c147785ca2},
	doi = {10.1109/ICDAR.2005.144},
	abstract = {Digital repositories raise the need for an effective and efficient retrieval of the stored material. In this paper we propose the intensive application of intelligent techniques to the steps of document layout analysis, document image classification and understanding on digital documents. Specifically, the complex interrelation existing among layout components, that are fundamental to assign them the proper semantic role, suggest the exploitation of first-order representations in some learning steps. Results obtained in a prototypical system for scientific conference management prove that the proposed approach can be beneficial both for the layout recognition and for the selection of interesting components of the document, from which extracting the text for categorizing the document according to its topic. © 2005 IEEE.},
	language = {English},
	urldate = {2005-08-31},
	booktitle = {Proc. {Int}. {Conf}. {Doc}. {Anal}. {Recognit}.},
	author = {Esposito, F. and Ferilli, S. and Basile, T.M.A. and Di Mauro, N.},
	collaborator = {{TC10 (Graph. Recog.) and TC11 (Read. Syst.) of the (IAPR)}},
	year = {2005},
	note = {Journal Abbreviation: Proc. Int. Conf. Doc. Anal. Recognit.},
	keywords = {Classification (of information), Data processing, Digital documents, HEMDIG - SCOPUS, HEMDIG - WOS, Information management, Information retrieval, Intelligent agents, Intelligent document processing, Layout recognition, SCOPUS, Semantics},
	pages = {1100--1104},
}

@inproceedings{gaceb_application_2008,
	title = {Application of graph coloring in physical layout segmentation},
	isbn = {10514651 (ISSN); 9781424421756 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957964928&doi=10.1109%2ficpr.2008.4761641&partnerID=40&md5=f0a1f0ef9c1d5ee90ffb19cf8787f87a},
	doi = {10.1109/icpr.2008.4761641},
	abstract = {Every-day, the postal sorting systems diffuse several tons of mails. It is noted that the principal origin of mail rejection is related to the failure of address-block localization task, particularly, of the physical layout segmentation stage. The bottom-up and top-down segmentation methods bring different knowledge that should not be ignored when we need to increase the robustness. Hybrid methods combine the two strategies in order to take advantages of one strategy to the detriment of other. Starting from these remarks, our proposal makes use of a hybrid segmentation strategy more adapted to the postal mails. The high level stages are based on the hierarchical graphs coloring. Today, no other work in this context has make use of the powerfulness of this tool. The performance evaluation of our approach was tested on a corpus of 10000 envelope images. The processing times and the rejection rate were considerably reduced. © 2008 IEEE.},
	language = {English},
	booktitle = {Proc. {Int}. {Conf}. {Pattern} {Recognit}.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Gaceb, D. and Eglin, V. and Lebourgeois, F. and Emptoz, H.},
	year = {2008},
	note = {Journal Abbreviation: Proc. Int. Conf. Pattern Recognit.},
	keywords = {Bottom-up and top-down, Graph colorings, HEMDIG - SCOPUS, HEMDIG - WOS, Hierarchical graphs, Hybrid segmentation, Pattern recognition, Physical layout, Processing time, Rejection rates, SCOPUS, Segmentation methods, Software engineering},
}

@article{kyriacou_vision-based_2005,
	title = {Vision-based urban navigation procedures for verbally instructed robots},
	volume = {51},
	issn = {09218890 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-22544440593&doi=10.1016%2fj.robot.2004.08.011&partnerID=40&md5=48971da18919c7c3deb996cf3ad03fbc},
	doi = {10.1016/j.robot.2004.08.011},
	abstract = {When humans explain a task to be executed by a robot they decompose it into chunks of actions. These form a chain of search-and-act sensory-motor loops that exit when a condition is met. In this paper we investigate the nature of these chunks in an urban visual navigation context, and propose a method for implementing the corresponding robot primitives such as "take the nth turn right/left". These primitives make use of a "short-lived" internal map updated as the robot moves along. The recognition and localisation of intersections is done in the map using task-guided template matching. This approach takes advantage of the content of human instructions to save computation time and improve robustness. © 2004 Elsevier B.V. All rights reserved.},
	language = {English},
	number = {1},
	journal = {Robotics and Autonomous Systems},
	author = {Kyriacou, T. and Bugmann, G. and Lauria, S.},
	year = {2005},
	keywords = {Computer vision, HEMDIG - SCOPUS, HEMDIG - WOS, Mobile robots, Natural language processing systems, NATURAL-LANGUAGE, Navigation, Pattern recognition, road layout recognition, Road layout recognition, Robot learning, robot primitives, Robot primitives, Robotics, route instructions, Route instructions, SCOPUS, SYSTEM, template matching, Template matching, urban navigation, Urban navigation, Urban planning},
	pages = {69--80},
}

@inproceedings{zhang_mask_2005,
	address = {San Jose, CA},
	title = {Mask cost analysis via write time estimation},
	volume = {5756},
	isbn = {0277786X (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-25144436352&doi=10.1117%2f12.598884&partnerID=40&md5=aa6606ca7f7be6a0869bd43f38cd27cc},
	doi = {10.1117/12.598884},
	abstract = {Long write times have been an industry wide concern regarding rising mask costs. The purpose of this study is to develop a simple model that can predict mask write time precisely, without an e-beam writer. With a good understanding of the trade-offs between design complexity and write time, mask makers can work with mask designers more closely to simplify design and minimize mask cost. This work compared several basic models including calculations based on write area with a fixed e-beam shot size, a software estimation with a pre-set exposure, and a mask stage settling time. Our proposed model uses a completely different approach to examine the correlation between layout complexity (vertices count, total line edge, figure, etc.) through a CATS layout segmentation and actual write time. It is found that write time is a strong function of layout figure, vertex count and total line edge. Errors between actual write time and estimated write time from the new model reduced from 7\% on average on the current production software to 3\%. Additionally, the new model can operate independent of the writer type and without fractured data being transferred onto a writer. Also provided are a few case studies to evaluate the interaction between write time and basic shape/OPC (optical proximity correction). Using a simple design shape and a better data snapping strategy can reduce write time up to 10 fold for applications in nano-imprint template manufacturing. Several strategies to reduce mask cost are proposed.},
	language = {English},
	urldate = {2005-03-03},
	booktitle = {Proc {SPIE} {Int} {Soc} {Opt} {Eng}},
	author = {Zhang, Y. and Gray, R. and Chou, S. and Rockwell, B. and Xiao, G. and Kamberian, H. and Cottle, R. and Wolleben, A. and Progler, C.},
	editor = {{Liebmann L.W.}},
	collaborator = {{SPIE}},
	year = {2005},
	note = {Journal Abbreviation: Proc SPIE Int Soc Opt Eng},
	keywords = {CATS, Cost benefit analysis, Data acquisition, design optimization, Design optimization, HEMDIG - SCOPUS, HEMDIG - WOS, Image segmentation, mask write time, Mask write time, OPC, Optical correlation, Optical design, Optical engineering, Optical systems, SCOPUS, vertex, Vertex},
	pages = {313--318},
}

@article{mukherjee_statistical_2005,
	title = {Statistical analysis and diagnosis methodology for {RF} circuits in {LCP} substrates},
	volume = {53},
	issn = {00189480 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-28144451013&doi=10.1109%2fTMTT.2005.855735&partnerID=40&md5=5c3447f342ee566ce6fbf235698cee71},
	doi = {10.1109/TMTT.2005.855735},
	abstract = {This paper presents the application of a fast and accurate layout-level statistical analysis methodology for the diagnosis of RF circuit layouts with embedded passives in liquid crystalline polymer substrates. The approach is based on layout-segmentation, lumped-element modeling, sensitivity analysis, and extraction of probability density function using convolution methods. The statistical analyses were utilized as a diagnosis tool to estimate distributed design parameter variations and yield of RF circuit layouts for a given measured performance. The results of statistical analysis and diagnosis were compared with measurement results of fabricated filters. Statistical methods were also applied for design space exploration to improve system performance, as well as estimation of yield and diagnosis of faults during batch fabrication. © 2005 IEEE.},
	language = {English},
	number = {11},
	journal = {IEEE Transactions on Microwave Theory and Techniques},
	author = {Mukherjee, S. and Swaminathan, M. and Matoglu, E.},
	year = {2005},
	keywords = {bandpass filter, Bandpass filter, Bandpass filters, Frequency converter circuits, HEMDIG - SCOPUS, HEMDIG - WOS, Integrated circuit layout, Liquid crystal polymers, liquid crystalline polymer (LCP), Liquid crystalline polymer (LCP), Lumped-element modeling, Mathematical models, parametric yield, Parametric yield, Passive networks, RF synthesis, SCOPUS, statistical diagnosis, Statistical diagnosis, Statistical methods, Substrates},
	pages = {3621--3630},
}

@inproceedings{erol_smart_2005,
	address = {Amsterdam},
	title = {Smart handouts: {Personalized} e-presentation documents},
	volume = {2005},
	isbn = {0780393325 (ISBN); 9780780393325 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750549032&doi=10.1109%2fICME.2005.1521451&partnerID=40&md5=b7e1dd287b1d1186e944f524e731b581},
	doi = {10.1109/ICME.2005.1521451},
	abstract = {A novel system is described that significantly enhances the usefulness of handwritten notes taken during a presentation by creating a multimedia document that includes scanned images of handouts, personal notes, and links to a multimedia recording of the presentation. Notes are linked to the e-presentation media with automatic content analysis without any special notes capture device. Layout segmentation and template matching automatically detects the presence of presentation handouts during scanning. Presentation-level and slide-level linking of handouts to e-media use text and image features from slides. Experimental results show 95\% accuracy in linking of the scanned handouts to the e-presentation media. © 2005 IEEE.},
	language = {English},
	urldate = {2005-07-06},
	booktitle = {{IEEE} {Int}. {Conf}. {Multimedia} {Expo}},
	author = {Erol, B. and Lee, D.-S. and Hull, J.J.},
	year = {2005},
	note = {Journal Abbreviation: IEEE Int. Conf. Multimedia Expo},
	keywords = {Automatic content analysis, Electronic document exchange, Feature extraction, Handouts, HEMDIG - SCOPUS, HEMDIG - WOS, Image recording, Multimedia documents, Multimedia recording, Multimedia systems, Pattern matching, SCOPUS, Telecommunication links, Text processing},
	pages = {426--429},
}

@inproceedings{takiguchi_study_2006,
	address = {Hong Kong},
	title = {A study on character recognition error correction at higher level recognition step for mathematical formulae understanding},
	volume = {2},
	isbn = {10514651 (ISSN); 0769525210 (ISBN); 9780769525211 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047223470&doi=10.1109%2fICPR.2006.180&partnerID=40&md5=df4aafba3e70262bdd526b57f01ccf13},
	doi = {10.1109/ICPR.2006.180},
	abstract = {In this paper we propose a method for correcting character recognition errors at the higher level recognition step in an understanding system for mathematical formulae. The system consists of two-level recognition steps: the low level recognition including character recognition, and the higher level recognition including layout recognition. We use the layout information recognized in the latter step to correct the character recognition errors by using two sources of information. One is based on some keywords such as mathematical function names, and the other is based on a cost tree and co-occurrence probabilities between symbols. The efficacy of the proposed method is verified by some experimental results, and the character recognition rate increased from 80.2\% to 89.2\%. © 2006 IEEE.},
	language = {English},
	urldate = {2006-08-20},
	booktitle = {Proc. {Int}. {Conf}. {Pattern} {Recognit}.},
	author = {Takiguchi, Y. and Okada, M. and Miyake, Y.},
	year = {2006},
	note = {Journal Abbreviation: Proc. Int. Conf. Pattern Recognit.},
	keywords = {Character recognition, Character recognition errors, Computational efficiency, Error correction, Functions, HEMDIG - SCOPUS, HEMDIG - WOS, Information analysis, Keywords, Mathematical formulae, Probability, SCOPUS, Trees (mathematics)},
	pages = {966--969},
}

@inproceedings{gaceb_physical_2008,
	address = {Nara},
	title = {Physical layout segmentation of mail application dedicated to automatic postal sorting system},
	isbn = {9780769533377 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649210479&doi=10.1109%2fDAS.2008.56&partnerID=40&md5=18ee74d82acb5e9d7ca1b5cf2d05698d},
	doi = {10.1109/DAS.2008.56},
	abstract = {Every-day, the postal sorting systems diffuse several tons of mails. It is noted that the principal origin of mail rejection is related to the failure of address-block localization task, particularly, of the physical layout segmentation stage. The bottom-up and top-down segmentation methods bring different knowledge that should not be ignored when we need to increase the robustness. Hybrid methods combine the two strategies in order to take advantages of one strategy to the detriment of other. Starting from these remarks, our proposal makes use of a hybrid segmentation strategy more adapted to the postal malls. The high level stages are based on the hierarchical graphs coloring, allowing managing through a pyramidal data organization, the complex rules leading the interpretation of the connected components decomposition of interest zones. Today, no other work in this context has make use of the powerfulness of this tool. The performance evaluation of our approach was tested on a corpus of 10000 envelope Images. The processing times and the rejection rate were considerably reduced. © 2008 IEEE.},
	language = {English},
	urldate = {2008-09-16},
	booktitle = {{DAS} 2008 - {Proc}. {IAPR} {Int}. {Workshop} {Document} {Anal}. {Syst}.},
	author = {Gaceb, D. and Eglin, V. and Lebourgeois, F. and Emptoz, H.},
	collaborator = {Osaka Prefecture University; International Association for Pattern Recognition, Ltd.; Hitachi Computer Peripherals Co., Ltd., IAPR; Japan Society for the Promotion of Science; International Information Science Foundation; Hitachi},
	year = {2008},
	note = {Journal Abbreviation: DAS 2008 - Proc. IAPR Int. Workshop Document Anal. Syst.},
	keywords = {Bottom-up, Connected components, Data organizations, Electronic warfare, HEMDIG - SCOPUS, HEMDIG - WOS, Hierarchical graphs, Hybrid methods, Hybrid segmentations, Intelligent vehicle highway systems, Military electronic countermeasures, Performance evaluations, Processing times, Rejection rates, SCOPUS, Segmentation methods, Sorting, Sorting systems, Technical presentations, Top downs},
	pages = {408--414},
}

@article{leydier_towards_2009,
	title = {Towards an omnilingual word retrieval system for ancient manuscripts},
	volume = {42},
	issn = {00313203 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-67349245536&doi=10.1016%2fj.patcog.2009.01.026&partnerID=40&md5=3c60d67964c4e975c186bec41cf8500e},
	doi = {10.1016/j.patcog.2009.01.026},
	abstract = {In this article, we introduce the first method that allows the indexation of ancient manuscripts of any language and alphabet. We describe a word retrieval engine inspired by recent word-spotting advances on ancient manuscripts. Our approach does not need any layout segmentation and makes use of features fitted to any type of alphabet (Latin, Arabic, Chinese, etc.) and writing. The engine is tested on numerous documents and in several use-cases. © 2009 Elsevier Ltd. All rights reserved.},
	language = {English},
	number = {9},
	journal = {Pattern Recognition},
	author = {Leydier, Y. and Ouji, A. and LeBourgeois, F. and Emptoz, H.},
	year = {2009},
	note = {Publisher: Elsevier Ltd},
	keywords = {Ancient documents, Data mining, Document indexing, Engines, GRADIENT, HEMDIG - SCOPUS, HEMDIG - WOS, Information retrieval, Omnilingual, SCOPUS, Segmentation-free, Word retrieval, Word Spotting, Word-spotting},
	pages = {2089--2105},
}

@inproceedings{ouji_advertisement_2011,
	address = {Barcelona},
	title = {Advertisement detection in digitized press images},
	isbn = {19457871 (ISSN); 9781612843490 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155136740&doi=10.1109%2fICME.2011.6011890&partnerID=40&md5=7c6ceb0cc50fa7a46e01aaf31af00725},
	doi = {10.1109/ICME.2011.6011890},
	abstract = {This paper presents the first method for detecting advertisements in digitized press. The system aims at locating and recognizing ads. A color segmentation approach which is robust against digitization noise is introduced. The color separation output is used to carry out layout segmentation in document pages and to compute visual features. Block classification results, given with a variety of magazine and newspaper pages, are presented and discussed. © 2011 IEEE.},
	language = {English},
	urldate = {2011-07-11},
	booktitle = {Proc. - {IEEE} {Int}. {Conf}. {Multimedia} {Expo}},
	publisher = {IEEE Computer Society},
	author = {Ouji, A. and Leydier, Y. and Lebourgeois, F.},
	collaborator = {{IEEE Circuit and System Society; IEEE Communication Society; IEEE Computer Society; IEEE Signal Processing Society}},
	year = {2011},
	note = {Journal Abbreviation: Proc. - IEEE Int. Conf. Multimedia Expo},
	keywords = {Advertisement detection, Advertisement detections, Block classification, Color segmentation, Color separation, document image, Document images, HEMDIG - SCOPUS, HEMDIG - WOS, Image segmentation, Presses (machine tools), SCOPUS, segmentation, SEGMENTATION, Visual feature},
}

@inproceedings{mehri_old_2013,
	address = {Burlingame, CA},
	title = {Old document image segmentation using the autocorrelation function and multiresolution analysis},
	volume = {8658},
	isbn = {0277786X (ISSN); 9780819494313 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875824773&doi=10.1117%2f12.2002365&partnerID=40&md5=345ff43ec8ee0d94a6fb96535dcc3db3},
	doi = {10.1117/12.2002365},
	abstract = {Recent progress in the digitization of heterogeneous collections of ancient documents has rekindled new challenges in information retrieval in digital libraries and document layout analysis. Therefore, in order to control the quality of historical document image digitization and to meet the need of a characterization of their content using intermediate level metadata (between image and document structure), we propose a fast automatic layout segmentation of old document images based on five descriptors. Those descriptors, based on the autocorrelation function, are obtained by multiresolution analysis and used afterwards in a specific clustering method. The method proposed in this article has the advantage that it is performed without any hypothesis on the document structure, either about the document model (physical structure), or the typographical parameters (logical structure). It is also parameter-free since it automatically adapts to the image content. In this paper, firstly, we detail our proposal to characterize the content of old documents by extracting the autocorrelation features in the different areas of a page and at several resolutions. Then, we show that is possible to automatically find the homogeneous regions defined by similar indices of autocorrelation without knowledge about the number of clusters using adapted hierarchical ascendant classification and consensus clustering approaches. To assess our method, we apply our algorithm on 316 old document images, which encompass six centuries (1200-1900) of French history, in order to demonstrate the performance of our proposal in terms of segmentation and characterization of heterogeneous corpus content. Moreover, we define a new evaluation metric, the homogeneity measure, which aims at evaluating the segmentation and characterization accuracy of our methodology. We find a 85\% of mean homogeneity accuracy. Those results help to represent a document by a hierarchy of layout structure and content, and to define one or more signatures for each page, on the basis of a hierarchical representation of homogeneous blocks and their topology. © 2013 SPIE-IS\&T.},
	language = {English},
	urldate = {2013-02-05},
	booktitle = {Proc {SPIE} {Int} {Soc} {Opt} {Eng}},
	author = {Mehri, M. and Gomez-Krämer, P. and Héroux, P. and Mullot, R.},
	collaborator = {{The Society for Imaging Science and Technology (IS and T); The Society of Photo-Optical Instrumentation Engineers (SPIE); Qualcomm Inc.; Google Inc.}},
	year = {2013},
	note = {Journal Abbreviation: Proc SPIE Int Soc Opt Eng},
	keywords = {autocorrelation, Autocorrelation, Autocorrelation features, Autocorrelation functions, Characterization, Cluster analysis, consensus clustering, Consensus clustering, consensus clustering., Digital image storage, Digital libraries, directional rose, Directional rose, Document layout analysis, HEMDIG - SCOPUS, HEMDIG - WOS, Heterogeneous collections, Hierarchical representation, Image segmentation, Metadata, Multi-resolutions, multiresolution, Multiresolution analysis, Optical character recognition, Quality control, Regression analysis, SCOPUS, Segmentation},
}

@inproceedings{svendsen_document_2013,
	address = {Burlingame, CA},
	title = {Document segmentation via oblique cuts},
	volume = {8658},
	isbn = {0277786X (ISSN); 9780819494313 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875834336&doi=10.1117%2f12.2003351&partnerID=40&md5=c43481deebc1e3545cf418aece80359d},
	doi = {10.1117/12.2003351},
	abstract = {This paper presents a novel solution for the layout segmentation of graphical elements in Business Intelligence documents. We propose a generalization of the recursive X-Y cut algorithm, which allows for cutting along arbitrary oblique directions. An intermediate processing step consisting of line and solid region removal is also necessary due to presence of decorative elements. The output of the proposed segmentation is a hierarchical structure which allows for the identification of primitives in pie and bar charts. The algorithm was tested on a database composed of charts from business documents. Results are very promising. © 2013 SPIE-IS\&T.},
	language = {English},
	urldate = {2013-02-05},
	booktitle = {Proc {SPIE} {Int} {Soc} {Opt} {Eng}},
	author = {Svendsen, J. and Branzan-Albu, A.},
	collaborator = {{The Society for Imaging Science and Technology (IS and T); The Society of Photo-Optical Instrumentation Engineers (SPIE); Qualcomm Inc.; Google Inc.}},
	year = {2013},
	note = {Journal Abbreviation: Proc SPIE Int Soc Opt Eng},
	keywords = {Algorithms, Business documents, Chart Recognition, Document analysis, Document Analysis and Recognition, Document segmentation, Document Segmentation, Engineering, Graphical elements, HEMDIG - SCOPUS, HEMDIG - WOS, Hierarchical structures, Molecular physics, Oblique direction, Processing steps, SCOPUS},
}

@inproceedings{baechler_text_2013,
	address = {Washington, DC},
	title = {Text line extraction using {DMLP} classifiers for historical manuscripts},
	isbn = {15205363 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889582038&doi=10.1109%2fICDAR.2013.206&partnerID=40&md5=94165d512203b615e4110187deb9f5a3},
	doi = {10.1109/ICDAR.2013.206},
	abstract = {This paper proposes a novel text line extraction method for historical documents. The method works in two steps. In the first step, layout analysis is performed to recognize the physical structure of a given document using a classification technique, more precisely the pixels of a coloured document image are classified into five classes: text-block, core-text-line, decoration, background, and periphery. This layout recognition is achieved by a cascade of two Dynamic Multilayer Perceptron (DMLP) classifiers and works without binarisation. In the second step, an algorithm takes the layout recognition results as an input, extracts the text lines, and groups them into blocks using the connected components approach. Finally, the algorithm refines the boundaries of the text lines using the binary image and the layout recognition results. Our system is evaluated on three historical manuscripts with a test set of 49 pages. The best obtained hit rate for text lines is 96.3\%. © 2013 IEEE.},
	language = {English},
	urldate = {2013-08-25},
	booktitle = {Proc. {Int}. {Conf}. {Doc}. {Anal}. {Recognit}.},
	author = {Baechler, M. and Liwicki, M. and Ingold, R.},
	collaborator = {{Raytheon BBN Technologies; ABBYY; VisionObjects; Google; HITACHI}},
	year = {2013},
	note = {Journal Abbreviation: Proc. Int. Conf. Doc. Anal. Recognit.},
	keywords = {Algorithms, Character recognition, Classification (of information), Classification technique, Connected component, Document images, DOCUMENTS, HEMDIG - SCOPUS, HEMDIG - WOS, Historical documents, History, Information retrieval systems, Layout analysis, Multi layer perceptron, Physical structures, SCOPUS, SEGMENTATION, Text processing, Text-line extractions},
	pages = {1029--1033},
}

@inproceedings{mehri_pixel_2013,
	address = {Washington, DC},
	title = {A {Pixel} {Labeling} {Approach} for {Historical} {Digitized} {Books}},
	isbn = {15205363 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889588853&doi=10.1109%2fICDAR.2013.167&partnerID=40&md5=7f0fa5416d9c0dbafb2cc28b47468f60},
	doi = {10.1109/ICDAR.2013.167},
	abstract = {In the context of historical collection conservation and worldwide diffusion, this paper presents an automatic approach of historical book page layout segmentation. In this article, we propose to search the homogeneous regions from the content of historical digitized books with little a priori knowledge by extracting and analyzing texture features. The novelty of this work lies in the unsupervised clustering of the extracted texture descriptors to find homogeneous regions, i.e. graphic and textual regions, by performing the clustering approach on an entire book instead of processing each page individually. We propose firstly to characterize the content of an entire book by extracting the texture information of each page, as our goal is to compare and index the content of digitized books. The extraction of texture features, computed without any hypothesis on the document structure, is based on two non-parametric tools: the autocorrelation function and multiresolution analysis. Secondly, we perform an unsupervised clustering approach on the extracted features in order to classify automatically the homogeneous regions of book pages. The clustering results are assessed by internal and external accuracy measures. The overall results are quite satisfying. Such analysis would help to construct a computer-aided categorization tool of pages. © 2013 IEEE.},
	language = {English},
	urldate = {2013-08-25},
	booktitle = {Proc. {Int}. {Conf}. {Doc}. {Anal}. {Recognit}.},
	author = {Mehri, M. and Heroux, P. and Gomez-Kramer, P. and Boucher, A. and Mullot, R.},
	collaborator = {{Raytheon BBN Technologies; ABBYY; VisionObjects; Google; HITACHI}},
	year = {2013},
	note = {Journal Abbreviation: Proc. Int. Conf. Doc. Anal. Recognit.},
	keywords = {autocorrelation, Autocorrelation, Cluster analysis, Clustering accuracy, clustering accuracy metrics, Clustering algorithms, Computer aided analysis, consensus clustering, Consensus clustering, HEMDIG - SCOPUS, HEMDIG - WOS, Historical books, homogeneity, multiresolution, Multiresolution, pixel labeling, Pixel labeling, Pixels, SCOPUS, texture, TEXTURE, Textures, Tools},
	pages = {817--821},
}

@inproceedings{komatsu_area_2015,
	title = {Area detection technology for air conditioner},
	isbn = {9784907764487 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960158187&doi=10.1109%2fSICE.2015.7285402&partnerID=40&md5=6228e69bab9578dd02804f70f8a46e82},
	doi = {10.1109/SICE.2015.7285402},
	abstract = {We developed room layout detection technology for air conditioners. This technology uses an image camera that is equipped with the air conditioners. The technology controls the direction of the wind and their air capacity. It has been installed in commercial products as a 'layout search.'. © 2015 The Society of Instrument and Control Engineers-SICE.},
	language = {English},
	urldate = {2015-07-28},
	booktitle = {Annu. {Conf}. {Soc}. {Instr}. {Control} {Eng}. {Japan}, {SICE}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Komatsu, Y. and Hamada, K. and Nukaga, N. and Kagehiro, T. and Ueda, Y. and Matsubara, E. and Jinno, N.},
	year = {2015},
	note = {Journal Abbreviation: Annu. Conf. Soc. Instr. Control Eng. Japan, SICE},
	keywords = {Air capacity, Air conditioner, Air conditioning, Area detection, Camera, Cameras, Commercial products, Detection technology, Domestic appliances, HEMDIG - SCOPUS, HEMDIG - WOS, Layout recognition, Partition recognition, SCOPUS, Technology use},
	pages = {1094--1100},
}

@article{lou_extracting_2015,
	title = {Extracting {3D} {Layout} {From} a {Single} {Image} {Using} {Global} {Image} {Structures}},
	volume = {24},
	issn = {10577149 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933056746&doi=10.1109%2fTIP.2015.2431443&partnerID=40&md5=f6fe8de073fdd311dd6584ddc9f0d0db},
	doi = {10.1109/TIP.2015.2431443},
	abstract = {Extracting the pixel-level 3D layout from a single image is important for different applications, such as object localization, image, and video categorization. Traditionally, the 3D layout is derived by solving a pixel-level classification problem. However, the image-level 3D structure can be very beneficial for extracting pixel-level 3D layout since it implies the way how pixels in the image are organized. In this paper, we propose an approach that first predicts the global image structure, and then we use the global structure for fine-grained pixel-level 3D layout extraction. In particular, image features are extracted based on multiple layout templates. We then learn a discriminative model for classifying the global layout at the image-level. Using latent variables, we implicitly model the sublevel semantics of the image, which enrich the expressiveness of our model. After the image-level structure is obtained, it is used as the prior knowledge to infer pixel-wise 3D layout. Experiments show that the results of our model outperform the state-of-the-art methods by 11.7\% for 3D structure classification. Moreover, we show that employing the 3D structure prior information yields accurate 3D scene layout segmentation. © 1992-2012 IEEE.},
	language = {English},
	number = {10},
	journal = {IEEE Transactions on Image Processing},
	author = {Lou, Z. and Gevers, T. and Hu, N.},
	year = {2015},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {3D layout, 3D layouts, Discriminative models, GRADIENTS, HEMDIG - SCOPUS, HEMDIG - WOS, Image analysis, Image processing, Image Structures, Object localization, Pixels, Prior information, SCENE, SCOPUS, Semantics, Stage classification, State-of-the-art methods, structural SVM, Structural SVM, Three dimensional computer graphics, Video categorization},
	pages = {3098--3108},
}

@article{chherawala_arabic_2014,
	title = {Arabic word descriptor for handwritten word indexing and lexicon reduction},
	volume = {47},
	issn = {00313203 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902372575&doi=10.1016%2fj.patcog.2014.04.025&partnerID=40&md5=98d011fd3855a5eb8f7b1fffe9394281},
	doi = {10.1016/j.patcog.2014.04.025},
	abstract = {Word recognition systems use a lexicon to guide the recognition process in order to improve the recognition rate. However, as the lexicon grows, the computation time increases. In this paper, we present the Arabic word descriptor (AWD) for Arabic word shape indexing and lexicon reduction in handwritten documents. It is formed in two stages. First, the structural descriptor (SD) is computed for each connected component (CC) of the word image. It describes the CC shape using the bag-of-words model, where each visual word represents a different local shape structure, extracted from the image with filters of different patterns and scales. Then, the AWD is formed by sorting and normalizing the SDs. This emphasizes the symbolic features of Arabic words, such as subwords and diacritics, without performing layout segmentation. In the context of lexicon reduction, the AWD is used to index a reference database. Given a query image, the reduced lexicon is obtained from the labels of the first entries in the indexed database. This framework has been tested on Arabic word databases. It has a low computational overhead, while providing a compact descriptor, with state-of-the-art results for lexicon reduction on the Ibn Sina and IFN/ENIT databases. © 2014 Elsevier Ltd.},
	language = {English},
	number = {10},
	journal = {Pattern Recognition},
	author = {Chherawala, Y. and Cheriet, M.},
	year = {2014},
	note = {Publisher: Elsevier Ltd},
	keywords = {Arabic handwritten documents, Arabic word descriptor, Database systems, Descriptors, HEMDIG - SCOPUS, HEMDIG - WOS, Holistic representation, Ibn Sina database, IFN/ENIT, Indexing (of information), Information retrieval, Lexicon reduction, Lexicon reductions, Query processing, RECOGNITION, SCOPUS, Shape indexing, Vocabulary control},
	pages = {3477--3486},
}

@inproceedings{zhu_segmentation_2017,
	title = {A segmentation algorithm based on image projection for complex text layout},
	volume = {1890},
	isbn = {0094243X (ISSN); 9780735415683 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031097249&doi=10.1063%2f1.5005199&partnerID=40&md5=79e4d70dccc9cde0902e9bdd2c8cde29},
	doi = {10.1063/1.5005199},
	abstract = {Segmentation algorithm is an important part of layout analysis, considering the efficiency advantage of the top-down approach and the particularity of the object, a breakdown of projection layout segmentation algorithm. Firstly, the algorithm will algorithm first partitions the text image, and divided into several columns, then for each column scanning projection, the text image is divided into several sub regions through multiple projection. The experimental results show that, this method inherits the projection itself and rapid calculation speed, but also can avoid the effect of arc image information page segmentation, and also can accurate segmentation of the text image layout is complex. © 2017 Author(s).},
	language = {English},
	urldate = {2017-10-27},
	booktitle = {{AIP} {Conf}. {Proc}.},
	publisher = {American Institute of Physics Inc.},
	author = {Zhu, W. and Chen, Q. and Wei, C. and Li, Z.},
	year = {2017},
	note = {Journal Abbreviation: AIP Conf. Proc.},
	keywords = {HEMDIG - SCOPUS, HEMDIG - WOS, projection, SCOPUS, Text image, Text layout segmentation, top-down breakdown},
}

@inproceedings{ma_time_2018,
	title = {From {Time} to {Space}: {Automatic} {Annotation} of {Unmarked} {Traffic} {Scene} {Based} on {Trajectory} {Data}},
	isbn = {9781728103761 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064140224&doi=10.1109%2fROBIO.2018.8665110&partnerID=40&md5=61c385ca89ac80d162d6ac953a7ea228},
	doi = {10.1109/ROBIO.2018.8665110},
	abstract = {Research on autonomous driving is requiring more affordance information from the environment, which can be efficiently used by the subject vehicle. Road layout parsing is fundamentally important to this, and has attracted deserved attention on the urban road net where traffic signs are well established. However, for the unmarked traffic scenes, like campus and residence community, the large variation on individual surroundings leads to a reduced attention on public research and a lack of both available dataset and generic methods. To explore this issue, we present a novel automatic annotation method to analyze road semantics, which regards the prior trajectories of a vehicle as multi-dimensional space series and extends the traditional time series methods to the space domain to process the data. In order to achieve this, we first propose Incremental Dynamic Space Warping (IDSW) to synchronize space series and extract statistical road feature, then a Bayesian nonparametric method based on the Hierarchical Dirichlet Process-Hidden Markov Model (HDP-HMM) is introduced to generate road semantic sections without explicitly specifying the number of class. Experiment results demonstrate the proposed method can produce road semantics in different level and annotate not only the physical road position but also the vision observations: based on the extracted road modes, the method can annotate corresponding vision image with a positive class label and a rough distance to the road intersection. © 2018 IEEE.},
	language = {English},
	urldate = {2018-12-12},
	booktitle = {{IEEE} {Int}. {Conf}. {Robot}. {Biomimetics}, {ROBIO}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Ma, H. and Wang, Y. and Xiong, R.},
	collaborator = {et al.; Guangdong University of Technology; Guangzhou University; IEEE; IEEE Robotics {and} Automation Society (RAS); Shenzhen Han's Robot Co., Ltd.},
	year = {2018},
	note = {Journal Abbreviation: IEEE Int. Conf. Robot. Biomimetics, ROBIO},
	keywords = {Automatic annotation, Autonomous driving, Autonomous vehicles, Biomimetics, DRIVING BEHAVIOR, HEMDIG - SCOPUS, HEMDIG - WOS, Hidden Markov models, Hierarchical dirichlet process hidden markov model (HDP HMM), Large dataset, Multi-dimensional space, Nonparametric methods, Number of class, PREDICTION, Road intersections, Roads and streets, Robotics, SCOPUS, Semantics, Time series method, Traffic signs},
	pages = {1177--1182},
}

@inproceedings{sharma_unified_2016,
	title = {A unified framework for semantic matching of architectural floorplans},
	volume = {0},
	isbn = {10514651 (ISSN); 9781509048472 (ISBN)},
	doi = {10.1109/ICPR.2016.7899999},
	abstract = {An automatic lookup tool, which matches and retrieves similar floorplans from a large repository of digitized architectural floorplans can prove to be of immense help for the architects while designing new projects. In this paper, we have proposed a framework for the matching and retrieval of similar architectural floorplans under the query by example paradigm. We propose a room layout segmentation and adjacent room detection algorithm to represent layouts as an undirected graph. We have also proposed a novel graph spectral embedding feature to uniquely represent the layout of the architectural floorplan. This helps in effective and efficient matching of the room layouts. Room semantics in terms of both the room structures and room decor is used to retrieve similar floorplans from the repository. To match the semantic similarity between a pair of floorplans, we have proposed a two stage matching technique. We have validated the effectiveness of our proposed framework by performing experiments on publicly available floorplan dataset and achieved high retrieval accuracy. © 2016 IEEE.},
	language = {English},
	urldate = {2016-12-04},
	booktitle = {Proc. {Int}. {Conf}. {Pattern} {Recognit}.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Sharma, D. and Chattopadhyay, C. and Harit, G.},
	year = {2016},
	note = {Journal Abbreviation: Proc. Int. Conf. Pattern Recognit.},
	keywords = {Detection algorithm, Graph spectral, HEMDIG - SCOPUS, HEMDIG - WOS, ISOMORPHISM, Pattern recognition, Query-by example, RECOGNITION, Retrieval accuracy, SCOPUS, Semantic matching, Semantic similarity, Semantics, Undirected graph, Unified framework},
	pages = {2422--2427},
}

@inproceedings{corbelli_historical_2016,
	title = {Historical document digitization through layout analysis and deep content classification},
	volume = {0},
	isbn = {10514651 (ISSN); 9781509048472 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019108141&doi=10.1109%2fICPR.2016.7900272&partnerID=40&md5=b82347e3a1e032e014b5a885733962de},
	doi = {10.1109/ICPR.2016.7900272},
	abstract = {Document layout segmentation and recognition is an important task in the creation of digitized documents collections, especially when dealing with historical documents. This paper presents an hybrid approach to layout segmentation as well as a strategy to classify document regions, which is applied to the process of digitization of an historical encyclopedia. Our layout analysis method merges a classic top-down approach and a bottom-up classification process based on local geometrical features, while regions are classified by means of features extracted from a Convolutional Neural Network merged in a Random Forest classifier. Experiments are conducted on the first volume of the 'Enciclopedia Treccani', a large dataset containing 999 manually annotated pages from the historical Italian encyclopedia. © 2016 IEEE.},
	language = {English},
	urldate = {2016-12-04},
	booktitle = {Proc. {Int}. {Conf}. {Pattern} {Recognit}.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Corbelli, A. and Baraldi, L. and Grana, C. and Cucchiara, R.},
	year = {2016},
	note = {Journal Abbreviation: Proc. Int. Conf. Pattern Recognit.},
	keywords = {Classification process, Content classification, Convolutional neural network, Decision trees, Document layouts, Geometrical features, HEMDIG - SCOPUS, HEMDIG - WOS, Historical documents, History, IMAGE CLASSIFICATION, Information retrieval systems, Neural networks, Pattern recognition, PERFORMANCE EVALUATION, Random forest classifier, SCOPUS, SEGMENTATION, Top down approaches},
	pages = {4077--4082},
}

@inproceedings{cote_layered_2016,
	title = {Layered ground truth: {Conveying} structural and statistical information for document image analysis and evaluation},
	volume = {0},
	isbn = {10514651 (ISSN); 9781509048472 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019160635&doi=10.1109%2fICPR.2016.7900137&partnerID=40&md5=548726d07d4e9e14000e49c61ab1eb05},
	doi = {10.1109/ICPR.2016.7900137},
	abstract = {This paper addresses the problem of semantic overlap across document objects in the context of ground truth representation for document layout analysis. Document object categories often share primitives from a low-level perspective (e.g. regions inside bars in a bar chart resemble background), making it difficult to evaluate document layout segmentation methods based on pixel classification, as most datasets and ground truth models focus on document objects. We propose a novel ground truth model that utilizes structural and statistical pattern recognition concepts. Statistical pixel-based data derived from low-level elemental patterns are layered onto high-level structural object-based data. We also present evaluation metrics that take advantage of the layered ground truth model, allowing a contextual evaluation of pixel classification algorithms. We apply the proposed model to two recent pixel classification approaches, evaluated on business document images that exhibit a challenging mixture of textual, graphical, and pictorial elements through varied layouts. The proposed model allows to obtain very detailed, comprehensive, and intuitive information on the strengths and limitations of the evaluated approaches that would be impossible to obtain through other models. © 2016 IEEE.},
	language = {English},
	urldate = {2016-12-04},
	booktitle = {Proc. {Int}. {Conf}. {Pattern} {Recognit}.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Cote, M. and Albu, A.B.},
	year = {2016},
	note = {Journal Abbreviation: Proc. Int. Conf. Pattern Recognit.},
	keywords = {BENCHMARKING, Classification (of information), Digital image storage, document image dataset, Document image dataset, Document images, document layout analysis, Document layout analysis, ground truth, Ground truth, HEMDIG - SCOPUS, HEMDIG - WOS, Image analysis, image segmentation, Image segmentation, Information retrieval systems, LAYOUT ANALYSIS, PAGE SEGMENTATION, Pattern recognition, performance evaluation, Performance evaluation, PERFORMANCE EVALUATION, Performance evaluations, Pixels, SCOPUS, SEGMENTATION ALGORITHMS, Semantics, statistical pattern, Statistical pattern, Structural analysis, structural pattern, Structural pattern},
	pages = {3258--3263},
}

@inproceedings{cu_watermarking_2018,
	title = {Watermarking for security issue of handwritten documents with fully convolutional networks},
	volume = {2018-August},
	isbn = {21676445 (ISSN); 9781538658758 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060059726&doi=10.1109%2fICFHR-2018.2018.00060&partnerID=40&md5=c713b12c2f1ed6f3b09b66712b181e21},
	doi = {10.1109/ICFHR-2018.2018.00060},
	abstract = {To prevent falsification of handwriting document images, the methods of forensic document examination are widely used to determine the origin and authenticity of a given document. In this paper, we propose an effective approach for security issue of handwriting documents in spatial domain by making use of watermarking technique. To begin with, the handwritten document is pre-processed by replacing gray level values holding high intensity with the mean value of document content. The document is then transformed into standard form to minimize geometric distortion. Next, fully convolutional networks (FCN) is leveraged to detect document's watermarking regions used for hiding secret information wherein an approach of FCN for document layout segmentation is adjusted to solve the problem of watermarking region detection. Lastly, the data hiding process is conducted by dividing gray level values of each connected object situated within watermarking regions into two sets for carrying one watermark bit. The experiments are performed on various handwritten documents, and our approach achieves high performance regarding such properties as imperceptibility and robustness against distortions caused by JPEG compression, geometric transformation and print-and-scan process. © 2018 IEEE.},
	language = {English},
	urldate = {2018-08-05},
	booktitle = {Proc. {Int}. {Conf}. {Front}. {Handwrit}. {Recognit}., {ICFHR}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Cu, V.L. and Burie, J.-C. and Ogier, J.-M.},
	year = {2018},
	note = {Journal Abbreviation: Proc. Int. Conf. Front. Handwrit. Recognit., ICFHR},
	keywords = {Character recognition, Convolution, Convolutional networks, Digital watermarking, document analysis, Document analysis, Document examinations, FCN, Geometric transformations, Handwriting documents, Handwritten document, handwritten document security, HEMDIG - SCOPUS, HEMDIG - WOS, Image compression, Mathematical transformations, Network security, Print and scan process, SCOPUS, watermarking, Watermarking, Watermarking algorithms, watermarking regions},
	pages = {303--308},
}

@inproceedings{xu_layout_2019,
	title = {Layout recognition attacks on split manufacturing},
	isbn = {9781450360074 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061118278&doi=10.1145%2f3287624.3287698&partnerID=40&md5=29af44051974fad487d442a8cee4505c},
	doi = {10.1145/3287624.3287698},
	abstract = {One technique to prevent attacks from an untrusted foundry is split manufacturing, where only a part of the layout is sent to the untrusted high-end foundry, and the rest is manufactured at a trusted low-end foundry. The untrusted foundry has front-end-of-line (FEOL) layout and the original circuit netlist and attempts to identify critical components on the layout for Trojan insertion. Although defense methods for this scenario have been developed, the corresponding attack technique is not well explored. For instance, Boolean satisfiability (SAT) based bijective mapping attack is mentioned without detailed research. Hence, the defense methods are mostly evaluated with the k-security metric without actual attacks. We provide the first systematic study, to the best of our knowledge, on attack techniques in this scenario. Besides of implementing SAT-based bijective mapping attack, we develop a new attack technique based on structural pattern matching. Experimental comparison with bijective mapping attack shows that the new attack technique achieves about the same success rate with much faster speed for cases without the k-security defense, and has a much better success rate at the same runtime for cases with k-security defense. The results offer an alternative and practical interpretation for k-security in split manufacturing. © 2019 Association for Computing Machinery.},
	language = {English},
	urldate = {2019-01-21},
	booktitle = {Proc {Asia} {South} {Pac} {Des} {Autom} {Conf}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Xu, W. and Feng, L. and Rajendran, J.J.V. and Hu, J.},
	collaborator = {ACM SIGDA; Cadence Design Systems, Inc.; CEDA; EIC; IEEE CAS; IPSJ},
	year = {2019},
	note = {Journal Abbreviation: Proc Asia South Pac Des Autom Conf},
	keywords = {Bijective mapping, Boolean satisfiability, Computer aided design, Critical component, Experimental comparison, Foundries, Front end of lines, Hardware security, HEMDIG - SCOPUS, HEMDIG - WOS, layout recognition attack, Layout recognition attack, Malware, Manufacture, Mapping, Network security, Pattern matching, Recognition attacks, SCOPUS, Security metrices, split manufacturing, Split manufacturing, Structural pattern matching},
	pages = {45--50},
}

@article{park_extended_2019,
	title = {An {Extended} {Agent} {Communication} {Framework} for {Rapid} {Reconfiguration} of {Distributed} {Manufacturing} {Systems}},
	volume = {15},
	issn = {15513203 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057893926&doi=10.1109%2fTII.2018.2883409&partnerID=40&md5=c636e384d6fb5c2ac5c10d953ec8ae0c},
	doi = {10.1109/TII.2018.2883409},
	abstract = {For proactive responses to customer requirements and rapidly advancing technologies, current manufacturing systems are required to be increasingly adaptable and reconfigurable. We present a communication framework between the coordinator, workstation agents, and executors to facilitate the reconfiguration process of manufacturing systems. The message structure of interagent information exchange is extended for the initiation, self-layout recognition, and control program update phases of reconfiguration. To provide more autonomy to a manufacturing system and to synchronize the updated shop floor condition, a self-layout recognition mechanism was developed based on infrared communication that enables each workstation to identify directly linked workstations and share the identified information with other shop floor entities for automatic layout change detection. Furthermore, we propose a message-based reprogramming approach to quickly update, insert, or delete command lines in the existing control program for a specific device in a workstation. We demonstrate a reconfiguration process via the communication framework in a factory testbed consisting of nine main workstations in a laboratory environment. © 2018 IEEE.},
	language = {English},
	number = {7},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Park, J.W. and Shin, M. and Kim, D.Y.},
	year = {2019},
	note = {Publisher: IEEE Computer Society},
	keywords = {ADACOR, Agent communication, Agent communications, Communication framework, Computer aided manufacturing, Computer workstations, DESIGN, Distributed manufacturing systems, Floors, HEMDIG - SCOPUS, HEMDIG - WOS, HOLONIC ARCHITECTURE, IEC 61499, IEC standards, Information dissemination, Manufacture, Network protocols, Rapidly advancing technology, Reconfigurable manufacturing system, reconfigurable manufacturing system (RMS), Reconfiguration process, SCOPUS, self-layout recognition},
	pages = {3845--3855},
}

@article{ma_segmentation_2020,
	title = {Segmentation and {Recognition} for {Historical} {Tibetan} {Document} {Images}},
	volume = {8},
	issn = {21693536 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082514305&doi=10.1109%2fACCESS.2020.2975023&partnerID=40&md5=e368df6705e144eff817b559dae4c422},
	doi = {10.1109/ACCESS.2020.2975023},
	abstract = {As a shining pearl in traditional Tibetan culture, historical Tibetan documents have received extensive attention from historians, linguists and Buddhist scholars. These documents are converted into digital form using Tibetan document segmentation and recognition methods. The document digitization is of great significance for the research, protection and inheritance of Tibetan history. This paper proposes an overall segmentation and recognition framework for historical Tibetan document images. Firstly, the historical Tibetan document image is preprocessed to correct imbalanced illumination, tilt and noises, and is further transformed into the binarized image. Secondly, we propose a layout segmentation method based on block projection to segment Tibetan document images into texts, lines and frames. Thirdly, in order to solve the problems of touching strokes between text-lines and curvilinear text-lines, we present a text-line segmentation method based on graph model for historical Tibetan text-line segmentation. Lastly, we present a touching segmentation method to segment touching Tibetan character string, and then recognize Tibetan characters. Experimental results show our proposed methods on layout segmentation, text-line segmentation and touching character string segmentation, achieve the satisfactory performance. The proposed methods can also be applied to other fonts in Tibetan font family. © 2013 IEEE.},
	language = {English},
	journal = {IEEE Access},
	author = {Ma, L. and Long, C. and Duan, L. and Zhang, X. and Li, Y. and Zhao, Q.},
	year = {2020},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {ALGORITHM, block projection, Character recognition, Character strings, Cultural differences, DATABASE, Document segmentation, Handwriting recognition, HANDWRITTEN, HEMDIG - SCOPUS, HEMDIG - WOS, Historical Tibetan document, History, Image recognition, Image segmentation, Layout, LAYOUT ANALYSIS, layout segmentation, Licenses, Recognition methods, SCOPUS, Segmentation methods, Text line segmentation, TEXT LINE SEGMENTATION, text-line segmentation, Tibetans, Touching character, touching character string segmentation},
	pages = {52641--52651},
}

@inproceedings{kiessling_escriptorium_2019,
	title = {{EScriptorium}: {An} open source platform for historical document analysis},
	volume = {2019-January},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097789248&doi=10.1109%2fICDARW.2019.10032&partnerID=40&md5=7e977ada73d514fca0a6a3d9534c8eb3},
	doi = {10.1109/ICDARW.2019.10032},
	abstract = {We describe the new open source document analysis and annotation platform eScriptorium. It allows to upload document collections, transcribe and segment them manually or automatically with the help of the kraken OCR engine. © 2019 IEEE.},
	language = {English},
	urldate = {2019-09-21},
	booktitle = {Int. {Conf}. {Doc}. {Anal}. {Recog}. {Workshops}, {ICDARW}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Kiessling, B. and Tissot, R. and Stokes, P. and Ezra, D.S.B.},
	year = {2019},
	note = {Journal Abbreviation: Int. Conf. Doc. Anal. Recog. Workshops, ICDARW},
	keywords = {document analysis, Document analysis, Document collection, GUI, handwritten text recognition, Handwritten text recognition, HEMDIG - SCOPUS, HEMDIG - WOS, historical documents, Historical documents, layout segmentation, Layout segmentation, OCR engines, Open source platforms, Open sources, SCOPUS},
	pages = {19--24},
}

@inproceedings{trivedi_hindola_2019,
	title = {{HInDoLA}: {A} {Unified} {Cloud}-based {Platform} for {Annotation}, {Visualization} and {Machine} {Learning}-based {Layout} {Analysis} of {Historical} {Manuscripts}},
	volume = {2019-January},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114243327&doi=10.1109%2fICDARW.2019.10035&partnerID=40&md5=6c28f20834d46bafdab474998a0ddb03},
	doi = {10.1109/ICDARW.2019.10035},
	abstract = {Palm-leaf manuscripts are one of the oldest medium of inscription in many Asian countries. Especially, manuscripts from the Indian subcontinent form an important part of the world's literary and cultural heritage. Despite their significance, large-scale datasets for layout parsing and targeted annotation systems do not exist. Addressing this, we propose a web-based layout annotation and analytics system. Our system, called HInDoLA, features an intuitive annotation GUI, a graphical analytics dashboard and interfaces with machine-learning based intelligent modules on the backend. HInDoLA has successfully helped us create the first ever large-scale dataset for layout parsing of Indic palm-leaf manuscripts. These manuscripts, in turn, have been used to train and deploy deep-learning based modules for fully automatic and semi-automatic instance-level layout parsing. © 2019 IEEE.},
	language = {English},
	urldate = {2019-09-21},
	booktitle = {Int. {Conf}. {Doc}. {Anal}. {Recog}. {Workshops}, {ICDARW}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Trivedi, A. and Sarvadevabhatla, R.K.},
	year = {2019},
	note = {Journal Abbreviation: Int. Conf. Doc. Anal. Recog. Workshops, ICDARW},
	keywords = {Analytics systems, Annotation systems, Cloud based platforms, Cultural heritages, Deep learning, HEMDIG - SCOPUS, HEMDIG - WOS, Historic preservation, Indian subcontinents, Intelligent modules, Large dataset, Large-scale dataset, Large-scale datasets, Learning systems, SCOPUS},
	pages = {31--35},
}

@article{zhang_pattern_2020,
	title = {Pattern understanding and synthesis based on layout tree descriptor},
	volume = {36},
	issn = {01782789 (ISSN)},
	doi = {10.1007/s00371-019-01723-5},
	abstract = {Synthesis from existing examples is a promising way to generate new patterns. However, pattern synthesis is challenging because it is difficult to understand and generate complex structures in patterns. In this paper, we propose an approach based on the layout tree descriptor (LTD) to understand and synthesize patterns from existing ones. The LTD is a binary tree that parametrically describes all primitives, layouts, their dependencies and hierarchies in a pattern. The LTD can be constructed automatically with proposed instance grouping, layout recognition, hyper-primitive matching and tree merging algorithms to realize pattern understanding. To meet specialists’ requirements for detailed modification and recombination of patterns, we designed LTD operations including add, remove, replace and grafting operations to allow users to get new patterns by simply adjusting the LTDs. For stylized synthesis, we gave the computing method of LTD similarity. Therefore, the styles of results and input can be compared and users can control generated serialized results by setting the input pattern weights. To meet user’s implicit preferences and provide novelty in creative design, we propose an evolutionary approach to creative synthesis. The system generates new patterns continuously based on LTD grafting, meanwhile user selection of preferred patterns will guide the direction of evolution. Experiments using the developed prototype system show that our approach can synthesize novel and complex patterns effectively, meeting different requirements in practice and providing plenty of digital textures for products. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.},
	language = {English},
	number = {6},
	journal = {Visual Computer},
	author = {Zhang, X. and Wang, J. and Lu, G. and Zhang, X.},
	year = {2020},
	note = {Publisher: Springer},
	keywords = {Binary trees, DESIGN, Design space exploration, Design tool, Design tools, GraphicaL model, Graphical models, HEMDIG - SCOPUS, HEMDIG - WOS, Layouts, Patterns, RELATIVE POSITIONS, REPRESENTATION, SCOPUS, Synthesis, Synthesis (chemical), SYSTEM, TEXTILE, Textures},
	pages = {1141--1155},
}

@article{posch_lima_2020,
	title = {Lima or cima? {Structure} recognition and {OCR} in building the corpus of the {Austrian} {Alpine} {Club} {Journal}},
	volume = {25},
	issn = {13846655 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097218873&doi=10.1075%2fijcl.19094.pos&partnerID=40&md5=e6d481b80d05c74c0a56e7a4d5e35f87},
	doi = {10.1075/ijcl.19094.pos},
	abstract = {This paper outlines the construction of the corpus Alpenwort, a large, genre-based corpus of German texts on alpinism. We report on issues related to building the corpus from the Austrian Alpine Club Journal (1869–2010). First, a general description of our data and the project phases from digitization and annotation to publication is given. We focus on the most interesting challenges that the diverse layouts and the extensive use of Fraktur typefacing posed for optical layout recognition and optical character recognition (OCR) as well as post correction. The corrected data was lemmatized and annotated with part-of-speech information including named entities as well as TEI-conformant metadata. The resulting 19.9-million-word corpus is designed to be queried using CQPweb and Hyperbase and can be accessed freely online. Lastly, we give a short roadmap of current and future expansions and improvements as corpus data has been and is being enhanced in follow-up projects. © John Benjamins Publishing Company},
	language = {English},
	number = {4},
	journal = {International Journal of Corpus Linguistics},
	author = {Posch, C. and Rampl, G.},
	year = {2020},
	note = {Publisher: John Benjamins Publishing Company},
	keywords = {alpinism, Alpinism, document structure recognition, Document structure recognition, German Fraktur typeface, HEMDIG - SCOPUS, HEMDIG - WOS, OCR, SCOPUS, specialized corpora, Specialized corpora},
	pages = {489--503},
}

@article{kim_modular_2020,
	title = {A modular factory testbed for the rapid reconfiguration of manufacturing systems},
	volume = {31},
	issn = {09565515 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064196111&doi=10.1007%2fs10845-019-01471-2&partnerID=40&md5=f6c59cffae8b6f225bd3e30900a2ac67},
	doi = {10.1007/s10845-019-01471-2},
	abstract = {The recent manufacturing trend toward mass customization and further personalization of products requires factories to be smarter than ever before in order to: (1) quickly respond to customer requirements, (2) resiliently retool machinery and adjust operational parameters for unforeseen system failures and product quality problems, and (3) retrofit old systems with upcoming new technologies. Furthermore, product lifecycles are becoming shorter due to unbounded and unpredictable customer requirements, thereby requiring reconfigurable and versatile manufacturing systems that underpin the basic building blocks of smart factories. This study introduces a modular factory testbed, emphasizing transformability and modularity under a distributed shop-floor control architecture. The main technologies and methods, being developed and verified through the testbed, are presented from the four aspects of rapid factory transformation: self-layout recognition, rapid workstation and robot reprogramming, inter-layer information sharing, and configurable software for shop-floor monitoring. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.},
	language = {English},
	number = {3},
	journal = {Journal of Intelligent Manufacturing},
	author = {Kim, D.-Y. and Park, J.-W. and Baek, S. and Park, K.-B. and Kim, H.-R. and Park, J.-I. and Kim, H.-S. and Kim, B.-B. and Oh, H.-Y. and Namgung, K. and Baek, W.},
	year = {2020},
	note = {Publisher: Springer},
	keywords = {ARCHITECTURE, Basic building block, Closed loop control systems, Customer requirements, DESIGN, Distributed control, Distributed parameter control systems, Floors, HEMDIG - SCOPUS, HEMDIG - WOS, Information sharing, INTEGRATION, Life cycle, Machinery, Mass customization, MODEL, Operational parameters, PLATFORM, PRODUCT, Product life cycles, Quality control, Reconfigurable, RESOURCE, ROBOT, SCOPUS, SMART, Smart factory, SUPPORT, Systems engineering, Testbed, Testbeds},
	pages = {661--680},
}

@article{liu_scut-autoalp_2020,
	title = {{SCUT}-{AutoALP}: {A} diverse benchmark dataset for automatic architectural layout parsing},
	volume = {E103D},
	issn = {09168532 (ISSN)},
	doi = {10.1587/transinf.2020EDL8076},
	abstract = {Computer aided design (CAD) technology is widely used for architectural design, but current CAD tools still require high-level design specifications from human. It would be significant to construct an intelligent CAD system allowing automatic architectural layout parsing (AutoALP), which generates candidate designs or predicts architectural attributes without much user intervention. To tackle these problems, many learning-based methods were proposed, and benchmark dataset become one of the essential elements for the data-driven AutoALP. This paper proposes a new dataset called SCUT-AutoALP for multi-paradigm applications. It contains two subsets: 1) Subset-I is for floor plan design containing 300 residential floor plan images with layout, boundary and attribute labels; 2) Subset-II is for urban plan design containing 302 campus plan images with layout, boundary and attribute labels. We analyzed the samples and labels statistically, and evaluated SCUT-AutoALP for different layout parsing tasks of floor plan/urban plan based on conditional generative adversarial networks (cGAN) models. The results verify the effectiveness and indicate the potential applications of SCUT-AutoALP. The dataset is available at https://github.com/designfuturelab702/SCUT-AutoALP-DatabaseRelease. © 2020 The Institute of Electronics, Information and Communication Engineers},
	language = {English},
	number = {12},
	journal = {IEICE Transactions on Information and Systems},
	author = {Liu, Y. and Lai, Y. and Chen, J. and Liang, L. and Deng, Q.},
	year = {2020},
	note = {Publisher: Institute of Electronics, Information and Communication, Engineers, IEICE},
	keywords = {Adversarial networks, Architectural attributes, Benchmark datasets, Computer aided design, Essential elements, floor plan, Floor plan, Floor plan designs, Floors, GAN, HEMDIG - SCOPUS, HEMDIG - WOS, High-level design, image parsing, Image parsing, Learning-based methods, SCOPUS, urban plan, Urban plan, User intervention},
	pages = {2725--2729},
}

@article{deng_design_2020,
	title = {The design of tourism product {CAD} threedimensional modeling system using {VR} technology},
	volume = {15},
	issn = {19326203 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098936481&doi=10.1371%2fjournal.pone.0244205&partnerID=40&md5=95aba59d73e143e3fbfac90b280e2958},
	doi = {10.1371/journal.pone.0244205},
	abstract = {In view of the high homogeneity of tourism products all over the country, an attempt is made to design virtual visit tourism products with cultural experience background, which can reflect the characteristics of culture + tourism in different scenic spots, so that tourists can deeply experience the local culture. Combined with computer aided design (CAD), the virtual three-dimensional (3D) modeling system of scenic spots is designed, and VR real scene visit interactive tourism products suitable for different scenic spots are designed. 360° VR panoramic display technology is used for 360° VR panoramic video shooting and visiting system display production of Elephant Trunk Hill park scenery. A total of 157 images are collected and 720 cloud panoramic interactive H5 tool is selected to produce a display system suitable for 360° VR panoramic display of scenic spots. Meanwhile, based on single view RGB-D image, the latest convolutional neural network (CNN) algorithm and point cloud processing algorithm are used to design the indoor 3D scene reconstruction algorithm based on semantic understanding. Experiments show that the pixel accuracy and mean intersection over union of the indoor scene layout segmentation network segmentation results are 89.5\% and 60.9\%, respectively, that is, it has high accuracy. The VR real scene visit interactive tourism product can make tourists have a more immersive sense of interaction and experience before, during and after the tour. © 2020 Deng et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
	language = {English},
	number = {12 December},
	journal = {PLoS ONE},
	author = {Deng, Y. and Han, S.-Y. and Li, J. and Rong, J. and Fan, W. and Sun, T.},
	year = {2020},
	note = {Publisher: Public Library of Science},
	keywords = {Article, cloud computing, computer aided design, Computer-Aided Design, controlled study, convolutional neural network, economics, EXPERIENCE, HEMDIG - SCOPUS, HEMDIG - WOS, human, Humans, image display, marketing, Marketing, Neural Networks, Computer, procedures, product design, reconstruction algorithm, SCOPUS, segmentation algorithm, semantics, technology, three dimensional computer aided design, tourism, Tourism, virtual reality, Virtual Reality, VIRTUAL-REALITY},
}

@inproceedings{manandhar_magic_2021,
	title = {Magic layouts: {Structural} prior for component detection in user interface designs},
	isbn = {10636919 (ISSN); 9781665445092 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123216048&doi=10.1109%2fCVPR46437.2021.01555&partnerID=40&md5=bf5f35aa4d05861a7f71a89956ea30f5},
	doi = {10.1109/CVPR46437.2021.01555},
	abstract = {We present Magic Layouts; a method for parsing screenshots or hand-drawn sketches of user interface (UI) layouts. Our core contribution is to extend existing detectors to exploit a learned structural prior for UI designs, enabling robust detection of UI components; buttons, text boxes and similar. Specifically we learn a prior over mobile UI layouts, encoding common spatial co-occurrence relationships between different UI components. Conditioning region proposals using this prior leads to performance gains on UI layout parsing for both hand-drawn UIs and app screenshots, which we demonstrate within the context an interactive application for rapidly acquiring digital prototypes of user experience (UX) designs. © 2021 IEEE},
	language = {English},
	urldate = {2021-06-19},
	booktitle = {Proc {IEEE} {Comput} {Soc} {Conf} {Comput} {Vision} {Pattern} {Recognit}},
	publisher = {IEEE Computer Society},
	author = {Manandhar, D. and Jin, H. and Collomosse, J.},
	year = {2021},
	note = {Journal Abbreviation: Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
	keywords = {Co-occurrence relationships, Component detection, Digital devices, Drawing (graphics), Hand-drawn sketches, HEMDIG - SCOPUS, HEMDIG - WOS, Learn+, Mobile user interface, Palmprint recognition, Robust detection, SCOPUS, Screenshots, User interface components, User interface designs, User interface layouts, User interfaces},
	pages = {15804--15813},
}

@inproceedings{zhang_room_2021,
	title = {Room {Layout} {Estimation} by {Learning} {Depth} {Maps} of {Planes} from {2D} {Layout} {Labels}},
	isbn = {9781665431538 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124793010&doi=10.1109%2fM2VIP49856.2021.9665046&partnerID=40&md5=ec6b34c581560641ff01edfde08c8012},
	doi = {10.1109/M2VIP49856.2021.9665046},
	abstract = {Room layout estimation is a basic mission in understanding the indoor scene and has a wide range of applications. Specifically, the task of room layout estimation is to segment an indoor RGB image with semantic surface labels, i.e., ceiling, floor, and walls. A straightforward solution for the problem is to estimate the geometry of the dominant indoor planes. However, since the ground truth depth maps of the layout is not easy to obtain, research in this direction is rare. In this paper, we focus on the cuboid rooms and propose an effective learning framework that can learn the depth maps of planes from 2D layout labels without ground truth depth maps. We employ a deep network to learn the surface parameters, which can be used to produce depth maps of planes and layout results. Then we propose stereo supervision mechanism that encourages the generated layout to be consistent with the ground truth layout segmentation along Z axis and layout edge on the image plane simultaneously, so that the learned surface parameters and layout results are reasonable. Experimental results show that our method can produce high-quality layout estimates and corresponding depth maps on benchmark datasets. © 2021 IEEE.},
	language = {English},
	urldate = {2021-11-26},
	booktitle = {Int. {Conf}. {Mechatronics} {Mach}. {Vis}. {Pract}., {M2VIP}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Zhang, W. and Liu, Y.},
	year = {2021},
	note = {Journal Abbreviation: Int. Conf. Mechatronics Mach. Vis. Pract., M2VIP},
	keywords = {Benchmarking, Computer vision, Depthmap, Effective learning, GEOMETRY, Ground truth, HEMDIG - SCOPUS, HEMDIG - WOS, High quality, Image plane, Learn+, Learning frameworks, RGB images, SCENE, SCOPUS, Semantic Segmentation, Semantics, Stereo image processing, Supervision mechanisms, Surface parameter},
	pages = {777--782},
}

@inproceedings{lu_self-supervised_2022,
	title = {Self-{Supervised} {Road} {Layout} {Parsing} with {Graph} {Auto}-{Encoding}},
	volume = {2022-June},
	isbn = {9781665488211 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135377781&doi=10.1109%2fIV51971.2022.9827287&partnerID=40&md5=ecfd22d28f85636b2e179839e48793d5},
	doi = {10.1109/IV51971.2022.9827287},
	abstract = {Aiming for higher-level scene understanding, this work presents a neural network approach that takes a road-layout map in bird's-eye-view as input, and predicts a human-interpretable graph that represents the road's topological layout. Our approach elevates the understanding of road layouts from pixel level to the level of graphs. To achieve this goal, an image-graph-image auto-encoder is utilized. The network is designed to learn to regress the graph representation at its auto-encoder bottleneck. This learning is self-supervised by an image reconstruction loss, without needing any external manual annotations. We create a synthetic dataset containing common road layout patterns and use it for training of the auto-encoder in addition to the real-world Argoverse dataset. By using this additional synthetic dataset, which conceptually captures human knowledge of road layouts and makes this available to the network for training, we are able to stabilize and further improve the performance of topological road layout understanding on the real-world Argoverse dataset. The evaluation shows that our approach exhibits comparable performance to a strong fully-supervised baseline. © 2022 IEEE.},
	language = {English},
	urldate = {2022-06-05},
	booktitle = {{IEEE} {Intell} {Veh} {Symp} {Proc}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Lu, C. and Dubbelman, G.},
	year = {2022},
	note = {Journal Abbreviation: IEEE Intell Veh Symp Proc},
	keywords = {Auto encoders, Bird's eye view, Computer vision, Encoding (symbols), Encodings, HEMDIG - SCOPUS, HEMDIG - WOS, Image reconstruction, Neural-networks, Performance, Pixel level, Real-world, Road layouts, Roads and streets, Scene understanding, SCOPUS, Signal encoding, Synthetic datasets, Topology, VISION},
	pages = {842--851},
}

@inproceedings{shekhar_opad_2022,
	title = {{OPAD}: {An} {Optimized} {Policy}-based {Active} {Learning} {Framework} for {Document} {Content} {Analysis}},
	volume = {2022-June},
	isbn = {978-1-66548-739-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137819057&doi=10.1109%2fCVPRW56347.2022.00320&partnerID=40&md5=4eceaffff6e78914a6437693887124d5},
	doi = {10.1109/CVPRW56347.2022.00320},
	abstract = {Documents are central to many business systems, and include forms, reports, contracts, invoices or purchase orders. The information in documents is typically in natural language, but can be organized in various layouts and formats. There have been recent spurt of interest in understanding document content with novel deep learning architectures. However, document understanding tasks need dense information annotations, which are costly to scale and generalize. Several active learning techniques have been proposed to reduce the overall budget of annotation while maintaining the performance of the underlying deep learning model. In this paper, we propose OPAD, a novel framework using reinforcement policy for active learning in content detection tasks for documents. The proposed framework learns the acquisition function to decide the samples to be selected while optimizing performance metrics that the tasks typically have. Furthermore, we extend to weak labelling scenarios to further reduce the cost of annotation significantly. We propose novel rewards to account for class imbalance and user feedback in the annotation interface, to improve the active learning method. We show superior performance of the proposed OPAD framework for active learning for various tasks related to document understanding like layout parsing, object detection and named entity recognition. Ablation studies for human feedback and class imbalance rewards are presented, along with a comparison of annotation times for different approaches. © 2022 IEEE.},
	language = {English},
	urldate = {2022-06-19},
	booktitle = {{IEEE} {Comput}. {Soc}. {Conf}. {Comput}. {Vis}. {Pattern} {Recogn}. {Workshops}},
	publisher = {IEEE Computer Society},
	author = {Shekhar, S. and Prakash Reddy Guda, B. and Chaubey, A. and Jindal, I. and Jain, A.},
	year = {2022},
	note = {Journal Abbreviation: IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn. Workshops},
	keywords = {Active Learning, Budget control, Business systems, Class imbalance, Content analysis, Deep learning, Document contents, Document understanding, HEMDIG - SCOPUS, HEMDIG - WOS, Learning frameworks, Learning systems, Object detection, Optimized policies, Performance, Policy-based, Purchasing, SCOPUS},
	pages = {2825--2835},
}

@article{raman_synthetic_2022,
	title = {Synthetic document generator for annotation-free layout recognition},
	volume = {128},
	issn = {00313203 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127197928&doi=10.1016%2fj.patcog.2022.108660&partnerID=40&md5=fe083de6fdf8e607e20729011e7b4d5f},
	doi = {10.1016/j.patcog.2022.108660},
	abstract = {Analyzing the layout of a document to identify headers, sections, tables, figures etc. is critical to understanding its content. Deep learning based approaches for detecting the layout structure of document images have been promising. However, these methods require a large number of annotated examples during training, which are both expensive and time consuming to obtain. We describe here a synthetic document generator that automatically produces realistic documents with labels for spatial positions, extents and categories of the layout elements. The proposed generative process treats every physical component of a document as a random variable and models their intrinsic dependencies using a Bayesian Network graph. Our hierarchical formulation using stochastic templates allow parameter sharing between documents for retaining broad themes and yet the distributional characteristics produces visually unique samples, thereby capturing complex and diverse layouts. We empirically illustrate that a deep layout detection model trained purely on the synthetic documents can match the performance of a model that uses real documents. © 2022 Elsevier Ltd},
	language = {English},
	journal = {Pattern Recognition},
	author = {Raman, N. and Shah, S. and Veloso, M.},
	year = {2022},
	note = {Publisher: Elsevier Ltd},
	keywords = {Bayesia n networks, Bayesian network, Bayesian networks, Deep learning, Document images, Free layouts, Generative process, HEMDIG - SCOPUS, HEMDIG - WOS, Image annotation, Layout analysis, Layout structure, Learning-based approach, Physical components, SCOPUS, Spatial positions, Stochastic systems, Synthetic image generation},
}

@article{elanwar_extracting_2021,
	title = {Extracting text from scanned {Arabic} books: a large-scale benchmark dataset and a fine-tuned {Faster}-{R}-{CNN} model},
	volume = {24},
	issn = {14332833 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117632510&doi=10.1007%2fs10032-021-00382-4&partnerID=40&md5=0300f16ad30c3d96f79b825ffb3032bd},
	doi = {10.1007/s10032-021-00382-4},
	abstract = {Datasets of documents in Arabic are urgently needed to promote computer vision and natural language processing research that addresses the specifics of the language. Unfortunately, publicly available Arabic datasets are limited in size and restricted to certain document domains. This paper presents the release of BE-Arabic-9K, a dataset of more than 9000 high-quality scanned images from over 700 Arabic books. Among these, 1500 images have been manually segmented into regions and labeled by their functionality. BE-Arabic-9K includes book pages with a wide variety of complex layouts and page contents, making it suitable for various document layout analysis and text recognition research tasks. The paper also presents a page layout segmentation and text extraction baseline model based on fine-tuned Faster R-CNN structure (FFRA). This baseline model yields cross-validation results with an average accuracy of 99.4\% and F1 score of 99.1\% for text versus non-text block classification on 1500 annotated images of BE-Arabic-9K. These results are remarkably better than those of the state-of-the-art Arabic book page segmentation system ECDP. FFRA also outperforms three other prior systems when tested on a competition benchmark dataset, making it an outstanding baseline model to challenge. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.},
	language = {English},
	number = {4},
	journal = {International Journal on Document Analysis and Recognition},
	author = {Elanwar, R. and Qin, W. and Betke, M. and Wijaya, D.},
	year = {2021},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Benchmark datasets, Character recognition, Convolutional neural networks, Cross validation, Document layout analysis, DOCUMENTS, HEMDIG - SCOPUS, HEMDIG - WOS, IMAGES, Large dataset, LAYOUT ANALYSIS, NAtural language processing, Natural language processing systems, Page segmentation, SCOPUS, SEGMENTATION, State of the art, Text block classification, Text processing, Text recognition},
	pages = {349--362},
}

@article{zhang_3d_2022,
	title = {{3D} {Layout} {Estimation} via {Weakly} {Supervised} {Learning} of {Plane} {Parameters} from {2D} {Segmentation}},
	volume = {31},
	issn = {10577149 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122074272&doi=10.1109%2fTIP.2021.3131025&partnerID=40&md5=594f7f3f1c04d419291928400b1b1d1d},
	doi = {10.1109/TIP.2021.3131025},
	abstract = {The task of 3D layout estimation in an indoor scene is to predict the holistic 3D structural information of the scene from an RGB image. It is costly to obtain the ground truth 3D layout, and this issue severely restricts the learning based 3D layout estimation approaches. In this paper, we present a novel weakly supervised learning framework that is able to learn the 3D layout effectively with 2D layout segmentation mask as supervision. We employ a deep neural network to predict the plane parameters and camera intrinsic parameters in the image. Based on the predicted plane instances, the 3D layout as well as the corresponding depth map and 2D segmentation can be generated. The key objectives for learning meaningful plane parameters are the label consistency of layout segmentation and depth consistency of border pixels from adjacent planes, with which the ground truth 2D layout segmentation is able to supervise the learning of the 3D layout. We further incorporate 3D geometric reasoning and prior knowledge in the learning process to ensure that the learned 3D layout is realistic and reasonable. Experimental results show that our method can produce accurate 3D layout estimates by weakly supervised learning. © 1992-2012 IEEE.},
	language = {English},
	journal = {IEEE Transactions on Image Processing},
	author = {Zhang, W. and Zhang, Y. and Song, R. and Liu, Y. and Zhang, W.},
	year = {2022},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {2D segmentation, 3d layout estimation, 3D layout estimation, 3D layouts, Cameras, Cognition, Computer vision, Deep neural networks, Estimation, GEOMETRY, HEMDIG - SCOPUS, HEMDIG - WOS, Image segmentation, INDOOR, Layout, monocular stereo vision, Monocular stereo vision, Parameter estimation, Plane parameter, plane parameters, SCOPUS, Solid modeling, Solid modelling, Stereo image processing, Stereo vision, Supervised learning, Three dimensional computer graphics, Three dimensional displays, Three-dimensional display, Three-dimensional displays, weakly supervised learning, Weakly supervised learning},
	pages = {868--879},
}

@article{chen_layout_2021,
	title = {Layout {Segmentation} and {Description} of {Tibetan} {Document} {Images} {Based} on {Adaptive} {Run} {Length} {Smoothing} {Algorithm}},
	volume = {58},
	issn = {1006-4125},
	doi = {10.3788/L0P202158.1410006},
	abstract = {Layout segmentation is an important basic step in the process of document image analysis and recognition. In order to explore a suitable method for layout segmentation and description of Tibetan document images, a research method based on the adaptive run length smoothing algorithm is proposed. Firstly, according to the layout structure of Tibetan document images, K-means clustering analysis is used to get the run length threshold suitable for the layout, smooth the run length, find the connected component, and realize the layout segmentation. Then, according to the external contour characteristics of each layout element, the text area and nontext area are simply distinguished. Finally, the text area is recognized by a Tibetan text recognizer, and then the extensible markup language is used to record layout information and realize layout description. Experiments on Tibetan primary and secondary school teaching materials and stereotyped Tibetan document images show that this method can achieve good layout analysis results.},
	language = {Chinese},
	number = {14},
	journal = {LASER \& OPTOELECTRONICS PROGRESS},
	author = {Chen, YY and Wang, WL and Liu, HM and Cai, ZQ and Zhao, PH},
	month = jul,
	year = {2021},
	keywords = {adaptive run length smoothing, HEMDIG - WOS, image processing, layout description, layout segmentation, Tibetan document image},
}

@article{de_nardin_few-shot_2023,
	title = {Few-{Shot} {Pixel}-{Precise} {Document} {Layout} {Segmentation} via {Dynamic} {Instance} {Generation} and {Local} {Thresholding}},
	issn = {0129-0657},
	doi = {10.1142/S0129065723500521},
	abstract = {Over the years, the humanities community has increasingly requested the creation of artificial intelligence frameworks to help the study of cultural heritage. Document Layout segmentation, which aims at identifying the different structural components of a document page, is a particularly interesting task connected to this trend, specifically when it comes to handwritten texts. While there are many effective approaches to this problem, they all rely on large amounts of data for the training of the underlying models, which is rarely possible in a real-world scenario, as the process of producing the ground truth segmentation task with the required precision to the pixel level is a very time-consuming task and often requires a certain degree of domain knowledge regarding the documents at hand. For this reason, in this paper, we propose an effective few-shot learning framework for document layout segmentation relying on two novel components, namely a dynamic instance generation and a segmentation refinement module. This approach is able of achieving performances comparable to the current state of the art on the popular Diva-HisDB dataset, while relying on just a fraction of the available data.},
	language = {English},
	journal = {INTERNATIONAL JOURNAL OF NEURAL SYSTEMS},
	author = {De Nardin, A and Zottin, S and Piciarelli, C and Colombi, E and Foresti, GL},
	month = aug,
	year = {2023},
	keywords = {Document layout segmentation, few-shot learning, handwritten documents analysis, HEMDIG - WOS, image segmentation, layout analysis},
}

@article{you_sublinear_2023,
	title = {Sublinear information bottleneck based two-stage deep learning approach to genealogy layout recognition},
	volume = {17},
	issn = {1662-453X},
	doi = {10.3389/fnins.2023.1230786},
	abstract = {As an important part of human cultural heritage, the recognition of genealogy layout is of great significance for genealogy research and preservation. This paper proposes a novel method for genealogy layout recognition using our introduced sublinear information bottleneck (SIB) and two-stage deep learning approach. We first proposed an SIB for extracting relevant features from the input image, and then uses the deep learning classifier SIB-ResNet and object detector SIB-YOLOv5 to identify and localize different components of the genealogy layout. The proposed method is evaluated on a dataset of genealogy images and achieves promising results, outperforming existing state-of-the-art methods. This work demonstrates the potential of using information bottleneck and deep learning object detection for genealogy layout recognition, which can have applications in genealogy research and preservation.},
	language = {English},
	journal = {FRONTIERS IN NEUROSCIENCE},
	author = {You, JN and Wang, Q},
	month = jun,
	year = {2023},
	keywords = {deep learning, genealogy layout recognition, HEMDIG - WOS, ResNet, sublinear information bottleneck, YOLOv5 detector},
}

@article{fischer_line-level_2023,
	title = {Line-{Level} {Layout} {Recognition} of {Historical} {Documents} with {Background} {Knowledge}},
	volume = {16},
	issn = {1999-4893},
	doi = {10.3390/a16030136},
	abstract = {Digitization and transcription of historic documents offer new research opportunities for humanists and are the topics of many edition projects. However, manual work is still required for the main phases of layout recognition and the subsequent optical character recognition (OCR) of early printed documents. This paper describes and evaluates how deep learning approaches recognize text lines and can be extended to layout recognition using background knowledge. The evaluation was performed on five corpora of early prints from the 15th and 16th Centuries, representing a variety of layout features. While the main text with standard layouts could be recognized in the correct reading order with a precision and recall of up to 99.9\%, also complex layouts were recognized at a rate as high as 90\% by using background knowledge, the full potential of which was revealed if many pages of the same source were transcribed.},
	language = {English},
	number = {3},
	journal = {ALGORITHMS},
	author = {Fischer, N and Hartelt, A and Puppe, F},
	month = mar,
	year = {2023},
	keywords = {background knowledge, baseline detection, fully convolutional neural networks, HEMDIG - WOS, historical document analysis, layout recognition, text line detection},
}

@inproceedings{kassis_scribble_2016,
	title = {Scribble {Based} {Interactive} {Page} {Layout} {Segmentation} {Using} {Gabor} {Filter}},
	isbn = {2167-6445},
	doi = {10.1109/ICFHR.2016.13},
	abstract = {This paper presents an interactive approach for fast and accurate page layout segmentation. It is a scribble based interactive segmentation approach, where the user draws scribbles on the various regions and the system performs page layout segmentation. The user can correct and refine the resulting segmentation by drawing new scribbles. To classify the various regions of the page, we apply a bank of Gabor filters, in several orientations and multiple frequencies, to capture the orientation, the stroke width, and size of the text. These properties also implicitly encode the writing style of the document. After combining the responses of the Gabor filter into a feature matrix, we classify various regions of the document by applying graph cuts, while taking into account the user made scribbles. The presented approach is very fast, easy to use, robust to user interaction, and provides accurate results.},
	language = {English},
	booktitle = {Ben {Gurion} {University}},
	author = {Kassis, M and El-Sana, J and {IEEE}},
	year = {2016},
	keywords = {CLASSIFICATION, DOCUMENTS, gabor filter, HEMDIG - WOS, IMAGE SEGMENTATION, interactive system, page segmentation, scribble-based},
	pages = {13--18},
}

@incollection{suvichakorn_simple_2002,
	title = {Simple layout segmentation of gray-scale document images},
	volume = {2423},
	isbn = {0302-9743},
	abstract = {A simple yet effective layout segmentation of document images is proposed in this paper. First, n x n blocks are roughly labeled as background, line, text, images, graphics or mixed class. For blocks in mixed class, they are split into 4 sub-blocks and the process repeats until no mixed class is found. By exploiting Savitzky-Golay derivative filter in the classification, the computation of features is kept to the minimum. Next, the boundaries of each object are refined. The experimental results yields a satisfactory results as a pre-process prior to OCR.},
	language = {English},
	booktitle = {{DOCUMENT} {ANALYSIS} {SYSTEM} {V}, {PROCEEDINGS}},
	author = {Suvichakorn, A and Watcharabusaracum, S and Sinthupinyo, W},
	editor = {Lopresti, EP and Hu, J and Kashi, R},
	year = {2002},
	keywords = {HEMDIG - WOS},
	pages = {245--248},
}

@inproceedings{mujibiya_glassnage_2015,
	title = {{GlassNage}: {Layout} {Recognition} for {Dynamic} {Content} {Retrieval} in {Multi}-{Section} {Digital} {Signage}},
	volume = {9189},
	isbn = {0302-9743},
	doi = {10.1007/978-3-319-20804-6_31},
	abstract = {We report our approach to support dynamic content transfer from publicly available large display digital signage to users' private display, specifically Glass-like wearable devices. We aim to address issues concerning dynamic multimedia signage where the content are divided into several sections. This type of signage has become increasingly popular due to optimal content exposures. In contrast to prior research, our approach excludes computer vision based object recognition, and instead took an approach to identify how contents are being laid-out in a digital signage. We incorporate techniques to recognize basic layout features including corners, lines, edges, and line segments; which are obtained from the camera frame taken by the user using their own device. Consequently, these layout features are combined to generate signage layout map, which is then compared to pre-learned layout map for position detection and perspective correction using homography estimation. To grab a specific content, users are able to choose a section within the captured layout using the device's interface, which in turn creates a request to contents server to send respective content information based on a timestamp and a unique section ID. In this paper, we describe implementation details, report user study results, and conclude with discussion of our experiences in implementation as well as highlighting future work.},
	language = {English},
	booktitle = {Rakuten {Group}, {Inc}},
	author = {Mujibiya, A},
	editor = {Streitz, N and Markopoulos, P},
	year = {2015},
	keywords = {Computer vision, Digital signage, HEMDIG - WOS, Layout recognition, Line segment, Multi section, Public display, Public-to-private, User study, Visual features},
	pages = {337--348},
}

@inproceedings{kawoosa_ncert5k-iitrpr_2022,
	title = {{NCERT5K}-{IITRPR}: {A} {Benchmark} {Dataset} for {Non}-textual {Component} {Detection} in {School} {Books}},
	volume = {13237},
	isbn = {0302-9743},
	doi = {10.1007/978-3-031-06555-2_31},
	abstract = {The STEM subjects books heavily rely on Non-textual Components (NTCs) such as charts, geometric figures, and equations to demonstrate the underlying complex concepts. However, the accessibility of STEM subjects for Blind and Visually Impaired (BVIP) students is a primary concern, especially in developing countries such as India. BVIP uses assistive technologies (ATs) like optical character recognition (OCR) and screen readers for reading/writing purposes. While parsing, such ATs skip NTCs and mainly rely on alternative texts to describe these visualization components. Integration of effective and automated document layout parsing frameworks for extracting data from non-textual components of digital documents are required with existing ATs for making these NTCs accessible. Although, the primary concern is the absence of an adequately annotated textbook dataset on which layout recognition and other vision-based frameworks can be trained. To improve the accessibility and automated parsing of such books, we introduce a new NCERT5K-IITRPR dataset of National Council of Educational Research and Training (NCERT) school books. Twenty-three annotated books covering more than 5000 pages from the eighth to twelve standards have been considered. The NCERT label objects are structurally different from the existing document layout analysis (DLA) dataset objects and contain diverse label categories. We benchmark the NCERT5K-IITRPR dataset with multiple object detection methods. A systematic analysis of detectors shows the label complexity and fine-tuning necessity of the NCERT5K-IITRPR dataset. We hope that our dataset helps in improving the accessibility of NCERT Books for BVIP students.},
	language = {English},
	booktitle = {Indian {Institute} of {Technology} {System} ({IIT} {System})},
	author = {Kawoosa, HS and Singh, M and Joshi, MM and Goyal, P},
	editor = {Uchida, S and Barney, E and Eglin, V},
	year = {2022},
	keywords = {Assistive reading, Document layout analysis, Graphical object detection, HEMDIG - WOS, NCERT books, NCERT dataset},
	pages = {461--475},
}

@inproceedings{zhu_docbed_2022,
	title = {{DocBed}: {A} {Multi}-{Stage} {OCR} {Solution} for {Documents} with {Complex} {Layouts}},
	isbn = {2159-5399},
	abstract = {Digitization of newspapers is of interest for many reasons including preservation of history, accessibility and search ability, etc. While digitization of documents such as scientific articles and magazines is prevalent in literature, one of the main challenges for digitization of newspaper lies in its complex layout (e.g. articles spanning multiple columns, text interrupted by images) analysis, which is necessary to preserve human read-order. This work provides a major breakthrough in the digitization of newspapers on three fronts: first, releasing a dataset of 3000 fully-annotated, real-world newspaper images from 21 different U.S. states representing an extensive variety of complex layouts for document layout analysis; second, proposing layout segmentation as a precursor to existing optical character recognition (OCR) engines, where multiple state-of-the-art image segmentation models and several post-processing methods are explored for document layout segmentation; third, providing a thorough and structured evaluation protocol for isolated layout segmentation and end-to-end OCR.},
	language = {English},
	author = {Zhu, WZ and Sokhandan, N and Yang, G and Martin, S and Sathyanarayana, S and {Assoc Advancement Artificial Intelligence}},
	year = {2022},
	keywords = {HEMDIG - WOS},
	pages = {12643--12649},
}

@inproceedings{belagavi_docvisor_2021,
	title = {{DocVisor}: {A} {Multi}-purpose {Web}-{Based} {Interactive} {Visualizer} for {Document} {Image} {Analytics}},
	volume = {12917},
	isbn = {0302-9743},
	doi = {10.1007/978-3-030-86159-9_14},
	abstract = {The performance for many document-based problems (OCR, Document Layout Segmentation, etc.) is typically studied in terms of a single aggregate performance measure (Intersection-Over-Union, Character Error Rate, etc.). While useful, the aggregation is a trade-off between instance-level analysis of predictions which may shed better light on a particular approach's biases and performance characteristics. To enable a systematic understanding of instance-level predictions, we introduce DocVisor - a web-based multi-purpose visualization tool for analyzing the data and predictions related to various document image understanding problems. DocVisor provides support for visualizing data sorted using custom-specified performance metrics and display styles. It also supports the visualization of intermediate outputs (e.g., attention maps, coarse predictions) of the processing pipelines. This paper describes the appealing features of DocVisor and showcases its multi-purpose nature and general utility. We illustrate DocVisor's functionality for four popular document understanding tasks - document region layout segmentation, tabular data detection, weakly-supervised document region segmentation and optical character recognition. DocVisor is available as a documented public repository for use by the community.},
	language = {English},
	booktitle = {International {Institute} of {Information} {Technology} {Hyderabad}},
	author = {Belagavi, K and Tadimeti, P and Sarvadevabhatla, RK},
	editor = {Smith, EHB and Pal, U},
	year = {2021},
	keywords = {HEMDIG - WOS},
	pages = {206--219},
}

@article{yao_3d_2023,
	title = {{3D} layout estimation of general rooms based on ordinal semantic segmentation},
	issn = {1751-9632},
	doi = {10.1049/cvi2.12149},
	abstract = {Room layout estimation aims to predict the location and range of layout planes of interior spaces. Previous works treat each layout plane as an independent individual without considering the ordinal relation between walls, resulting the loss of the wall planes and the lack of integrity. This paper proposes a novel two-branch neural networks model to estimate 3D layouts of cuboid and non-cuboid room types. The model embeds the ordinal relation between layout planes into the layout segmentation branch through an proposed ordinal classification loss function, and outputs both pixel-level layout segmentation maps and layout plane parameter maps. Then, the instance-level plane parameters of each layout plane are determined by using an instance-aware pooling layer. Finally, the sharpness of layout edges of the 2D layout semantic segmentation map is optimized by using an improved depth map intersection algorithm. Furthermore, we annotate a large-scale 3D room layout estimation dataset, InteriorNet-Layout, to obtain a steady model. Experiments on synthesized real-world datasets show that the proposed method achieves faster calculation while maintaining high accuracy. Code is available at .},
	language = {English},
	journal = {IET COMPUTER VISION},
	author = {Yao, H and Miao, J and Zhang, GX and Chu, J},
	month = jan,
	year = {2023},
	keywords = {dataset, depth estimation, HEMDIG - WOS, plane parameters, room layout estimation, semantic segmentation},
}

@inproceedings{maderlechner_extraction_2000,
	title = {Extraction of relevant information from document images using measures of visual attention},
	isbn = {1051-4651},
	abstract = {This paper describes an approach to attention based layout segmentation using general principles of the human visual perception to achieve this goal. The text is considered as texture in different resolution levels. A new measure of attractiveness is introduced. The segmentation is generic and not limited to specific document classes and models. The resulting regions of interest may be used for further interpretation. The overall speed of browsing and searching lar-ge volumes of scanned documents cart be increased considerably.},
	language = {English},
	booktitle = {Siemens {AG}},
	author = {Maderlechner, G and Schreyer, A and Suda, P},
	editor = {Sanfeliu, A and Villanueva, JJ and Vanrell, M and Alquezar, R and Crowley, J and Shirai, Y},
	year = {2000},
	keywords = {HEMDIG - WOS},
	pages = {385--388},
}

@inproceedings{corbelli_layout_2017,
	title = {Layout {Analysis} and {Content} {Classification} in {Digitized} {Books}},
	volume = {701},
	isbn = {1865-0929},
	doi = {10.1007/978-3-319-56300-8_14},
	abstract = {Automatic layout analysis has proven to be extremely important in the process of digitization of large amounts of documents. In this paper we present amixed approach to layout analysis, introducing a SVM-aided layout segmentation process and a classification process based on local and geometrical features. The final output of the automatic analysis algorithm is a complete and structured annotation in JSON format, containing the digitalized text aswell as all the references to the illustrations of the input page, and which can be used by visualization interfaces as well as annotation interfaces. We evaluate our algorithm on a large dataset built upon the first volume of the "Enciclopedia Treccani".},
	language = {English},
	booktitle = {Universita di {Modena} e {Reggio} {Emilia}},
	author = {Corbelli, A and Baraldi, L and Balducci, F and Grana, C and Cucchiara, R},
	editor = {Agosti, M and Bertini, M and Ferilli, S and Marinai, S and Orio, N},
	year = {2017},
	keywords = {Annotation interfaces, Content classification, HEMDIG - WOS, IMAGES, Layout analysis, LINES, SEGMENTATION, SVM},
	pages = {153--165},
}

@inproceedings{gordo_diagonal_2009,
	title = {The {Diagonal} {Split}: {A} {Pre}-segmentation {Step} for {Page} {Layout} {Analysis} and {Classification}},
	volume = {5524},
	isbn = {0302-9743},
	abstract = {Document classification is an important task in all the processes related to document storage and retrieval. In the case of complex documents, structural features are needed to achieve a correct classification. Unfortunately, physical layout analysis is error prone. In this paper we present a pre-segmentation step based on a divide \& conquer strategy that can be used to improve the page segmentation results, independently of the segmentation algorithm used. This pre-segmentation step is evaluated in classification and retrieval using the selective CRLA algorithm for layout segmentation together with a clustering based on the voronoi area diagram, and tested on two different databases, MARC and Girona Archives.},
	language = {English},
	booktitle = {Centre de {Visio} per {Computador} ({CVC})},
	author = {Gordo, A and Valveny, E},
	editor = {Araujo, H and Mendonca, AM and Pinho, AJ and Torres, MI},
	year = {2009},
	keywords = {HEMDIG - WOS},
	pages = {290--297},
}

@inproceedings{hou_research_2016,
	title = {Research on {Dimension} {Adjustment} {Method} of {Variant} {Design}},
	volume = {43},
	isbn = {2352-5401},
	abstract = {In order to study the problem of the dimension of the dimension of the model modified design. A geometric model for the layout of dimension dimension. Through the containment relationship between the dimensions of the relationship between the size of the resulting constraint relationship, on the basis of the creation of the size of the layout recognition algorithm, and the application validation. The size adaptive adjustment of the 3D variant design is realized.},
	language = {English},
	booktitle = {Guilin {University} of {Electronic} {Technology}},
	author = {Hou, BQ and Li, XM and Wang, K},
	editor = {Yue, XG and Cao, Y and Habib, MM and Boddhu, SK},
	year = {2016},
	keywords = {dimension dimension, dimension layout, HEMDIG - WOS, size adjustment, variant design},
	pages = {122--126},
}

@inproceedings{sharan_palmira_2021,
	title = {Palmira: {A} {Deep} {Deformable} {Network} for {Instance} {Segmentation} of {Dense} and {Uneven} {Layouts} in {Handwritten} {Manuscripts}},
	volume = {12822},
	isbn = {0302-9743},
	doi = {10.1007/978-3-030-86331-9_31},
	abstract = {Handwritten documents are often characterized by dense and uneven layout. Despite advances, standard deep network based approaches for semantic layout segmentation are not robust to complex deformations seen across semantic regions. This phenomenon is especially pronounced for the low-resource Indic palm-leaf manuscript domain. To address the issue, we first introduce Indiscapes2, a new large-scale diverse dataset of Indic manuscripts with semantic layout annotations. Indiscapes2 contains documents from four different historical collections and is 150\% larger than its predecessor, Indiscapes. We also propose a novel deep network PALMIRA for robust, deformation-aware instance segmentation of regions in handwritten manuscripts. We also report Hausdorff distance and its variants as a boundary-aware performance measure. Our experiments demonstrate that PALMIRA provides robust layouts, outperforms strong baseline approaches and ablative variants. We also include qualitative results on Arabic, South-East Asian and Hebrew historical manuscripts to showcase the generalization capability of PALMIRA.},
	language = {English},
	booktitle = {International {Institute} of {Information} {Technology} {Hyderabad}},
	author = {Sharan, SP and Aitha, S and Kumar, A and Trivedi, A and Augustine, A and Sarvadevabhatla, RK},
	editor = {Llados, J and Lopresti, D and Uchida, S},
	year = {2021},
	keywords = {Dataset, Deformable convolutional network, Document image segmentation, HEMDIG - WOS, Historical document analysis, Instance segmentation},
	pages = {477--491},
}

@inproceedings{cao_towards_2021,
	title = {Towards {Document} {Panoptic} {Segmentation} with {Pinpoint} {Accuracy}: {Method} and {Evaluation}},
	volume = {12822},
	isbn = {0302-9743},
	doi = {10.1007/978-3-030-86331-9_1},
	abstract = {In this paper we study the task of document layout recognition for digital documents, requiring that the model should detect the exact physical object region without missing any text or containing any redundant text outside objects. It is the vital step to support high-quality information extraction, table understanding and knowledge base construction over the documents from various vertical domains (e.g. financial, legal, and government fields). Here, we consider digital documents, where characters and graphic elements are given with their exact texts, positions inside document pages, compared with image documents. Towards document layout recognition with pinpoint accuracy, we consider this problem as a document panoptic segmentation task, that each token in the document page must be assigned a class label and an instance id. Considering that two predicted objects may intersect under traditional visual panoptic segmentation method, like Mask R-CNN, however, document objects never intersect because most document pages follow manhattan layout. Therefore, we propose a novel framework, named document panoptic segmentation (DPS) model. It first splits the document page into column regions and groups tokens into line regions, then extracts the textual and visual features, and finally assigns class label and instance id to each line region. Additionally, we propose a novel metric based on the intersection over union (IoU) between the tokens contained in predicted and the ground-truth object, which is more suitable than metric based on the area IoU between predicted and the ground-truth bounding box. Finally, the empirical experiments based on PubLayNet, ArXiv and Financial datasets show that the proposed DPS model obtains 0.8833, 0.9205 and 0.8530 mAP scores on three datasets. The proposed model obtains great improvement on mAP score compared with Faster R-CNN and Mask R-CNN models.},
	language = {English},
	booktitle = {Chinese {Academy} of {Sciences}},
	author = {Cao, RY and Li, HW and Zhou, GB and Luo, P},
	editor = {Llados, J and Lopresti, D and Uchida, S},
	year = {2021},
	keywords = {HEMDIG - WOS},
	pages = {3--18},
}

@inproceedings{yepes_icdar_2021,
	title = {{ICDAR} 2021 {Competition} on {Scientific} {Literature} {Parsing}},
	volume = {12824},
	isbn = {0302-9743},
	doi = {10.1007/978-3-030-86337-1_40},
	abstract = {Scientific literature contain important information related to cutting-edge innovations in diverse domains. Advances in natural language processing have been driving the fast development in automated information extraction from scientific literature. However, scientific literature is often available in unstructured PDF format. While PDF is great for preserving basic visual elements, such as characters, lines, shapes, etc., on a canvas for presentation to humans, automatic processing of the PDF format by machines presents many challenges. With over 2.5 trillion PDF documents in existence, these issues are prevalent in many other important application domains as well.
A critical challenge for automated information extraction from scientific literature is that documents often contain content that is not in natural language, such as figures and tables. Nevertheless, such content usually illustrates key results, messages, or summarizations of the research. To obtain a comprehensive understanding of scientific literature, the automated system must be able to recognize the layout of the documents and parse the non-natural-language content into a machine readable format.
Our ICDAR 2021 Scientific Literature Parsing Competition (ICDAR2021-SLP) aims to drive the advances specifically in document understanding. ICDAR2021-SLP leverages the PubLayNet and PubTab-Net datasets, which provide hundreds of thousands of training and evaluation examples. In Task A, Document Layout Recognition, submissions with the highest performance combine object detection and specialised solutions for the different categories. In Task B, Table Recognition, top submissions rely on methods to identify table components and post-processing methods to generate the table structure and content. Results from both tasks show an impressive performance and opens the possibility for high performance practical applications.},
	language = {English},
	booktitle = {University of {Melbourne}},
	author = {Yepes, AJ and Zhong, P and Burdick, D},
	editor = {Llados, J and Lopresti, D and Uchida, S},
	year = {2021},
	keywords = {Document layout understanding, HEMDIG - WOS, ICDAR competition, Table recognition},
	pages = {605--617},
}
