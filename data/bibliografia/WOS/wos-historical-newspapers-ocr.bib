
@article{menhour_searchable_2021,
	title = {Searchable {Turkish} {OCRed} historical newspaper collection 1928–1942},
	volume = {49},
	issn = {0165-5515},
	doi = {10.1177/01655515211000642},
	abstract = {The newspaper emerged as a distinct cultural form in early 17th-century Europe. It is bound up with the early modern period of history. Historical newspapers are of utmost importance to nations and its people, and researchers from different disciplines rely on these papers to improve our understanding of the past. In pursuit of satisfying this need, Istanbul University Head Office of Library and Documentation provides access to a big database of scanned historical newspapers. To take it another step further and make the documents more accessible, we need to run optical character recognition (OCR) and named entity recognition (NER) tasks on the whole database and index the results to allow for full-text search mechanism. We design and implement a system encompassing the whole pipeline starting from scrapping the dataset from the original website to providing a graphical user interface to run search queries, and it manages to do that successfully. Proposed system provides to search people, culture and security-related keywords and to visualise them. © The Author(s) 2021.},
	language = {English},
	number = {2},
	journal = {Journal of Information Science},
	author = {Menhour, H. and Şahin, H.B. and Sarıkaya, R.N. and Aktaş, M. and Sağlam, R. and Ekinci, E. and Eken, S.},
	year = {2021},
	note = {Publisher: SAGE Publications Ltd},
	keywords = {Data journalism, Design and implements, full-text search, Full-text search, Graphical user interfaces, HEMDIG - SCOPUS, HEMDIG - WOS, historical newspapers, Historical newspapers, Istanbul, microservices, named entity recognition, Named entity recognition, NAMED ENTITY RECOGNITION, Natural language processing systems, Newsprint, Office buildings, optical character recognition, Optical character recognition, Optical character recognition (OCR), SCOPUS, Search queries, SEGMENTATION, Turkishs, visualisation},
	pages = {335--347},
}

@inproceedings{neudecker_making_2016,
	title = {Making {Europe}'s {Historical} {Newspapers} {Searchable}},
	isbn = {9781509017928 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979556437&doi=10.1109%2fDAS.2016.83&partnerID=40&md5=8f07aa6473046bc026f95a822340502c},
	doi = {10.1109/DAS.2016.83},
	abstract = {This paper provides a rare glimpse into the overall approach for the refinement, i.e. the enrichment of scanned historical newspapers with text and layout recognition, in the Europeana Newspapers project. Within three years, the project processed more than 10 million pages of historical newspapers from 12 national and major libraries to produce the largest open access and fully searchable text collection of digital historical newspapers in Europe. In this, a wide variety of legal, logistical, technical and other challenges were encountered. After introducing the background issues in newspaper digitization in Europe, the paper discusses the technical aspects of refinement in greater detail. It explains what decisions were taken in the design of the large-scale processing workflow to address these challenges, what were the results produced and what were identified as best practices. © 2016 IEEE.},
	language = {English},
	urldate = {2016-04-11},
	booktitle = {Proc. - {IAPR} {Int}. {Workshop} {Doc}. {Anal}. {Syst}., {DAS}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Neudecker, C. and Antonacopoulos, A.},
	year = {2016},
	note = {Journal Abbreviation: Proc. - IAPR Int. Workshop Doc. Anal. Syst., DAS},
	keywords = {Best practices, Character recognition, digital libraries, Digital libraries, Digitisation, Electronic publishing, europeana, HEMDIG - PROJETOS SIMILARES, HEMDIG - SCOPUS, HEMDIG - WOS, HEMDIG FRAMEWORK, historical newspapers, Historical newspapers, large-scale digitisation, Large-scale processing, layout analysis, Layout analysis, newspapers, Newsprint, OCR, OLR, Open Access, optical character recognition, Optical character recognition, SCOPUS, Technical aspects, Text collection},
	pages = {405--410},
	file = {neudecker_antonacopoulos_2016_making europe's historical newspapers searchable.pdf:/home/ebn/pCloudDrive/zot_library/IEEE/2016/neudecker_antonacopoulos_2016_making europe's historical newspapers searchable.pdf:application/pdf;Snapshot:/home/ebn/Zotero/storage/L8533WTD/7490152.html:text/html},
}

@inproceedings{kae_improving_2010,
	address = {San Francisco, CA},
	title = {Improving state-of-the-art {OCR} through high-precision document-specific modeling},
	isbn = {10636919 (ISSN); 9781424469840 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955987982&doi=10.1109%2fCVPR.2010.5539867&partnerID=40&md5=ae0c419734cdac7692fc9fcb27b380ea},
	doi = {10.1109/CVPR.2010.5539867},
	abstract = {Optical character recognition (OCR) remains a difficult problem for noisy documents or documents not scanned at high resolution. Many current approaches rely on stored font models that are vulnerable to cases in which the document is noisy or is written in a font dissimilar to the stored fonts. We address these problems by learning character models directly from the document itself, rather than using pre-stored font models. This method has had some success in the past, but we are able to achieve substantial improvement in error reduction through a novel method for creating nearly error-free document-specific training data and building character appearance models from this data. In particular, we first use the state-of-the-art OCR system Tesseract to produce an initial translation. Then, our method identifies a subset of words that we have high confidence have been recognized correctly and uses this subset to bootstrap document-specific character models. We present theoretical justification that a word in the selected subset is very unlikely to be incorrectly recognized, and empirical results on a data set of difficult historical newspaper scans demonstrating that we make only two errors in 56 documents. We then relax the theoretical constraint in order to create a larger training set, and using document-specific character models generated from this data, we are able to reduce the error over properly segmented characters by 34.1\% overall from the initial Tesseract translation. ©2010 IEEE.},
	language = {English},
	urldate = {2010-06-13},
	booktitle = {Proc {IEEE} {Comput} {Soc} {Conf} {Comput} {Vision} {Pattern} {Recognit}},
	author = {Kae, A. and Huang, G. and Doersch, C. and Learned-Miller, E.},
	year = {2010},
	note = {Journal Abbreviation: Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
	keywords = {Appearance models, Character models, Computational methods, Computer vision, Data sets, Empirical results, Error reduction, HEMDIG - SCOPUS, HEMDIG - WOS, High confidence, High resolution, High-precision, Historical newspapers, Novel methods, Optical character recognition, SCOPUS, Security of data, Tesseract, Training data, Training sets},
	pages = {1935--1942},
}

@inproceedings{terasawa_fast_2011,
	address = {Beijing},
	title = {A fast appearance-based full-text search method for historical newspaper images},
	isbn = {15205363 (ISSN); 9780769545202 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-82355169321&doi=10.1109%2fICDAR.2011.277&partnerID=40&md5=f4fd6408346ced1de800b9ad01bb6b3c},
	doi = {10.1109/ICDAR.2011.277},
	abstract = {This paper presents a fast appearance-based full-text search method for historical newspaper images. Since historical newspapers differ from recent newspapers in image quality, type fonts and language usages, optical character recognition (OCR) does not provide sufficient quality. Instead of OCR approach, we adopted appearance-based approach, that means we matched character to character with its shapes. Assuming proper character segmentation and proper feature description, full-text search problem is reduced to sequence matching problem of feature vector. To increase computational efficiency, we adopted pseudo-code expression called LSPC, which is a compact sketch of feature vector while retaining a good deal of its information. Experimental result showed that our method can retrieve a query string from a text of over eight million characters within a second. In addition, we predict that more sophisticated algorithm could be designed for LSPC. As an example, we established the Extended Boyer-Moore-Horspool algorithm that can reduce the computational cost further especially when the query string becomes longer. © 2011 IEEE.},
	language = {English},
	urldate = {2011-09-18},
	booktitle = {Proc. {Int}. {Conf}. {Doc}. {Anal}. {Recognit}.},
	author = {Terasawa, K. and Shima, T. and Kawashima, T.},
	collaborator = {{TC10 (Graph. Recogn.) TC11 (Read. Syst.) (IAPR); Chinese Academy of Sciences; NSFC; FUJITSU; Hanvon Technology}},
	year = {2011},
	note = {Journal Abbreviation: Proc. Int. Conf. Doc. Anal. Recognit.},
	keywords = {Algorithms, Appearance based, Boyer-Moore-Horspool algorithm, Character segmentation, Codes (symbols), Computational costs, Computational efficiency, Feature description, Feature vectors, Full-text search, HEMDIG - SCOPUS, HEMDIG - WOS, historical document images, Historical documents, Historical newspapers, Image matching, Image quality, Locality-Sensitive Pseudo-Code, Newsprint, Optical character recognition, Pseudo-code, Query string, SCOPUS, Sequence matching, string matching, String matching, word spotting, Word Spotting},
	pages = {1379--1383},
}

@article{huang_bounding_2012,
	title = {Bounding the probability of error for high precision optical character recognition},
	volume = {13},
	issn = {15324435 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857846731&partnerID=40&md5=dde2c8ca41053322491950ffc6c95211},
	abstract = {We consider a model for which it is important, early in processing, to estimate some variables with high precision, but perhaps at relatively low recall. If some variables can be identified with near certainty, they can be conditioned upon, allowing further inference to be done efficiently. Specifically, we consider optical character recognition (OCR) systems that can be bootstrapped by identifying a subset of correctly translated document words with very high precision. This "clean set" is subsequently used as document-specific training data. While OCR systems produce confidence measures for the identity of each letter or word, thresholding these values still produces a significant number of errors. We introduce a novel technique for identifying a set of correct words with very high precision. Rather than estimating posterior probabilities, we bound the probability that any given word is incorrect using an approximate worst case analysis. We give empirical results on a data set of difficult historical newspaper scans, demonstrating that our method for identifying correct words makes only two errors in 56 documents. Using document-specific character models generated from this data, we are able to reduce the error over properly segmented characters by 34.1\% from an initial OCR system's translation.1 © 2012 Gary B. Huang, Andrew Kae, Carl Doersch and Erik Learned-Miller.},
	language = {English},
	journal = {Journal of Machine Learning Research},
	author = {Huang, G.B. and Kae, A. and Doersch, C. and Learned-Miller, E.},
	year = {2012},
	keywords = {Character models, computer vision, Computer vision, Confidence Measure, Data sets, document-specific modeling, Document-specific modeling, Errors, HEMDIG - SCOPUS, HEMDIG - WOS, High precision, Historical newspapers, Novel techniques, optical character recognition, Optical character recognition, Posterior probability, Probability, probability bounding, Probability bounding, Probability of errors, SCOPUS, Thresholding, Training data, Worst-case analysis},
	pages = {363--387},
}

@inproceedings{palfray_plair_2012,
	address = {Copenhagen},
	title = {"{PlaIR}": {A} system to provide full access to digitized newspaper archives},
	isbn = {9780892083008 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876739050&partnerID=40&md5=58f35f22cb351eca4adb61ada6711027},
	abstract = {This paper presents a platform dedicated to the analysis and the online consultation of historical newspaper archives. This platform has been designed to provide a user experience as intuitive as possible by using mature open source tools. All the features are implemented thanks to the Spring framework. To meet this goal, we created a system to display tiled high-resolution images operating without a plug-in but based on an open source solution called IIPImage. The platform also allows for full-text searches thanks to the Java search library Apache Lucene and displays the results in the form of newspaper articles. In addition, we established collaborative features to provide the users with the ability to correct the content automatically generated by our document processing workflow and accessed through the browsing platform. The system is able to store all the corrections of the users, by using the couple Hibernate/MySQL. The aim is to enable continuous improvement of both the content quality and the search accuracy, by exploiting the ability of the users to recognize significant errors, in order to enhance the digital objects representing the newspaper issues. The proposed system is designed to generate metadata describing the physical layout, but also the logical structure of newspaper documents. Our article segmentation analyses a newspaper issue and recognizes articles, even if they straddle more than one page or if they spread in a complex structure. The workflow can also consider as input data, the results of optical character recognition (OCR) engines in order to provide a textual indexation of the segmented articles. By using this system, we want to create a true and representative digital object using standard formats (i.e. METS / ALTO) and containing the logical description of the content, making easier reading and understanding by the users. ©2012 Society for Imaging Science and Technology.},
	language = {English},
	urldate = {2012-06-12},
	booktitle = {Arch. - {Preserv}. {Strateg}. {Imaging} {Technol}. {Cult}. {Herit}. {Inst}. {Mem}. {Organ}. - {Final} {Program} {Proc}.},
	author = {Palfray, T. and Nicolas, S. and Paquet, T. and Tranouez, P.},
	collaborator = {{Society for Imaging Science and Technology (IS and T); MAM-A; Nationalmuseet}},
	year = {2012},
	note = {Journal Abbreviation: Arch. - Preserv. Strateg. Imaging Technol. Cult. Herit. Inst. Mem. Organ. - Final Program Proc.},
	keywords = {Automatically generated, Continuous improvements, Document-processing, HEMDIG - SCOPUS, HEMDIG - WOS, High resolution image, Historical newspapers, Imaging techniques, Metadata, Newsprint, Open systems, Open-source solutions, Optical character recognition, Optical character recognition engines, SCOPUS, Segmentation analysis, Societies and institutions},
	pages = {48--53},
}

@inproceedings{konya_character_2011,
	address = {Beijing},
	title = {Character enhancement for historical newspapers printed using hot metal typesetting},
	isbn = {15205363 (ISSN); 9780769545202 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-82355168372&doi=10.1109%2fICDAR.2011.190&partnerID=40&md5=bf1ed4cf1a0ca788232b9e7813020465},
	doi = {10.1109/ICDAR.2011.190},
	abstract = {We propose a new method for an effective removal of the printing artifacts occurring in historical newspapers which are caused by problems in the hot metal typesetting, a widely used printing technique in the late 19th and early 20th century. Such artifacts typically appear as thin lines between single characters or glyphs and are in most cases connected to one of the neighboring characters. The quality of the optical character recognition (OCR) is heavily influenced by this type of printing artifacts. The proposed method is based on the detection of (near) vertical segments by means of directional single-connected chains (DSCC). In order to allow the robust processing of complex decorative fonts such as Fraktur, a set of rules is introduced. This allows us to successfully process prints exhibiting artifacts with a stroke width even higher than that of most thin characters stems. We evaluate our approach on a dataset consisting of old newspaper excerpts printed using Fraktur fonts. The recognition results on the enhanced images using two independent OCR engines (ABBYY Fine Reader and Tesseract) show significant improvements over the originals. © 2011 IEEE.},
	language = {English},
	urldate = {2011-09-18},
	booktitle = {Proc. {Int}. {Conf}. {Doc}. {Anal}. {Recognit}.},
	author = {Konya, I. and Eickeler, S. and Seibert, C.},
	collaborator = {{TC10 (Graph. Recogn.) TC11 (Read. Syst.) (IAPR); Chinese Academy of Sciences; NSFC; FUJITSU; Hanvon Technology}},
	year = {2011},
	note = {Journal Abbreviation: Proc. Int. Conf. Doc. Anal. Recognit.},
	keywords = {20th century, character enhancement, Data processing, Data sets, HEMDIG - SCOPUS, HEMDIG - WOS, historical documents, Historical documents, Historical newspapers, Hot metal, hot metal typesetting, Newsprint, OCR, OCR engines, Old newspapers, Optical character recognition, Printing techniques, retro-digitization, Robust processing, SCOPUS, Set of rules, Tesseract, Typesetting},
	pages = {936--940},
}

@article{jarvelin_information_2016,
	title = {Information retrieval from historical newspaper collections in highly inflectional languages: {A} query expansion approach},
	volume = {67},
	issn = {23301635 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995514103&doi=10.1002%2fasi.23379&partnerID=40&md5=12fa38eeabf192ce98b1158889c02054},
	doi = {10.1002/asi.23379},
	abstract = {The aim of the study was to test whether query expansion by approximate string matching methods is beneficial in retrieval from historical newspaper collections in a language rich with compounds and inflectional forms (Finnish). First, approximate string matching methods were used to generate lists of index words most similar to contemporary query terms in a digitized newspaper collection from the 1800s. Top index word variants were categorized to estimate the appropriate query expansion ranges in the retrieval test. Second, the effectiveness of approximate string matching methods, automatically generated inflectional forms, and their combinations were measured in a Cranfield-style test. Finally, a detailed topic-level analysis of test results was conducted. In the index of historical newspaper collection the occurrences of a word typically spread to many linguistic and historical variants along with optical character recognition (OCR) errors. All query expansion methods improved the baseline results. Extensive expansion of around 30 variants for each query word was required to achieve the highest performance improvement. Query expansion based on approximate string matching was superior to using the inflectional forms of the query words, showing that coverage of the different types of variation is more important than precision in handling one type of variation. © 2015 ASIS\&T},
	language = {English},
	number = {12},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Järvelin, A. and Keskustalo, H. and Sormunen, E. and Saastamoinen, M. and Kettunen, K.},
	year = {2016},
	note = {Publisher: John Wiley and Sons Inc.},
	keywords = {Approximate string matching, Automatic test pattern generation, Automatically generated, Baseline results, Character recognition, error, HEMDIG - SCOPUS, HEMDIG - WOS, Historical newspapers, human, human experiment, information retrieval, Information retrieval, language, MANAGEMENT, N-GRAMS, Newsprint, Optical character recognition, Optical character recognition (OCR), publication, query expansion, Query expansion, query processing, Query processing, Query terms, Query words, recognition, SCOPUS, TEXT RETRIEVAL, WORDS},
	pages = {2928--2946},
}

@inproceedings{xu_retrieving_2017,
	title = {Retrieving and {Combining} {Repeated} {Passages} to {Improve} {OCR}},
	isbn = {15525996 (ISSN); 9781538638613 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028025577&doi=10.1109%2fJCDL.2017.7991587&partnerID=40&md5=f9f4917706b39d9661899847fb81cc67},
	doi = {10.1109/JCDL.2017.7991587},
	abstract = {We present a novel approach to improve the output of optical character recognition (OCR) systems by first detecting duplicate passages in their output and then performing consensus decoding combined with a language model. This approach is orthogonal to, and may be combined with, previously proposed methods for combining the output of different OCR systems on the same image or the output of the same OCR system on differently processed images of the same text. It may also be combined with methods to estimate the parameters of a noisy channel model of OCR errors. Additionally, the current method generalizes previous proposals for a simple majority- vote combination of known duplicated texts. On a corpus of historical newspapers, an annotated set of clusters has a baseline word error rate (WER) of 33\%. A majority vote procedure reaches 23\% on passages where one or more duplicates were found, and consensus decoding combined with a language model achieves 18\% WER. In a separate experiment, newspapers were aligned to very widely reprinted texts such as State of the Union speeches, producing clusters with up to 58 witnesses. Beyond 20 witnesses, majority vote outperforms language model rescoring, though the gap between them is much less in this experiment. © 2017 IEEE.},
	language = {English},
	urldate = {2017-06-19},
	booktitle = {Proc. {ACM} {IEEE} {Joint} {Conf}. {Digit}. {Libr}.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Xu, S. and Smith, D.},
	collaborator = {{Elsevier; et al.; IEEE; Libraries Bloomington; University of Toronto Faculty of Information; University of Toronto Libraries}},
	year = {2017},
	note = {Journal Abbreviation: Proc. ACM IEEE Joint Conf. Digit. Libr.},
	keywords = {Character recognition, Computational linguistics, Decoding, Digital libraries, HEMDIG - SCOPUS, HEMDIG - WOS, Historical newspapers, Language model, Majority vote, Newsprint, Noisy channel models, Optical character recognition, Optical character recognition (OCR), Processed images, SCOPUS, Simple majority, Speech recognition, Word error rate},
}

@inproceedings{ito_extraction_2020,
	title = {Extraction of distinctive keywords and articles from untranscribed historical newspaper images},
	volume = {11515},
	isbn = {0277786X (ISSN); 9781510638358 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086628923&doi=10.1117%2f12.2566612&partnerID=40&md5=d5438781affedd4d6c417b7097acab60},
	doi = {10.1117/12.2566612},
	abstract = {This paper proposes a novel approach to extract distinctive keywords from historical newspaper images without using character recognition. We converted an image of the text block on an entire newspaper page into a sequence of codes based on discretization of the feature vectors, an approach that eliminated the errors introduced by optical character recognition (OCR). This conversion makes it possible to analyze untranscribed newspaper images by using text-processing methods. We examined the daily occurrence of every tri-gram string, and extracted strings with a dense appearance as distinctive keywords. In addition, we highlighted articles that contain distinctive keywords as distinctive articles. The proposed method was evaluated on an archive of Japanese newspaper images published in the 19th century, and the results were promising. © 2020 SPIE CCC.},
	language = {English},
	urldate = {2020-01-05},
	booktitle = {Proc {SPIE} {Int} {Soc} {Opt} {Eng}},
	publisher = {SPIE},
	author = {Ito, S. and Terasawa, K.},
	editor = {{Lau P.Y.} and {Shobri M.}},
	year = {2020},
	note = {Journal Abbreviation: Proc SPIE Int Soc Opt Eng},
	keywords = {19th century, Discretizations, Feature vectors, HEMDIG - SCOPUS, HEMDIG - WOS, historical documents, Historical documents, historical newspapers, Historical newspapers, Imaging techniques, keyword extraction, Keyword extraction, Newsprint, Optical character recognition, Optical character recognition (OCR), SCOPUS, Text processing, Tri grams},
}

@inproceedings{anderson_segmenting_2020,
	title = {Segmenting messy text: {Detecting} boundaries in text derived from historical newspaper images},
	isbn = {10514651 (ISSN); 9781728188089 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110522850&doi=10.1109%2fICPR48806.2021.9413279&partnerID=40&md5=36c913ef41ad50aa0202f5d15965e617},
	doi = {10.1109/ICPR48806.2021.9413279},
	abstract = {Text segmentation, the task of dividing a document into sections, is often a prerequisite for performing additional natural language processing tasks. Existing text segmentation methods have typically been developed and tested using clean, narrative-style text with segments containing distinct topics. Here we consider a challenging text segmentation task: dividing newspaper marriage announcement lists into units of one announcement each. In many cases the information is not structured into sentences, and adjacent segments are not topically distinct from each other. In addition, the text of the announcements, which is derived from images of historical newspapers via optical character recognition, contains many typographical errors. As a result, these announcements are not amenable to segmentation with existing techniques. We present a novel deep learning-based model for segmenting such text and show that it significantly outperforms an existing state-of-the-art method on our task. © 2020 IEEE},
	language = {English},
	urldate = {2021-01-10},
	booktitle = {Proc. {Int}. {Conf}. {Pattern} {Recognit}.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Anderson, C. and Crone, P.},
	year = {2020},
	note = {Journal Abbreviation: Proc. Int. Conf. Pattern Recognit.},
	keywords = {deep learning, Deep learning, document analysis, Document analysis, HEMDIG - SCOPUS, HEMDIG - WOS, Historical newspapers, information extraction, Information extraction, Learning Based Models, natural language processing, Natural language processing, NAtural language processing, Natural language processing systems, Newsprint, Optical character recognition, SCOPUS, State-of-the-art methods, text mining, Text mining, text segmentation, Text segmentation, Typographical errors},
	pages = {5543--5550},
}

@article{pack_visual_2021,
	title = {Visual domain knowledge-based multimodal zoning for textual region localization in noisy historical document images},
	volume = {30},
	issn = {10179909 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122680308&doi=10.1117%2f1.JEI.30.6.063028&partnerID=40&md5=41c5af1b2f0dcef5c9a6d42b754cee6e},
	doi = {10.1117/1.JEI.30.6.063028},
	abstract = {Document layout analysis, or zoning, is important for textual content analysis such as optical character recognition. Zoning document images such as digitized historical newspaper pages are challenging due to noise and quality of the document images. Recently, effective data-driven approaches, such as leveraging deep learning, have been proposed, albeit with the concern of requiring larger training data and thus incurring additional cost of ground truthing. We propose a zoning solution by incorporating a knowledge-driven document representation, gravity map, into a multimodal deep learning framework to reduce the amount of time and data required for training. We first generate a gravity map for each image, considering the centroid distance and area between a cell in a Voronoi tessellation and its content to encode visual domain knowledge of a zoning task. Second, we inject the gravity maps into a deep convolution neural network (DCNN) during training, as an additional modality to boost performance. We report on two investigations using two state-of-the-art DCNN architectures and three datasets: two sets of historical newspapers and a set of born-digital contemporary documents. Evaluations show that our solution achieved comparable segmentation accuracy using fewer training epochs and less training data compared to a naïve training scheme. © 2021 SPIE and IS\&T.},
	language = {English},
	number = {6},
	journal = {Journal of Electronic Imaging},
	author = {Pack, C. and Soh, L.-K. and Lorang, E.},
	year = {2021},
	note = {Publisher: SPIE},
	keywords = {ALGORITHM, Deep learning, document image processing, Document image processing, Document images, Domain knowledge, Domain Knowledge, Gravity maps, HEMDIG - SCOPUS, HEMDIG - WOS, Historical newspapers, image analysis, image decomposition, Image decomposition, image recognition, Image recognition, image segmentation, Image segmentation, Image-analysis, Images segmentations, Multi-modal, Newsprint, Optical character recognition, Optical data processing, SCOPUS, SEGMENTATION, Training data, Zoning},
}

@article{kettunen_optical_2023,
	title = {Optical character recognition quality affects subjective user perception of historical newspaper clippings},
	volume = {79},
	issn = {0022-0418},
	doi = {10.1108/JD-01-2023-0002},
	abstract = {PurposeThis study aims to identify user perception of different qualities of optical character recognition (OCR) in texts. The purpose of this paper is to study the effect of different quality OCR on users' subjective perception through an interactive information retrieval task with a collection of one digitized historical Finnish newspaper.Design/methodology/approachThis study is based on the simulated work task model used in interactive information retrieval. Thirty-two users made searches to an article collection of Finnish newspaper Uusi Suometar 1869-1918 which consists of ca. 1.45 million autosegmented articles. The article search database had two versions of each article with different quality OCR. Each user performed six pre-formulated and six self-formulated short queries and evaluated subjectively the top 10 results using a graded relevance scale of 0-3. Users were not informed about the OCR quality differences of the otherwise identical articles.FindingsThe main result of the study is that improved OCR quality affects subjective user perception of historical newspaper articles positively: higher relevance scores are given to better-quality texts.Originality/valueTo the best of the authors' knowledge, this simulated interactive work task experiment is the first one showing empirically that users' subjective relevance assessments are affected by a change in the quality of an optically read text.},
	language = {English},
	number = {7},
	journal = {JOURNAL OF DOCUMENTATION},
	author = {Kettunen, K and Keskustalo, H and Kumpulainen, S and Paakkonen, T and Rautiainen, J},
	month = may,
	year = {2023},
	keywords = {Evaluation, Finnish, HEMDIG - WOS, Historical newspapers, INFORMATION-RETRIEVAL, Interactive information retrieval, OCR quality, PAPER, Simulated work task},
	pages = {137--156},
}

@inproceedings{kettunen_keep_2016,
	title = {Keep, {Change} or {Delete}? {Setting} up a {Low} {Resource} {OCR} {Post}-correction {Framework} for a {Digitized} {Old} {Finnish} {Newspaper} {Collection}},
	volume = {612},
	isbn = {1865-0929},
	doi = {10.1007/978-3-319-41938-1_11},
	abstract = {There has been a huge interest in digitization of both hand-written and printed historical material in the last 10-15 years and most probably this interest will only increase in the ongoing Digital Humanities era. As a result of the interest we have lots of digital historical document collections available and will have more of them in the future.
The National Library of Finland has digitized a large proportion of the historical newspapers published in Finland between 1771 and 1910 [1-3]; the collection, Digi, can be reached at http://digi.kansalliskirjasto.fi/. This collection contains approximately 1.95 million pages in Finnish and Swedish, the Finnish part being about 2.385 billion words. In the output of the Optical Character Recognition (OCR) process, errors are common especially when the texts are printed in the Gothic (Fraktur, blackletter) typeface. The errors lower the usability of the corpus both from the point of view of human users as well as considering possible elaborated text mining applications. Automatic spell checking and correction of the data is also difficult due to the historical spelling variants and low OCR quality level of the material.
This paper discusses the overall situation of the intended post-correction of the Digi content and evaluation of the correction. We shall present results of our post-correction trials, and discuss some aspects of methodology of evaluation. These are the first reported evaluation results of post-correction of the data and the experiences will be used in planning of the post-correction of the whole material.},
	language = {English},
	author = {Kettunen, K},
	editor = {Calvanese, D and DeNart, D and Tasso, C},
	year = {2016},
	keywords = {Evaluation, HEMDIG - WOS, Historical newspaper collections, OCR post-correction, TEXT},
	pages = {95--103},
}

@inproceedings{beshirov_duosearch_2022,
	title = {{DuoSearch}: {A} {Novel} {Search} {Engine} for {Bulgarian} {Historical} {Documents}},
	volume = {13186},
	isbn = {0302-9743},
	doi = {10.1007/978-3-030-99739-7_31},
	abstract = {Search in collections of digitised historical documents is hindered by a two-prong problem, orthographic variety and optical character recognition (OCR) mistakes. We present a new search engine for historical documents, DuoSearch, which uses ElasticSearch and machine learning methods based on deep neural networks to offer a solution to this problem. It was tested on a collection of historical newspapers in Bulgarian from the mid-19th to the mid-20th century. The system provides an interactive and intuitive interface for the end-users allowing them to enter search terms in modern Bulgarian and search across historical spellings. This is the first solution facilitating the use of digitised historical documents in Bulgarian.},
	language = {English},
	booktitle = {University of {Sofia}},
	author = {Beshirov, A and Hadzhieva, S and Koychev, I and Dobreva, M},
	editor = {Hagen, M and Verberne, S and Macdonald, C and Seifert, C and Balog, K and Norvag, K and Setty, V},
	year = {2022},
	keywords = {BERT, HEMDIG - WOS, Historical newspapers search engine, Orthographic variety, Post-OCR text correction},
	pages = {265--269},
}

@inproceedings{torao-pingi_understanding_2015,
	title = {Understanding {People} {Relationship}: {Analysis} of {Digitised} {Historical} {Newspaper} {Articles}},
	volume = {9457},
	isbn = {0302-9743},
	doi = {10.1007/978-3-319-26350-2_51},
	abstract = {The study of historical persons and their relationships gives an insight into the lives of people and the way society functioned in early times. Such information concerning Australian history can be gleaned from Trove's digitized collection of historical newspapers (1803-1954). This research aims to mine Trove's articles using closed and maximal association rules mining along with visualization tools to discover, conceptualize and understand the type, size and complexity of the notable relationships that existed between persons in historical Australia. Before the articles could be mined, they needed vigorous cleaning. Given the data's source, type and extraction methods, estimated word-error rates were at 50-75 \%. Pre-processing efforts were aimed at reducing errors originating from optical character recognition (OCR), natural language processing and some co-referencing both within and between articles. Only after cleaning were the datasets able to return interesting associations at higher support thresholds.},
	language = {English},
	booktitle = {University of {Papua} {New} {Guinea}},
	author = {Torao-Pingi, S and Nayak, R},
	editor = {Pfahringer, B and Renz, J},
	year = {2015},
	keywords = {Association rule mining, HEMDIG - WOS, Natural language processing, OCR errors},
	pages = {572--588},
}

@inproceedings{koistinen_how_2020,
	title = {How to {Improve} {Optical} {Character} {Recognition} of {Historical} {Finnish} {Newspapers} {Using} {Open} {Source} {Tesseract} {OCR} {Engine} - {Final} {Notes} on {Development} and {Evaluation}},
	volume = {12598},
	isbn = {0302-9743},
	doi = {10.1007/978-3-030-66527-2_2},
	abstract = {The current paper presents work that has been carried out in the National Library of Finland (NLF) to improve optical character recognition (OCR) quality of the historical Finnish newspaper collection 1771-1910. Evaluation results reported in the paper are based mainly on a 500 000 word sample of the Finnish language part of the whole collection. The sample has three different parallel parts: a manually corrected ground truth version, original OCR with ABBYY FineReader v. 7 or v. 8, and an ABBYY FineReader v. 11 re-OCRed version for comparison with Tesseract's OCR. Using this sample and its page image originals we have developed a re-OCRing procedure using the open source software package Tesseract v. 3.04.01. Our method achieved initially 27.48\% improvement vs. ABBYY FineReader 7 or 8 and 9.16\% improvement vs. ABBYY FineReader 11 on document level. On word level our method achieved 36.25\% improvement vs. ABBYY FineReader 7 or 8 and 20.14\% improvement vs. ABBYY FineReader 11. Our final precision and recall results on word level show clear improvement in the quality: recall is 76.0 and precision 92.0 in comparison to GT OCR. Other measures, such as recognizability of words with a morphological analyzer and character accuracy rate, show also steady improvement after re-OCRing.},
	language = {English},
	author = {Koistinen, M and Kettunen, K and Kervinen, J},
	editor = {Vetulani, Z and Paroubek, P and Kubis, M},
	year = {2020},
	keywords = {Evaluation, Finnish, HEMDIG - WOS, Historical newspaper collections, Optical character recognition, POST-CORRECTION, RETRIEVAL},
	pages = {17--30},
}
