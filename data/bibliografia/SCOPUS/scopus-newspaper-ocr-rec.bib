
@article{menhour_searchable_2021,
	title = {Searchable {Turkish} {OCRed} historical newspaper collection 1928–1942},
	issn = {01655515 (ISSN)},
	doi = {10.1177/01655515211000642},
	abstract = {The newspaper emerged as a distinct cultural form in early 17th-century Europe. It is bound up with the early modern period of history. Historical newspapers are of utmost importance to nations and its people, and researchers from different disciplines rely on these papers to improve our understanding of the past. In pursuit of satisfying this need, Istanbul University Head Office of Library and Documentation provides access to a big database of scanned historical newspapers. To take it another step further and make the documents more accessible, we need to run optical character recognition (OCR) and named entity recognition (NER) tasks on the whole database and index the results to allow for full-text search mechanism. We design and implement a system encompassing the whole pipeline starting from scrapping the dataset from the original website to providing a graphical user interface to run search queries, and it manages to do that successfully. Proposed system provides to search people, culture and security-related keywords and to visualise them. © The Author(s) 2021.},
	language = {English},
	journal = {Journal of Information Science},
	author = {Menhour, H. and Şahin, H.B. and Sarıkaya, R.N. and Aktaş, M. and Sağlam, R. and Ekinci, E. and Eken, S.},
	year = {2021},
	note = {Publisher: SAGE Publications Ltd},
	keywords = {Data journalism, Design and implements, full-text search, Full-text search, Graphical user interfaces, HEMDIG - SCOPUS, historical newspapers, Historical newspapers, Istanbul, microservices, named entity recognition, Named entity recognition, Natural language processing systems, Newsprint, Office buildings, optical character recognition, Optical character recognition, Optical character recognition (OCR), SCOPUS, Search queries, Turkishs, visualisation},
}

@inproceedings{ahonen_publishing_2009,
	address = {Berkeley, CA},
	title = {Publishing historical texts on the semantic web - {A} case study},
	isbn = {9780769538006 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-73449099947&doi=10.1109%2fICSC.2009.9&partnerID=40&md5=5d790750349018eb5f66c51e8566a384},
	doi = {10.1109/ICSC.2009.9},
	abstract = {Historical texts are an important component of cultural heritage, and are being digitized and published on the web in various portals for the researchers and the public. However, searching and linking them with related contents is challenging due to the non-structured text form, digitization errors, and the differences and variations between old and modern language, including historical names (e.g. places), used for querying. This paper addresses these issues by presenting an approach and a system for publishing old texts on the semantic web. As a case study, an existing historical newspaper archive on the web is considered. In our model, semantic metadata is added to the text using automated concept extraction methods. Search is implemented with semantic techniques, by creating a multi-faceted search interface for the text materials. Problems due to OCR errors and spelling variants are addressed with a fuzzy string matching algorithm trying to guess corresponding words in a lexicon, and giving suggestions for corrected word forms. References between texts in the library as well as links between the library and external knowledge sources are formed by using shared ontologies for semantic annotations. © 2009 IEEE.},
	language = {English},
	urldate = {2009-09-14},
	booktitle = {{ICSC} - {IEEE} {Int}. {Conf}. {Semant}. {Comput}.},
	author = {Ahonen, E. and Hyvönen, E.},
	year = {2009},
	note = {Journal Abbreviation: ICSC - IEEE Int. Conf. Semant. Comput.},
	keywords = {AS-links, Automatic semantic annotation, Concept extraction, Cultural heritages, Data communication systems, Errors, External knowledge, Faceted search, HEMDIG - SCOPUS, Historical newspapers, Metadata, Modern languages, Multi-faceted search, Newsprint, Ontology, Related content, Research, SCOPUS, Semantic annotations, Semantic metadata, Semantic Web, Semantics, Shared ontologies, String matching algorithm, Structured text, Text materials},
	pages = {167--173},
}

@inproceedings{kettunen_ocr_2022,
	title = {{OCR} {Quality} {Affects} {Perceived} {Usefulness} of {Historical} {Newspaper} {Clippings}. {A} {User} {Study}},
	volume = {3160},
	isbn = {16130073 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134238849&partnerID=40&md5=9f7e20ebd018d0437bbba585d1eda714},
	abstract = {Effects of Optical Character Recognition (OCR) quality on historical information retrieval have so far been studied in data-oriented scenarios regarding the effectiveness of retrieval results. Such studies have either focused on the effects of artificially degraded OCR quality (see, e.g., [1-2]) or utilized test collections containing texts based on authentic low quality OCR data (see, e.g., [3]). In this paper the effects of OCR quality are studied in a user-oriented information retrieval setting. Thirty-two users evaluated subjectively query results of six topics each (out of 30 topics) based on pre-formulated queries using a simulated work task setting. To the best of our knowledge our simulated work task experiment is the first one showing empirically that users' subjective relevance assessments of retrieved documents are affected by a change in the quality of optically read text. Users of historical newspaper collections have so far commented effects of OCR'ed data quality mainly in impressionistic ways, and controlled user environments for studying effects of OCR quality on users' relevance assessments of the retrieval results have so far been missing. To remedy this The National Library of Finland (NLF) set up an experimental query environment for the contents of one Finnish historical newspaper, Uusi Suometar 1869-1918, to be able to compare users' evaluation of search results of two different OCR qualities for digitized newspaper articles. The query interface was able to present the same underlying document for the user based on two alternatives: either based on the lower OCR quality, or based on the higher OCR quality, and the choice was randomized. The users did not know about quality differences in the article texts they evaluated. The main result of the study is that improved optical character recognition quality affects perceived usefulness of historical newspaper articles significantly. The mean average evaluation score for the improved OCR results was 7.94\% higher than the mean average evaluation score of the old OCR results. © 2022 Copyright for this paper by its authors.},
	language = {English},
	urldate = {2022-02-24},
	booktitle = {{CEUR} {Workshop} {Proc}.},
	publisher = {CEUR-WS},
	author = {Kettunen, K. and Keskustalo, H. and Kumpulainen, S. and Pääkkönen, T. and Rautiainen, J.},
	editor = {{Di Nunzio G.M.} and {Portelli B.} and {Redavid D.} and {Silvello G.}},
	year = {2022},
	note = {Journal Abbreviation: CEUR Workshop Proc.},
	keywords = {evaluation, Evaluation, HEMDIG - SCOPUS, historical newspapers, Historical newspapers, Information retrieval, Interactive information search, Newsprint, OCR quality, Optical character recognition, Optical character recognition quality, Perceived usefulness, Quality control, query engine, Query engines, Relevance assessments, SCOPUS, Search engines, simulated work task, Simulated work task, User study, Work task},
}

@inproceedings{neudecker_making_2016,
	title = {Making {Europe}'s {Historical} {Newspapers} {Searchable}},
	isbn = {9781509017928 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979556437&doi=10.1109%2fDAS.2016.83&partnerID=40&md5=8f07aa6473046bc026f95a822340502c},
	doi = {10.1109/DAS.2016.83},
	abstract = {This paper provides a rare glimpse into the overall approach for the refinement, i.e. the enrichment of scanned historical newspapers with text and layout recognition, in the Europeana Newspapers project. Within three years, the project processed more than 10 million pages of historical newspapers from 12 national and major libraries to produce the largest open access and fully searchable text collection of digital historical newspapers in Europe. In this, a wide variety of legal, logistical, technical and other challenges were encountered. After introducing the background issues in newspaper digitization in Europe, the paper discusses the technical aspects of refinement in greater detail. It explains what decisions were taken in the design of the large-scale processing workflow to address these challenges, what were the results produced and what were identified as best practices. © 2016 IEEE.},
	language = {English},
	urldate = {2016-04-11},
	booktitle = {Proc. - {IAPR} {Int}. {Workshop} {Doc}. {Anal}. {Syst}., {DAS}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Neudecker, C. and Antonacopoulos, A.},
	year = {2016},
	note = {Journal Abbreviation: Proc. - IAPR Int. Workshop Doc. Anal. Syst., DAS},
	keywords = {Best practices, Character recognition, digital libraries, Digital libraries, Digitisation, Electronic publishing, europeana, HEMDIG - PROJETOS SIMILARES, HEMDIG - SCOPUS, HEMDIG FRAMEWORK, historical newspapers, Historical newspapers, large-scale digitisation, Large-scale processing, layout analysis, Layout analysis, newspapers, Newsprint, OCR, OLR, Open Access, optical character recognition, Optical character recognition, SCOPUS, Technical aspects, Text collection},
	pages = {405--410},
	file = {neudecker_antonacopoulos_2016_making europe's historical newspapers searchable.pdf:/home/ebn/pCloudDrive/zot_library/IEEE/2016/neudecker_antonacopoulos_2016_making europe's historical newspapers searchable.pdf:application/pdf;Snapshot:/home/ebn/Zotero/storage/L8533WTD/7490152.html:text/html},
}

@inproceedings{liebl_historical_2020,
	address = {Amsterdam},
	title = {From historical newspapers to machine-readable data: {The} origami {OCR} pipeline},
	volume = {2723},
	isbn = {16130073 (ISSN)},
	url = {http://ceur-ws.org/Vol-2723/long20.pdf},
	abstract = {While historical newspapers recently have gained a lot of attention in the digital humanities, transforming them into machine-readable data by means of OCR poses some major challenges. In order to address these challenges, we have developed an end-to-end OCR pipeline named Origami. This pipeline is part of a current project on the digitization and quantitative analysis of the German newspaper "Berliner Börsen-Zeitung"(BBZ), from 1872 to 1931. The Origami pipeline reuses existing open source OCR components and on top offers a new configurable architecture for layout detection, a simple table recognition, a two-stage X-Y cut for reading order detection, and a new robust implementation for document dewarping. In this paper we describe the different stages of the workflow and discuss how they meet the above-mentioned challenges posed by historical newspapers. © 2020 Copyright for this paper by its authors.},
	language = {en},
	urldate = {2020-11-18},
	booktitle = {{CEUR} {Workshop} {Proc}.},
	publisher = {CEUR-WS},
	author = {Liebl, B. and Burghardt, M.},
	editor = {{Karsdorp F.} and {McGillivray B.} and {Nerghes A.} and {Wevers M.}},
	year = {2020},
	note = {Journal Abbreviation: CEUR Workshop Proc.},
	keywords = {Configurable architectures, Current projects, Deep neural networks, Different stages, Digital humanities, digital tool, End-to-end OCR, German newspapers, HEMDIG - SCOPUS, HEMDIG FRAMEWORK, Historical newspapers, Humanities computing, Layout detection, Metadata, newspapers, Newsprint, OCR, OLR, Order detections, origami ocr pipeline, Pipelines, Readable data, SCOPUS},
	pages = {351--373},
	file = {liebl_burghardt_from_historical_newspapers_to_machine-readable_data.pdf:/home/ebn/pCloudDrive/zot_library/undefined/undefined/liebl_burghardt_from_historical_newspapers_to_machine-readable_data.pdf:application/pdf},
}

@inproceedings{pletschacher_europeana_2015,
	title = {Europeana newspapers {OCR} workflow evaluation},
	isbn = {9781450336024 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014883617&doi=10.1145%2f2809544.2809554&partnerID=40&md5=9106dbb1686730d5f3b8f2b30d737722},
	doi = {10.1145/2809544.2809554},
	abstract = {This paper summarises the final performance evaluation results of the OCR workflow which was employed for large-scale production in the Europeana Newspapers project. It gives a detailed overview of how the involved software performed on a representative dataset of historical newspaper pages (for which ground truth was created) with regard to general text accuracy as well as layout-related factors which have an impact on how the material can be used in specific use scenarios. Specific types of errors are examined and evaluated in order to identify possible improvements related to the employed document image analysis and recognition methods. Moreover, alternatives to the standard production workflow are assessed to determine future directions and give advice on best practice related to OCR projects. © 2015 ACM.},
	language = {English},
	urldate = {2015-08-22},
	booktitle = {Proceedings of the 3rd international workshop on historical document imaging and processing},
	publisher = {Association for Computing Machinery},
	author = {Pletschacher, S. and Clausner, C. and Antonacopoulos, A.},
	collaborator = {{FamilySearch}},
	year = {2015},
	note = {Journal Abbreviation: ACM Int. Conf. Proc. Ser.},
	keywords = {Document image analysis and recognition, europeana, evaluation, Evaluation results, HEMDIG - PROJETOS SIMILARES, HEMDIG - SCOPUS, HEMDIG FRAMEWORK, Historical documents, Historical newspapers, History, Image processing, Large scale productions, newspapers, Newspapers, Newsprint, OCR, Optical character recognition, Performance evaluation, Production workflows, SCOPUS, workflow},
	pages = {39--46},
	file = {pletschacheret al_2015_europeana newspapers ocr workflow evaluation.pdf:/home/ebn/pCloudDrive/zot_library/undefined/2015/pletschacheret al_2015_europeana newspapers ocr workflow evaluation.pdf:application/pdf;Snapshot:/home/ebn/Zotero/storage/8BZCEDCV/2809544.html:text/html},
}

@book{beshirov_duosearch_2022,
	series = {44th {European} {Conference} on {Information} {Retrieval}, {ECIR} 2022},
	title = {{DuoSearch}: {A} {Novel} {Search} {Engine} for {Bulgarian} {Historical} {Documents}},
	volume = {13186 LNCS},
	isbn = {03029743 (ISSN); 9783030997380 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128757275&doi=10.1007%2f978-3-030-99739-7_31&partnerID=40&md5=b4128bd267c85d77a1fc847f41f198e0},
	abstract = {Search in collections of digitised historical documents is hindered by a two-prong problem, orthographic variety and optical character recognition (OCR) mistakes. We present a new search engine for historical documents, DuoSearch, which uses ElasticSearch and machine learning methods based on deep neural networks to offer a solution to this problem. It was tested on a collection of historical newspapers in Bulgarian from the mid-19th to the mid-20th century. The system provides an interactive and intuitive interface for the end-users allowing them to enter search terms in modern Bulgarian and search across historical spellings. This is the first solution facilitating the use of digitised historical documents in Bulgarian. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	language = {English},
	urldate = {2022-04-10},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Beshirov, A. and Hadzhieva, S. and Koychev, I. and Dobreva, M.},
	editor = {{Hagen M.} and {Verberne S.} and {Macdonald C.} and {Seifert C.} and {Balog K.} and {Norvag K.} and {Setty V.}},
	year = {2022},
	doi = {10.1007/978-3-030-99739-7_31},
	note = {Journal Abbreviation: Lect. Notes Comput. Sci.
Pages: 269
Publication Title: Lect. Notes Comput. Sci.},
	keywords = {20th century, BERT, Deep neural networks, HEMDIG - SCOPUS, Historical documents, Historical newspaper search engine, Historical newspapers, Historical newspapers search engine, History, Interactive interfaces, Intuitive interfaces, Machine learning methods, Newsprint, Optical character recognition, Orthographic variety, Post-OCR text correction, Post-optical character recognition text correction, SCOPUS, Search engines},
}

@inproceedings{tolonen_m_ceur_2018,
	title = {{CEUR} {Workshop} {Proceedings}},
	volume = {2084},
	isbn = {16130073 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045298658&partnerID=40&md5=3bec9a3d6c5fe4c00af1fdf4801855cb},
	abstract = {The proceedings contain 52 papers. The topics discussed include: verifying the consistency of the digitized Indo-European sound law system generating the data of the 120 most archaic languages from proto-Indo-European; sentimentator: gamifying fine-grained sentiment annotation; skin tone emoji and sentiment on twitter; towards an open science infrastructure for the digital humanities: the case of CLARIN; Zelige door on golborne road: exploring the design of a multisensory interface for arts, migration and critical heritage studies; creating and using ground truth OCR sample data for Finnish historical newspapers and journals; emerging language spaces learned from massively multilingual corpora; and Aalto observatory for digital valuation systems.},
	language = {English},
	urldate = {2018-03-07},
	booktitle = {{CEUR} {Workshop} {Proc}.},
	publisher = {CEUR-WS},
	editor = {{Tolonen M.} and {Tuominen J.} and {Makela E.}},
	year = {2018},
	note = {Journal Abbreviation: CEUR Workshop Proc.},
	keywords = {HEMDIG - SCOPUS, SCOPUS},
}

@inproceedings{klaus_can_2020,
	title = {Can umlauts ruin your research in digitized newspaper collections? {A} newseye case study on ‘{The} dark sides of war’ (1914–1918)},
	volume = {2612},
	isbn = {16130073 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086073635&partnerID=40&md5=e52a70aabcf2494dc7d25773a94a3164},
	abstract = {Digitized newspaper collections facilitate the access to historical newspapers. Even though they offer several useful possibilities regarding the research in historical newspapers and magazines, the (automatic) research in these collections is (still) full of limitations and pitfalls. Based on the research conducted on the platform AustriaN Newspapers Online (ANNO) for the NewsEye case study ‘the dark sides of war’, the main challenges of working with digitized newspaper collections will be discussed in this paper. Especially two aspects – the fire catastrophe at the munitions factory Wöllersdorf (1918/09/18) in Lower Austria and the Austrian press coverage about war widows during the First World War – will be used as specific examples. The discussed limitations include the Optical Character Recognition (OCR) quality, provided search options and metadata, as well as others. Furthermore, possible improvements regarding these challenges, e.g. Optical Layout Recognition (OLR), Named-entity Recognition (NER) and Named-entity Linking (NEL), will be presented in this paper. Copyright © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
	language = {English},
	urldate = {2020-10-21},
	booktitle = {{CEUR} {Workshop} {Proc}.},
	publisher = {CEUR-WS},
	author = {Klaus, B.},
	editor = {{Reinsone S.} and {Skadina I.} and {Baklane A.} and {Daugavietis J.}},
	year = {2020},
	note = {Journal Abbreviation: CEUR Workshop Proc.},
	keywords = {Austria, Digital Humanities, Digitized Newspaper Collections, First World War, HEMDIG - SCOPUS, Historical newspapers, Historical Research Interfaces, Military operations, Named entities, Named entity recognition, Newsprint, Optical character recognition, Optical character recognition (OCR), Optical layouts, SCOPUS},
	pages = {267--274},
}

@inproceedings{allen_metadata_1999,
	address = {New York},
	title = {Metadata and data structures for the historical newspaper digital library},
	isbn = {1581131461 (ISBN); 9781581131468 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033279047&doi=10.1145%2f319950.319971&partnerID=40&md5=cabd3dc0f43392c8b6e4edb1f6999fa7},
	doi = {10.1145/319950.319971},
	abstract = {We examine metadata and data-structure issues for the Historical Newspaper Digital Library. This project proposes to digitize and then do OCR and linguisting processing on several years worth of historical newspapers. Newspapers are very complex information objects so developing a rich description of their content is challenging. In addition to frameworks for the logical structure and physical layout, we propose metadata relevant to the image processing and to the historians who will use this collection. Finally, we consider how the metadata infrastructure might be managed as it evolves with improved text processing capabilities and how an infrastructure might be developed to support a community of users.},
	language = {English},
	urldate = {1999-11-02},
	booktitle = {Int {Conf} {Inf} {Knowledge} {Manage}},
	publisher = {ACM},
	author = {Allen, Robert B. and Schalow, John},
	collaborator = {{ACM}},
	year = {1999},
	note = {Journal Abbreviation: Int Conf Inf Knowledge Manage},
	keywords = {Data structures, Database systems, Digital libraries, HEMDIG - SCOPUS, Management information systems, SCOPUS},
	pages = {147--153},
}

@book{allen_automated_2008,
	address = {Bali},
	series = {11th {International} {Conference} on {Asian} {Digital} {Libraries}, {ICADL} 2008},
	title = {Automated processing of digitized historical newspapers: {Identification} of segments and genres},
	volume = {5362 LNCS},
	isbn = {03029743 (ISSN); 3540895329 (ISBN); 9783540895329 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-58049129541&doi=10.1007%2f978-3-540-89533-6_49&partnerID=40&md5=6f944408c26c615b1bacecd986eb69f8},
	abstract = {Many historical newspapers are being digitized. We aim to support access to them via text analysis of the OCRd content. However, the OCR includes many errors; so extracting meaningful content from it is difficult. A pipeline of processing steps is proposed. Here, we describe the first two steps: segmentation and genre identification. The segmentation procedure based on headings was quite successful. Genre identification worked well for easily defined genre categories such as weather reports. We also propose additional techniques which may improve the accuracy still farther. © 2008 Springer Berlin Heidelberg.},
	language = {English},
	urldate = {2008-12-02},
	publisher = {Springer Verlag},
	author = {Allen, R.B. and Waldstein, I. and Zhu, W.},
	year = {2008},
	doi = {10.1007/978-3-540-89533-6_49},
	note = {Journal Abbreviation: Lect. Notes Comput. Sci.
Pages: 386
Publication Title: Lect. Notes Comput. Sci.},
	keywords = {Automated processing, Categorization, Digital libraries, Genre identification, Genres, HEMDIG - SCOPUS, Historical newspapers, Image segmentation, Newsprint, OCR, Optical character recognition, Pipeline processing systems, Processing steps, SCOPUS, Segmentation, Segmentation procedure, Text analysis},
}

@book{reynaert_non-interactive_2008,
	address = {Haifa},
	series = {9th {International} {Conference} on {Computational} {Linguistics} and {Intelligent} {Text} {Processing}, {CICLing} 2008},
	title = {Non-interactive {OCR} post-correction for giga-scale digitization projects},
	volume = {4919 LNCS},
	isbn = {03029743 (ISSN); 354078134X (ISBN); 9783540781349 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-49949085967&doi=10.1007%2f978-3-540-78135-6_53&partnerID=40&md5=72d6e814cca1503a6ec7356f40c19112},
	abstract = {This paper proposes a non-interactive system for reducing the level of OCR-induced typographical variation in large text collections, contemporary and historical. Text-Induced Corpus Clean-up or ticcl (pronounce 'tickle') focuses on high-frequency words derived from the corpus to be cleaned and gathers all typographical variants for any particular focus word that lie within the predefined Levenshtein distance (henceforth: ld). Simple text-induced filtering techniques help to retain as many as possible of the true positives and to discard as many as possible of the false positives. ticcl has been evaluated on a contemporary OCR-ed Dutch text corpus and on a corpus of historical newspaper articles, whose OCR-quality is far lower and which is in an older Dutch spelling. Representative samples of typographical variants from both corpora have allowed us not only to properly evaluate our system, but also to draw effective conclusions towards the adaptation of the adopted correction mechanism to OCR-error resolution. The performance scores obtained up to ld 2 mean that the bulk of undesirable OCR-induced typographical variation present can fully automatically be removed. © 2008 Springer-Verlag Berlin Heidelberg.},
	language = {English},
	urldate = {2008-02-17},
	author = {Reynaert, M.},
	year = {2008},
	doi = {10.1007/978-3-540-78135-6_53},
	note = {Journal Abbreviation: Lect. Notes Comput. Sci.
Pages: 630
Publication Title: Lect. Notes Comput. Sci.},
	keywords = {Computational linguistics, Correction mechanism, Error correction, False positives, Filtering techniques, HEMDIG - SCOPUS, High-frequency words, Intelligent text processing, International conferences, Ionizing radiation, Levenshtein distances, Linguistics, Newspaper articles, Non-interactive, Performance scores, SCOPUS, Security of data, Software agents, Text corpora, Text processing, Word processing},
}

@book{allen_automated_2010,
	address = {Gold Coast, QLD},
	series = {12th {International} {Conference} on {Asia}-{Pacific} {Digital} {Libraries}, {ICADL} 2010},
	title = {Automated processing of digitized historical newspapers beyond the article level: {Sections} and regular features},
	volume = {6102 LNCS},
	isbn = {03029743 (ISSN); 3642136532 (ISBN); 9783642136535 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955455282&doi=10.1007%2f978-3-642-13654-2_11&partnerID=40&md5=a17acc4673a1e94d3c16aec806d39e21},
	abstract = {Millions of pages of historical newspapers have been digitized but in most cases access to these are supported by only basic search services. We are exploring interactive services for these collections which would be useful for supporting access, including automatic categorization of articles. Such categorization is difficult because of the uneven quality of the OCR text, but there are many clues which can be useful for improving the accuracy of the categorization. Here, we describe observations of several historical newspapers to determine the characteristics of sections. We then explore how to automatically identify those sections and how to detect serialized feature articles which are repeated across days and weeks. The goal is not the introduction of new algorithms but the development of practical and robust techniques. For both analyses we find substantial success for some categories and articles, but others prove very difficult. © 2010 Springer-Verlag.},
	language = {English},
	urldate = {2010-06-21},
	author = {Allen, R.B. and Hall, C.},
	translator = {{Microsoft Research; SAGE Publications Asia Pacific Pte Ltd; ExLibris (Australia) Pty Ltd; The University of Queensland Library}},
	year = {2010},
	doi = {10.1007/978-3-642-13654-2_11},
	note = {Journal Abbreviation: Lect. Notes Comput. Sci.
Pages: 101
Publication Title: Lect. Notes Comput. Sci.},
	keywords = {Access, Automated processing, Automatic categorization, Classification, Digital Humanities, Digital libraries, HEMDIG - SCOPUS, Historian's Workbench, Historical newspapers, History, Interactive services, Newspapers, Newsprint, Robust technique, SCOPUS, Search services, Text processing, Text Processing, Word processing},
}

@inproceedings{kae_improving_2010,
	address = {San Francisco, CA},
	title = {Improving state-of-the-art {OCR} through high-precision document-specific modeling},
	isbn = {10636919 (ISSN); 9781424469840 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955987982&doi=10.1109%2fCVPR.2010.5539867&partnerID=40&md5=ae0c419734cdac7692fc9fcb27b380ea},
	doi = {10.1109/CVPR.2010.5539867},
	abstract = {Optical character recognition (OCR) remains a difficult problem for noisy documents or documents not scanned at high resolution. Many current approaches rely on stored font models that are vulnerable to cases in which the document is noisy or is written in a font dissimilar to the stored fonts. We address these problems by learning character models directly from the document itself, rather than using pre-stored font models. This method has had some success in the past, but we are able to achieve substantial improvement in error reduction through a novel method for creating nearly error-free document-specific training data and building character appearance models from this data. In particular, we first use the state-of-the-art OCR system Tesseract to produce an initial translation. Then, our method identifies a subset of words that we have high confidence have been recognized correctly and uses this subset to bootstrap document-specific character models. We present theoretical justification that a word in the selected subset is very unlikely to be incorrectly recognized, and empirical results on a data set of difficult historical newspaper scans demonstrating that we make only two errors in 56 documents. We then relax the theoretical constraint in order to create a larger training set, and using document-specific character models generated from this data, we are able to reduce the error over properly segmented characters by 34.1\% overall from the initial Tesseract translation. ©2010 IEEE.},
	language = {English},
	urldate = {2010-06-13},
	booktitle = {Proc {IEEE} {Comput} {Soc} {Conf} {Comput} {Vision} {Pattern} {Recognit}},
	author = {Kae, A. and Huang, G. and Doersch, C. and Learned-Miller, E.},
	year = {2010},
	note = {Journal Abbreviation: Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
	keywords = {Appearance models, Character models, Computational methods, Computer vision, Data sets, Empirical results, Error reduction, HEMDIG - SCOPUS, High confidence, High resolution, High-precision, Historical newspapers, Novel methods, Optical character recognition, SCOPUS, Security of data, Tesseract, Training data, Training sets},
	pages = {1935--1942},
}

@article{muhlbergeb_digitalisierung_2011,
	title = {Digitalisierung historischer {Zeitungen} aus dem blickwinkel der automatisierten text-{Und} strukturerkennung ({OCR})},
	volume = {58},
	issn = {00442380 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955498905&partnerID=40&md5=81242f0fea911490e53a978ed4fd81f0},
	abstract = {OCR recognition is a key technology which cannot be circumvented when systematically digitizing historical newspapers. Although often achieving a word accuracy of only 80\% or less for newspapers of the 19th and early 20th century, these imperfect files nevertheless provide a basis for a number of interesting applications - from full-text searching to indexing by search engines and online correction by users. However, in comparison to traditional digitization projects, the use of OCR requires a fundamental change of thinking during the project planning, the design of the workflow, the implementation of quality control, and in the designing of long-term preservation and presentation of digitized material on the Internet.},
	language = {German},
	number = {1},
	journal = {Zeitschrift fur Bibliothekswesen und Bibliographie},
	author = {Mühlbergeb, G.},
	year = {2011},
	keywords = {HEMDIG - SCOPUS, SCOPUS},
	pages = {10--18},
}

@inproceedings{shima_image_2011,
	address = {Beijing},
	title = {Image processing for historical newspaper archives},
	isbn = {9781450309165 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054790219&doi=10.1145%2f2037342.2037363&partnerID=40&md5=e46729c9f7fb2ad0830d030511bb36d1},
	doi = {10.1145/2037342.2037363},
	abstract = {This paper presents some image processing methods that could produce accurate character segmentation results for historical newspaper archives. A full text search using a word spotting technique is no doubt a promising approach in order to facilitate the utilization of digital archives. Some word spotting techniques require the target images to be segmented into character images in advance, however character segmentation is a difficult issue especially for old and degraded document images. This paper figures out the causes that make the character segmentation difficult, and removes them in order to improve the accuracy of character segmentation. We first detect the ruled lines using Hough Transform in order to segment a whole newspaper image into column-separated images. Then we remove the ruled lines as well as ruby characters and noise. The proposed system is tested for 20 column-separated images of historical newspapers, and the accuracy of character segmentation is improved to 96.3\%. © 2011 ACM.},
	language = {English},
	urldate = {2011-09-16},
	booktitle = {{ACM} {Int}. {Conf}. {Proc}. {Ser}.},
	author = {Shima, T. and Terasawa, K. and Kawashima, T.},
	collaborator = {{FamilySearch}},
	year = {2011},
	note = {Journal Abbreviation: ACM Int. Conf. Proc. Ser.},
	keywords = {Character images, character segmentation, Character segmentation, Degraded document images, digital archive, Digital archives, Digital image storage, full text search, Full-text search, HEMDIG - SCOPUS, historical document, Historical documents, Historical newspapers, Hough transforms, Image processing - methods, Image segmentation, Imaging systems, Newsprint, optical character recognition, Optical character recognition, Optical data processing, Processing, SCOPUS, Target images, Word Spotting},
	pages = {127--132},
}

@inproceedings{terasawa_fast_2011,
	address = {Beijing},
	title = {A fast appearance-based full-text search method for historical newspaper images},
	isbn = {15205363 (ISSN); 9780769545202 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-82355169321&doi=10.1109%2fICDAR.2011.277&partnerID=40&md5=f4fd6408346ced1de800b9ad01bb6b3c},
	doi = {10.1109/ICDAR.2011.277},
	abstract = {This paper presents a fast appearance-based full-text search method for historical newspaper images. Since historical newspapers differ from recent newspapers in image quality, type fonts and language usages, optical character recognition (OCR) does not provide sufficient quality. Instead of OCR approach, we adopted appearance-based approach, that means we matched character to character with its shapes. Assuming proper character segmentation and proper feature description, full-text search problem is reduced to sequence matching problem of feature vector. To increase computational efficiency, we adopted pseudo-code expression called LSPC, which is a compact sketch of feature vector while retaining a good deal of its information. Experimental result showed that our method can retrieve a query string from a text of over eight million characters within a second. In addition, we predict that more sophisticated algorithm could be designed for LSPC. As an example, we established the Extended Boyer-Moore-Horspool algorithm that can reduce the computational cost further especially when the query string becomes longer. © 2011 IEEE.},
	language = {English},
	urldate = {2011-09-18},
	booktitle = {Proc. {Int}. {Conf}. {Doc}. {Anal}. {Recognit}.},
	author = {Terasawa, K. and Shima, T. and Kawashima, T.},
	collaborator = {{TC10 (Graph. Recogn.) TC11 (Read. Syst.) (IAPR); Chinese Academy of Sciences; NSFC; FUJITSU; Hanvon Technology}},
	year = {2011},
	note = {Journal Abbreviation: Proc. Int. Conf. Doc. Anal. Recognit.},
	keywords = {Algorithms, Appearance based, Boyer-Moore-Horspool algorithm, Character segmentation, Codes (symbols), Computational costs, Computational efficiency, Feature description, Feature vectors, Full-text search, HEMDIG - SCOPUS, historical document images, Historical documents, Historical newspapers, Image matching, Image quality, Locality-Sensitive Pseudo-Code, Newsprint, Optical character recognition, Pseudo-code, Query string, SCOPUS, Sequence matching, string matching, String matching, word spotting, Word Spotting},
	pages = {1379--1383},
}

@article{huang_bounding_2012,
	title = {Bounding the probability of error for high precision optical character recognition},
	volume = {13},
	issn = {15324435 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857846731&partnerID=40&md5=dde2c8ca41053322491950ffc6c95211},
	abstract = {We consider a model for which it is important, early in processing, to estimate some variables with high precision, but perhaps at relatively low recall. If some variables can be identified with near certainty, they can be conditioned upon, allowing further inference to be done efficiently. Specifically, we consider optical character recognition (OCR) systems that can be bootstrapped by identifying a subset of correctly translated document words with very high precision. This "clean set" is subsequently used as document-specific training data. While OCR systems produce confidence measures for the identity of each letter or word, thresholding these values still produces a significant number of errors. We introduce a novel technique for identifying a set of correct words with very high precision. Rather than estimating posterior probabilities, we bound the probability that any given word is incorrect using an approximate worst case analysis. We give empirical results on a data set of difficult historical newspaper scans, demonstrating that our method for identifying correct words makes only two errors in 56 documents. Using document-specific character models generated from this data, we are able to reduce the error over properly segmented characters by 34.1\% from an initial OCR system's translation.1 © 2012 Gary B. Huang, Andrew Kae, Carl Doersch and Erik Learned-Miller.},
	language = {English},
	journal = {Journal of Machine Learning Research},
	author = {Huang, G.B. and Kae, A. and Doersch, C. and Learned-Miller, E.},
	year = {2012},
	keywords = {Character models, Computer vision, Confidence Measure, Data sets, Document-specific modeling, Errors, HEMDIG - SCOPUS, High precision, Historical newspapers, Novel techniques, Optical character recognition, Posterior probability, Probability, Probability bounding, Probability of errors, SCOPUS, Thresholding, Training data, Worst-case analysis},
	pages = {363--387},
}

@inproceedings{palfray_plair_2012,
	address = {Copenhagen},
	title = {"{PlaIR}": {A} system to provide full access to digitized newspaper archives},
	isbn = {9780892083008 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876739050&partnerID=40&md5=58f35f22cb351eca4adb61ada6711027},
	abstract = {This paper presents a platform dedicated to the analysis and the online consultation of historical newspaper archives. This platform has been designed to provide a user experience as intuitive as possible by using mature open source tools. All the features are implemented thanks to the Spring framework. To meet this goal, we created a system to display tiled high-resolution images operating without a plug-in but based on an open source solution called IIPImage. The platform also allows for full-text searches thanks to the Java search library Apache Lucene and displays the results in the form of newspaper articles. In addition, we established collaborative features to provide the users with the ability to correct the content automatically generated by our document processing workflow and accessed through the browsing platform. The system is able to store all the corrections of the users, by using the couple Hibernate/MySQL. The aim is to enable continuous improvement of both the content quality and the search accuracy, by exploiting the ability of the users to recognize significant errors, in order to enhance the digital objects representing the newspaper issues. The proposed system is designed to generate metadata describing the physical layout, but also the logical structure of newspaper documents. Our article segmentation analyses a newspaper issue and recognizes articles, even if they straddle more than one page or if they spread in a complex structure. The workflow can also consider as input data, the results of optical character recognition (OCR) engines in order to provide a textual indexation of the segmented articles. By using this system, we want to create a true and representative digital object using standard formats (i.e. METS / ALTO) and containing the logical description of the content, making easier reading and understanding by the users. ©2012 Society for Imaging Science and Technology.},
	language = {English},
	urldate = {2012-06-12},
	booktitle = {Arch. - {Preserv}. {Strateg}. {Imaging} {Technol}. {Cult}. {Herit}. {Inst}. {Mem}. {Organ}. - {Final} {Program} {Proc}.},
	author = {Palfray, T. and Nicolas, S. and Paquet, T. and Tranouez, P.},
	collaborator = {{Society for Imaging Science and Technology (IS and T); MAM-A; Nationalmuseet}},
	year = {2012},
	note = {Journal Abbreviation: Arch. - Preserv. Strateg. Imaging Technol. Cult. Herit. Inst. Mem. Organ. - Final Program Proc.},
	keywords = {Automatically generated, Continuous improvements, Document-processing, HEMDIG - SCOPUS, High resolution image, Historical newspapers, Imaging techniques, Metadata, Newsprint, Open systems, Open-source solutions, Optical character recognition, Optical character recognition engines, SCOPUS, Segmentation analysis, Societies and institutions},
	pages = {48--53},
}

@book{chaloupka_using_2013,
	address = {Naples},
	series = {17th {International} {Conference} on {Image} {Analysis} and {Processing}, {ICIAP} 2013},
	title = {Using various types of multimedia resources to train system for automatic transcription of {Czech} historical oral archives},
	volume = {8158 LNCS},
	isbn = {03029743 (ISSN); 9783642411892 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887032019&doi=10.1007%2f978-3-642-41190-8_25&partnerID=40&md5=28858f4286f8052bbf123ae29accf206},
	abstract = {Historical spoken documents represent a unique segment of national cultural heritage. In order to disclose the large Czech Radio audio archive to research community and to public, we have been developing a system whose aim is to transcribe automatically the archive files, index them and make them searchable. The transcription of contemporary (1 or 2 decades old) documents is based on the lexicon and statistical language model (LM) built from a large amount of recent texts available in electronic form. From the older periods (before 1990), however, digital texts do not exist. Therefore, we needed a) to find resources that represent language of those times, b) to convert them from their original form to text, c) to utilize this text for creating epoch specific lexicons and LMs, and eventually, d) to apply them in the developed speech recognition system. In our case, the main resources included: scanned historical newspapers, shorthand notes from the national parliament and subtitles from retro TV programs. When converted into text, they allowed us to built a more appropriate lexicon and to produce a preliminary version of the transcriptions. These were reused for unsupervised retraining of the final LM. In this way, we significantly improved the accuracy of the automatically transcribed radio news broadcast in 1969-1989 era, from initial 83 \% to 88 \%. © 2013 Springer-Verlag.},
	language = {English},
	urldate = {2013-09-09},
	author = {Chaloupka, J. and Nouza, J. and Kucharova, M.},
	translator = {Italian Ministry of Education, University {and} Research (MIUR); Italian Ministry of Economic Development (MiSE); Comune di Napoli; Google Inc.; Ansaldo STS},
	year = {2013},
	doi = {10.1007/978-3-642-41190-8_25},
	note = {Journal Abbreviation: Lect. Notes Comput. Sci.
Pages: 237
Publication Title: Lect. Notes Comput. Sci.},
	keywords = {Audio archives, Automatic transcription, C (programming language), Cultural heritages, HEMDIG - SCOPUS, historical audio archives, Historical newspapers, Image analysis, Learning systems, lexicon building, machine learning, Multimedia resources, Multimedia systems, Natural language processing systems, OCR, Optical character recognition, Research communities, SCOPUS, Speech recognition, Speech recognition systems, speech-to-text transcription, Statistical language modeling, Transcription},
}

@book{torao-pingi_understanding_2015,
	series = {28th {Australasian} {Joint} {Conference} on {Artificial} {Intelligence}, {AI} 2015},
	title = {Understanding people relationship: {Analysis} of digitised historical newspaper articles},
	volume = {9457},
	isbn = {03029743 (ISSN); 9783319263496 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952683304&doi=10.1007%2f978-3-319-26350-2_51&partnerID=40&md5=3273212d72ed62b13c10eccd83e69ae8},
	abstract = {The study of historical persons and their relationships gives an insight into the lives of people and the way society functioned in early times. Such information concerning Australian history can be gleaned from Trove’s digitized collection of historical newspapers (1803–1954). This research aims to mine Trove’s articles using closed and maximal association rules mining along with visualization tools to discover, conceptualize and understand the type, size and complexity of the notable relationships that existed between persons in historical Australia. Before the articles could be mined, they needed vigorous cleaning. Given the data’s source, type and extraction methods, estimated word-error rates were at 50–75\%. Pre-processing efforts were aimed at reducing errors originating from optical character recognition (OCR), natural language processing and some co-referencing both within and between articles. Only after cleaning were the datasets able to return interesting associations at higher support thresholds. © Springer International Publishing Switzerland 2015.},
	language = {English},
	urldate = {2015-11-30},
	publisher = {Springer Verlag},
	author = {Torao-Pingi, S. and Nayak, R.},
	editor = {{Renz J.} and {Pfahringer B.}},
	translator = {{AAAI; College of Engineering and Computer Science of the Australian National University; et al; NICTA; UNSW Canberra}},
	year = {2015},
	doi = {10.1007/978-3-319-26350-2_51},
	note = {Journal Abbreviation: Lect. Notes Comput. Sci.
Pages: 588
Publication Title: Lect. Notes Comput. Sci.},
	keywords = {Artificial intelligence, Association rule mining, Association rules, Character recognition, Computational linguistics, Data mining, Errors, Extraction method, HEMDIG - SCOPUS, Historical newspapers, Information concerning, Maximal association rules, Natural language processing, NAtural language processing, Natural language processing systems, Newsprint, OCR errors, Optical character recognition, Optical character recognition (OCR), SCOPUS, Speech recognition, Support threshold, Visualization tools},
}

@inproceedings{konya_character_2011,
	address = {Beijing},
	title = {Character enhancement for historical newspapers printed using hot metal typesetting},
	isbn = {15205363 (ISSN); 9780769545202 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-82355168372&doi=10.1109%2fICDAR.2011.190&partnerID=40&md5=bf1ed4cf1a0ca788232b9e7813020465},
	doi = {10.1109/ICDAR.2011.190},
	abstract = {We propose a new method for an effective removal of the printing artifacts occurring in historical newspapers which are caused by problems in the hot metal typesetting, a widely used printing technique in the late 19th and early 20th century. Such artifacts typically appear as thin lines between single characters or glyphs and are in most cases connected to one of the neighboring characters. The quality of the optical character recognition (OCR) is heavily influenced by this type of printing artifacts. The proposed method is based on the detection of (near) vertical segments by means of directional single-connected chains (DSCC). In order to allow the robust processing of complex decorative fonts such as Fraktur, a set of rules is introduced. This allows us to successfully process prints exhibiting artifacts with a stroke width even higher than that of most thin characters stems. We evaluate our approach on a dataset consisting of old newspaper excerpts printed using Fraktur fonts. The recognition results on the enhanced images using two independent OCR engines (ABBYY Fine Reader and Tesseract) show significant improvements over the originals. © 2011 IEEE.},
	language = {English},
	urldate = {2011-09-18},
	booktitle = {Proc. {Int}. {Conf}. {Doc}. {Anal}. {Recognit}.},
	author = {Konya, I. and Eickeler, S. and Seibert, C.},
	collaborator = {{TC10 (Graph. Recogn.) TC11 (Read. Syst.) (IAPR); Chinese Academy of Sciences; NSFC; FUJITSU; Hanvon Technology}},
	year = {2011},
	note = {Journal Abbreviation: Proc. Int. Conf. Doc. Anal. Recognit.},
	keywords = {20th century, character enhancement, Data processing, Data sets, HEMDIG - SCOPUS, historical documents, Historical documents, Historical newspapers, Hot metal, hot metal typesetting, Newsprint, OCR, OCR engines, Old newspapers, Optical character recognition, Printing techniques, retro-digitization, Robust processing, SCOPUS, Set of rules, Tesseract, Typesetting},
	pages = {936--940},
}

@inproceedings{hebert_pivaj_2014,
	address = {Madrid},
	title = {{PIVAJ}: {Displaying} and augmenting digitized newspapers on the web experimental feedback from the "journal de rouen" collection},
	isbn = {9781450325882 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901636569&doi=10.1145%2f2595188.2595217&partnerID=40&md5=14910a74d75e2d3edf06339b46706259},
	doi = {10.1145/2595188.2595217},
	abstract = {In this paper we present PIVAJ, an Indexing and Valorization Platform for Journal Archives from the French Upper Normandy and EU funded PlaIR project. This web platform proposes sophisticated searching and browsing facilities over the transcription of the text content. The search engine is able to list the relevant articles from a textual query. The platform also integrates a collaborative OCR correction tool which was ranked as one of the most efficient in an evaluation campaign carried out by the National Library of the Netherlands during the European project IMPACT. The first real scale experiment was made to provide access to digitizations of the "Journal de Rouen" collection. Consultation statistics of the web platform and comments of users illustrate the interest for PIVAJ.},
	language = {English},
	urldate = {2014-05-19},
	booktitle = {{ACM} {Int}. {Conf}. {Proc}. {Ser}.},
	publisher = {Association for Computing Machinery},
	author = {Hebert, D. and Palfray, T. and Nicolas, S. and Tranouez, P. and Paquet, T.},
	collaborator = {{The Support Action Centre of Competence in Digitisation (SUCCEED)}},
	year = {2014},
	note = {Journal Abbreviation: ACM Int. Conf. Proc. Ser.},
	keywords = {Articles extraction in newspapers, Collaborative ocr correction, Crowdsourcing, HEMDIG - SCOPUS, Historical newspapers, Indexing (of information), Information extraction from document images, Logical structure, Newsprint, Page layout analysis, SCOPUS, Search engines, Structural analysis, Text content, Text content indexing, Web platform},
	pages = {173--178},
}

@inproceedings{berg-kirkpatrick_improved_2014,
	address = {Baltimore, MD},
	title = {Improved typesetting models for historical {OCR}},
	volume = {2},
	isbn = {9781937284732 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906932608&doi=10.3115%2fv1%2fp14-2020&partnerID=40&md5=cc665ff61a62506fb7026a51e721ba12},
	doi = {10.3115/v1/p14-2020},
	abstract = {We present richer typesetting models that extend the unsupervised historical document recognition system of Berg- Kirkpatrick et al. (2013). The first model breaks the independence assumption between vertical offsets of neighboring glyphs and, in experiments, substantially decreases transcription error rates. The second model simultaneously learns multiple font styles and, as a result, is able to accurately track italic and nonitalic portions of documents. Richer models complicate inference so we present a new, streamlined procedure that is over 25× faster than the method used by Berg- Kirkpatrick et al. (2013). Our final system achieves a relative word error reduction of 22\% compared to state-of-the-art results on a dataset of historical newspapers. © 2014 Association for Computational Linguistics.},
	language = {English},
	urldate = {2014-06-22},
	booktitle = {Annu. {Meet}. {Assoc}. {Comput}. {Linguist}., {ACL} - {Proc}. {Conf}.},
	publisher = {Association for Computational Linguistics (ACL)},
	author = {Berg-Kirkpatrick, T. and Klein, D.},
	collaborator = {{amazon.com; Baidu; Bloomberg; et al.; Google; IBM Watson}},
	year = {2014},
	note = {Journal Abbreviation: Annu. Meet. Assoc. Comput. Linguist., ACL - Proc. Conf.},
	keywords = {Computational linguistics, Error rate, Error reduction, HEMDIG - SCOPUS, Historical documents, Historical newspapers, Independence assumption, Kirkpatrick, SCOPUS, State of the art, Typesetting},
	pages = {118--123},
}

@inproceedings{taghva_utilizing_2014,
	address = {San Francisco, CA},
	title = {Utilizing web data in identification and correction of {OCR} errors},
	volume = {9021},
	isbn = {0277786X (ISSN); 9780819499387 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894563950&doi=10.1117%2f12.2042403&partnerID=40&md5=9a261864f7869da963031ee9ab1c8547},
	doi = {10.1117/12.2042403},
	abstract = {In this paper, we report on our experiments for detection and correction of OCR errors with web data. More specifically, we utilize Google search to access the big data resources available to identify possible candidates for correction. We then use a combination of the Longest Common Subsequences (LCS) and Bayesian estimates to automatically pick the proper candidate. Our experimental results on a small set of historical newspaper data show a recall and precision of 51\% and 100\%, respectively. The work in this paper further provides a detailed classification and analysis of all errors. In particular, we point out the shortcomings of our approach in its ability to suggest proper candidates to correct the remaining errors. © 2014 SPIE-IS\&T.},
	language = {English},
	urldate = {2014-02-05},
	booktitle = {Proc {SPIE} {Int} {Soc} {Opt} {Eng}},
	author = {Taghva, K. and Agarwal, S.},
	collaborator = {{The Society for Imaging Science and Technology (IS and T); The Society of Photo-Optical Instrumentation Engineers (SPIE)}},
	year = {2014},
	note = {Journal Abbreviation: Proc SPIE Int Soc Opt Eng},
	keywords = {Big Data, Error Correction, Error Identification, HEMDIG - SCOPUS, Information Extraction, Mining, Post Processing, SCOPUS},
}

@inproceedings{hachey_b_australasian_2015,
	title = {Australasian {Language} {Technology} {Association} {Workshop} 2014, {ALTA} 2014 - {Proceedings}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119436769&partnerID=40&md5=1b613151835f8e5fe5b7103da96b7dfc},
	abstract = {The proceedings contain 21 papers. The topics discussed include: query-based single document summarization using an ensemble noisy auto-encoder; comparison of visual and logical character segmentation in tesseract OCR language data for Indic writing scripts; analysis of word embeddings and sequence features for clinical information extraction; using entity information from a knowledge base to improve relation extraction; likelihood ratio-based forensic voice comparison on L2 speakers: a case of Hong Kong native male production of English vowels; similarity metrics for clustering PubMed abstracts for evidence based medicine; and finding names in trove: named entity recognition for Australian historical newspapers.},
	language = {English},
	urldate = {2015-12-08},
	booktitle = {Australas. {Lang}. {Technol}. {Assoc}. {Workshop} , {ALTA} - {Proc}.},
	publisher = {Association for Computational Linguistics (ACL)},
	editor = {{Hachey B.} and {Webster K.}},
	collaborator = {{CMCRC; CSIRO; Data 61; Google; Hugo; The University of Sydney}},
	year = {2015},
	note = {Journal Abbreviation: Australas. Lang. Technol. Assoc. Workshop , ALTA - Proc.},
	keywords = {HEMDIG - SCOPUS, SCOPUS},
}

@inproceedings{gupta_classification_2015,
	title = {Classification of crowdsourced text correction},
	volume = {18-21-March-2015},
	isbn = {9781450334365 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958531177&doi=10.1145%2f2732587.2732619&partnerID=40&md5=b9fc2edc219d06e2614d2d42e3c7b2e8},
	doi = {10.1145/2732587.2732619},
	abstract = {Optical Character Recognition (OCR) is a commonly used technique for digitizing printed material enabling them to be displayed online, searched and used in text mining applications. The text generated from OCR devices is often garbled due to variations in quality of the input paper, size and style of the font and column layout. This adversely affects retrieval effectiveness and hence techniques for cleaning the garbled text need to be improvised.This prototype system is expected to be deployed on historical newspaper archives that make extensive use of user text corrections. Copyright 2015 ACM.},
	language = {English},
	urldate = {2015-03-18},
	booktitle = {{ACM} {Int}. {Conf}. {Proc}. {Ser}.},
	publisher = {Association for Computing Machinery},
	author = {Gupta, M. and Dutta, H. and Geiger, B.},
	year = {2015},
	note = {Journal Abbreviation: ACM Int. Conf. Proc. Ser.},
	keywords = {Character recognition, Data mining, HEMDIG - SCOPUS, Historical newspapers, Optical character recognition, Optical character recognition (OCR), Printed materials, Prototype system, Retrieval effectiveness, SCOPUS, Text mining, Text processing},
	pages = {142--143},
}

@inproceedings{clausner_enp_2015,
	title = {The {ENP} image and ground truth dataset of historical newspapers},
	volume = {2015-November},
	isbn = {15205363 (ISSN); 9781479918058 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962571343&doi=10.1109%2fICDAR.2015.7333898&partnerID=40&md5=50f230217eb44bb96f07aa24093c28e7},
	doi = {10.1109/ICDAR.2015.7333898},
	abstract = {This paper presents a research dataset of historical newspapers comprising over 500 page images, uniquely representative of European cultural heritage from the digitization projects of 12 national and major European libraries, created within the scope of the large-scale digitisation Europeana Newspapers Project (ENP). Every image is accompanied by comprehensive ground truth (Unicode encoded full-text, layout information with precise region outlines, type labels, and reading order) in PAGE format and searchable metadata about document characteristics and artefacts. The first part of the paper describes the nature of the dataset, how it was built, and the challenges encountered. In the second part, a baseline for two state-of-the-art OCR systems (ABBYY FineReader Engine 11 and Tesseract 3.03) is given with regard to both text recognition and segmentation/layout analysis performance. © 2015 IEEE.},
	language = {English},
	urldate = {2015-08-23},
	booktitle = {Proc. {Int}. {Conf}. {Doc}. {Anal}. {Recognit}.},
	publisher = {IEEE Computer Society},
	author = {Clausner, C. and Papadopoulos, C. and Pletschacher, S. and Antonacopoulos, A.},
	collaborator = {{ABBYY; Google; iTESOFT; MyScript; Yooz}},
	year = {2015},
	note = {Journal Abbreviation: Proc. Int. Conf. Doc. Anal. Recognit.},
	keywords = {Character recognition, Cultural heritages, document analysis, Document analysis, European libraries, ground truth, Ground truth, Ground-truth dataset, HEMDIG - SCOPUS, Historic preservation, historical documents, Historical documents, Historical newspapers, image dataset, Image datasets, Newsprint, Optical character recognition, SCOPUS},
	pages = {931--935},
}

@book{kettunen_keep_2016,
	series = {11th {Italian} {Research} {Conference} on {Digital} {Libraries}, {IRCDL} 2015},
	title = {Keep, change or delete? {Setting} up a low resource {OCR} post-correction framework for a digitized old finnish newspaper collection},
	volume = {612},
	isbn = {18650929 (ISSN); 9783319419374 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978234685&doi=10.1007%2f978-3-319-41938-1_11&partnerID=40&md5=66b642a4a5af69c302d213407346a529},
	abstract = {There has been a huge interest in digitization of both hand-written and printed historical material in the last 10–15 years and most probably this interest will only increase in the ongoing Digital Humanities era. As a result of the interest we have lots of digital historical document collections available and will have more of them in the future. The National Library of Finland has digitized a large proportion of the historical newspapers published in Finland between 1771 and 1910 [1–3]; the collection, Digi, can be reached at http://digi.kansalliskirjasto.fi/. This collection contains approximately 1.95 million pages in Finnish and Swedish, the Finnish part being about 2.385 billion words. In the output of the Optical Character Recognition (OCR) process, errors are common especially when the texts are printed in the Gothic (Fraktur, blackletter) typeface. The errors lower the usability of the corpus both from the point of view of human users as well as considering possible elaborated text mining applications. Automatic spell checking and correction of the data is also difficult due to the historical spelling variants and low OCR quality level of the material. This paper discusses the overall situation of the intended post-correction of the Digi content and evaluation of the correction. We shall present results of our post-correction trials, and discuss some aspects of methodology of evaluation. These are the first reported evaluation results of post-correction of the data and the experiences will be used in planning of the post-correction of the whole material. © Springer International Publishing Switzerland 2016.},
	language = {English},
	urldate = {2015-01-29},
	publisher = {Springer Verlag},
	author = {Kettunen, K.},
	editor = {{Calvanese D.} and {De Nart D.} and {Tasso C.}},
	translator = {{DELOS; EU FP6 Network of Excellence on digital libraries}},
	year = {2016},
	doi = {10.1007/978-3-319-41938-1_11},
	note = {Journal Abbreviation: Commun. Comput. Info. Sci.
Pages: 103
Publication Title: Commun. Comput. Info. Sci.},
	keywords = {Character recognition, Data mining, Digital humanities, Digital libraries, Evaluation, Evaluation results, HEMDIG - SCOPUS, Historical documents, Historical newspaper collections, Historical newspapers, National libraries, Newsprint, OCR post-correction, Optical character recognition, Optical character recognition (OCR), SCOPUS, Spell-checking},
}

@inproceedings{kettunen_modern_2016,
	title = {Modern tools for old content-in search of named entities in a finnish ocred historical newspaper collection 1771-1910},
	volume = {1670},
	isbn = {16130073 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988825893&partnerID=40&md5=91f8e8db6f6b58eef1a64012aa9f72b4},
	abstract = {Named entity recognition (NER), search, classification and tagging of names and name like frequent informational elements in texts, has become a standard information extraction procedure for textual data. NER has been applied to many types of texts and different types of entities: newspapers, fiction, historical records, persons, locations, chemical compounds, protein families, animals etc. In general a NER system's performance is genre and domain dependent and also used entity categories vary [1]. The most general set of named entities is usually some version of three partite categorization of locations, persons and organizations. In this paper we report first trials and evaluation of NER with data out of a digitized Finnish historical newspaper collection Digi. Digi collection contains 1,960,921 pages of newspaper material from years 1771-1910 both in Finnish and Swedish. We use only material of Finnish documents in our evaluation. The OCRed newspaper collection has lots of OCR errors; its estimated word level correctness is about 74-75 \% [2]. Our principal NER tagger is a rule-based tagger of Finnish, FiNER, provided by the FIN-CLARIN consortium. We show also results of limited category semantic tagging with tools of the Semantic Computing Research Group (SeCo) of the Aalto University. FiNER is able to achieve up to 60.0 F-score with named entities in the evaluation data. Seco's tools achieve 30.0-60.0 F-score with locations and persons. Performance of FiNER and SeCo's tools with the data shows that at best about half of named entities can be recognized even in a quite erroneous OCRed text. Copyright © 2016.},
	language = {English},
	urldate = {2016-09-12},
	booktitle = {{CEUR} {Workshop} {Proc}.},
	publisher = {CEUR-WS},
	author = {Kettunen, K. and Mäkelä, E. and Kuokkala, J. and Ruokolainen, T. and Niemi, J.},
	editor = {{Muller E.} and {Krestel R.} and {Mottin D.}},
	year = {2016},
	note = {Journal Abbreviation: CEUR Workshop Proc.},
	keywords = {Character recognition, Chemical compounds, Classification (of information), Data mining, Extraction procedure, Finnish, HEMDIG - SCOPUS, Historical Newspaper Collections, Historical newspapers, Historical records, Information retrieval systems, Location, Named entity recognition, Named Entity Recognition, Natural language processing systems, Newsprint, Persons and organizations, SCOPUS, Semantic Computing, Semantic tagging, Semantics},
	pages = {124--135},
}

@inproceedings{kettunen_measuring_2016,
	title = {Measuring lexical quality of a historical {Finnish} newspaper collection - {Analysis} of garbled {OCR} data with basic {Language} technology tools and means},
	isbn = {9782951740891 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992559765&partnerID=40&md5=152c8a12afd5d390bae359cfc6115f83},
	abstract = {The National Library of Finland has digitized a large proportion of the historical newspapers published in Finland between 1771 and 1910 (Bremer-Laamanen 2001). This collection contains approximately 1.95 million pages in Finnish and Swedish. Finnish part of the collection consists of about 2.39 billion words. The National Library's Digital Collections are offered via the digi.kansalliskirjasto.fi web service, also known as Digi. Part of this material is also available freely downloadable in The Language Bank of Finland provided by the Fin-CLARIN consortium. The collection can also be accessed through the Korp environment that has been developed by Språkbanken at the University of Gothenburg and extended by FIN-CLARIN team at the University of Helsinki to provide concordances of text resources. A Cranfield-style information retrieval test collection has been produced out of a small part of the Digi newspaper material at the University of Tampere (Järvelin et al., 2015). The quality of the OCRed collections is an important topic in digital humanities, as it affects general usability and searchability of collections. There is no single available method to assess the quality of large collections, but different methods can be used to approximate the quality. This paper discusses different corpus analysis style ways to approximate the overall lexical quality of the Finnish part of the Digi collection.},
	language = {English},
	urldate = {2016-05-23},
	booktitle = {Int. {Conf}. {Lang}. {Resourc}. and {Eval}. - {LREC}},
	publisher = {European Language Resources Association (ELRA)},
	author = {Kettunen, K. and Pääkkönen, T.},
	editor = {{Calzolari N.} and {Choukri K.} and {Mazo H.} and {Moreno A.} and {Declerck T.} and {Goggi S.} and {Grobelnik M.} and {Odijk J.} and {Piperidis S.} and {Maegaard B.} and {Mariani J.}},
	collaborator = {{European Media Laboratory GmbH (EML); Intel}},
	year = {2016},
	note = {Journal Abbreviation: Int. Conf. Lang. Resourc. and Eval. - LREC},
	keywords = {Digital collections, Digital humanities, Digital libraries, Finnish, Fins (heat exchange), HEMDIG - SCOPUS, Historical newspaper collections, Historical newspapers, Language technology, Lexical qualities, National libraries, Newsprint, Ocr evaluation, Quality control, SCOPUS, University of helsinki, Web services},
	pages = {956--961},
}

@inproceedings{clausner_quality_2016,
	title = {Quality {Prediction} {System} for {Large}-{Scale} {Digitisation} {Workflows}},
	isbn = {9781509017928 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979593632&doi=10.1109%2fDAS.2016.82&partnerID=40&md5=6a0d1a82e9a04da14e4c9e01414e86d2},
	doi = {10.1109/DAS.2016.82},
	abstract = {The feasibility of large-scale OCR projects can so far only be assessed by running pilot studies on subsets of the target document collections and measuring the success of different workflows based on precise ground truth, which can be very costly to produce in the required volume. The premise of this paper is that, as an alternative, quality prediction may be used to approximate the success of a given OCR workflow. A new system is thus presented where a classifier is trained using metadata, image and layout features in combination with measured success rates (based on minimal ground truth). Subsequently, only document images are required as input for the numeric prediction of the quality score (no ground truth required). This way, the system can be applied to any number of similar (unseen) documents in order to assess their suitability for being processed using the particular workflow. The usefulness of the system has been validated using a realistic dataset of historical newspaper pages. © 2016 IEEE.},
	language = {English},
	urldate = {2016-04-11},
	booktitle = {Proc. - {IAPR} {Int}. {Workshop} {Doc}. {Anal}. {Syst}., {DAS}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Clausner, C. and Pletschacher, S. and Antonacopoulos, A.},
	collaborator = {{ABBYY; et al.; Family Search; HP; IAPR; iTESOFT}},
	year = {2016},
	note = {Journal Abbreviation: Proc. - IAPR Int. Workshop Doc. Anal. Syst., DAS},
	keywords = {Digitisation, Document analysis, Forecasting, Ground truthing, HEMDIG - SCOPUS, Large-scale, Numeric prediction, Performance evaluation, Quality control, Quality prediction, SCOPUS, Supervised learning},
	pages = {138--143},
}

@article{jarvelin_information_2016,
	title = {Information retrieval from historical newspaper collections in highly inflectional languages: {A} query expansion approach},
	volume = {67},
	issn = {23301635 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995514103&doi=10.1002%2fasi.23379&partnerID=40&md5=12fa38eeabf192ce98b1158889c02054},
	doi = {10.1002/asi.23379},
	abstract = {The aim of the study was to test whether query expansion by approximate string matching methods is beneficial in retrieval from historical newspaper collections in a language rich with compounds and inflectional forms (Finnish). First, approximate string matching methods were used to generate lists of index words most similar to contemporary query terms in a digitized newspaper collection from the 1800s. Top index word variants were categorized to estimate the appropriate query expansion ranges in the retrieval test. Second, the effectiveness of approximate string matching methods, automatically generated inflectional forms, and their combinations were measured in a Cranfield-style test. Finally, a detailed topic-level analysis of test results was conducted. In the index of historical newspaper collection the occurrences of a word typically spread to many linguistic and historical variants along with optical character recognition (OCR) errors. All query expansion methods improved the baseline results. Extensive expansion of around 30 variants for each query word was required to achieve the highest performance improvement. Query expansion based on approximate string matching was superior to using the inflectional forms of the query words, showing that coverage of the different types of variation is more important than precision in handling one type of variation. © 2015 ASIS\&T},
	language = {English},
	number = {12},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Järvelin, A. and Keskustalo, H. and Sormunen, E. and Saastamoinen, M. and Kettunen, K.},
	year = {2016},
	note = {Publisher: John Wiley and Sons Inc.},
	keywords = {Approximate string matching, Automatic test pattern generation, Automatically generated, Baseline results, Character recognition, error, HEMDIG - SCOPUS, Historical newspapers, human, human experiment, information retrieval, Information retrieval, language, Newsprint, Optical character recognition, Optical character recognition (OCR), publication, query expansion, Query expansion, query processing, Query processing, Query terms, Query words, recognition, SCOPUS},
	pages = {2928--2946},
}

@inproceedings{drobac_ocr_2017,
	title = {{OCR} and post-correction of historical {Finnish} texts},
	isbn = {9789176856017 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045335800&partnerID=40&md5=5c388cb98829a5a8d6849007571f1fc1},
	abstract = {This paper presents experiments on Optical character recognition (OCR) as a combination of Ocropy software and data-driven spelling correction that uses Weighted Finite-State Methods. Both model training and testing were done on Finnish corpora of historical newspaper text and the best combination of OCR and post-processing models give 95.21\% character recognition accuracy. © 2017 Linköping University Electronic Press.},
	language = {English},
	urldate = {2017-05-23},
	booktitle = {{NoDaLiDa} - {Nordic} {Conf}. {Comput}. {Linguist}., {Proc}. {Conf}.},
	publisher = {Association for Computational Linguistics (ACL)},
	author = {Drobac, S. and Kauppinen, P. and Lindén, K.},
	editor = {{Tiedemann J.}},
	year = {2017},
	note = {Journal Abbreviation: NoDaLiDa - Nordic Conf. Comput. Linguist., Proc. Conf.},
	keywords = {Data driven, Finite state methods, Finnish, HEMDIG - SCOPUS, Historical newspapers, Model testing, Model training, Optical character recognition, Post-processing, Processing model, SCOPUS, Spelling correction, Training and testing},
	pages = {70--76},
}

@inproceedings{kettunen_names_2017,
	title = {Names, right or wrong: {Named} entities in an {OCRed} historical {Finnish} newspaper collection},
	volume = {Part F129473},
	isbn = {9781450352659 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028047979&doi=10.1145%2f3078081.3078084&partnerID=40&md5=e47fe70d47ad3f0310e15fd9f09a234e},
	doi = {10.1145/3078081.3078084},
	abstract = {Named Entity Recognition (NER), search, classification and tagging of names and name like frequent informational elements in texts, has become a standard information extraction procedure for textual data. NER has been applied to many types of texts and different types of entities: newspapers, fiction, historical records, persons, locations, chemical compounds, protein families, animals etc. In general a NER system's performance is genre and domain dependent and also used entity categories vary [16]. The most general set of named entities is usually some version of three partite categorization of locations, persons and organizations. In this paper we report evaluation result of NER with data out of a digitized Finnish historical newspaper collection Digi. Experiments, results and discussion of this research serve development of the Web collection of historical Finnish newspapers. Digi collection contains 1,960,921 pages of newspaper material from years 1771-1910 both in Finnish and Swedish. We use only material of Finnish documents in our evaluation. The OCRed newspaper collection has lots of OCR errors; its estimated word level correctness is about 70-75\% [7]. Our baseline NER tagger is a rule-based tagger of Finnish, FiNER, provided by the FIN-CLARIN consortium. Three other available tools are also evaluated: a Finnish Semantic Tagger (FST), Connexor's NER tool and Polyglot's NER. © 2017 Association for Computing Machinery. ACM.},
	language = {English},
	urldate = {2017-06-01},
	booktitle = {{ACM} {Int}. {Conf}. {Proc}. {Ser}.},
	publisher = {Association for Computing Machinery},
	author = {Kettunen, K. and Ruokolainen, T.},
	year = {2017},
	note = {Journal Abbreviation: ACM Int. Conf. Proc. Ser.},
	keywords = {Chemical compounds, Classification (of information), Data mining, Evaluation results, Extraction procedure, Finnish, HEMDIG - SCOPUS, Historical newspaper collections, Historical newspapers, Historical records, Named entity recognition, Named Entity Recognition, Natural language processing systems, Newsprint, Persons and organizations, SCOPUS, Semantics, Web collections},
	pages = {181--186},
}

@inproceedings{xu_retrieving_2017,
	title = {Retrieving and {Combining} {Repeated} {Passages} to {Improve} {OCR}},
	isbn = {15525996 (ISSN); 9781538638613 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028025577&doi=10.1109%2fJCDL.2017.7991587&partnerID=40&md5=f9f4917706b39d9661899847fb81cc67},
	doi = {10.1109/JCDL.2017.7991587},
	abstract = {We present a novel approach to improve the output of optical character recognition (OCR) systems by first detecting duplicate passages in their output and then performing consensus decoding combined with a language model. This approach is orthogonal to, and may be combined with, previously proposed methods for combining the output of different OCR systems on the same image or the output of the same OCR system on differently processed images of the same text. It may also be combined with methods to estimate the parameters of a noisy channel model of OCR errors. Additionally, the current method generalizes previous proposals for a simple majority- vote combination of known duplicated texts. On a corpus of historical newspapers, an annotated set of clusters has a baseline word error rate (WER) of 33\%. A majority vote procedure reaches 23\% on passages where one or more duplicates were found, and consensus decoding combined with a language model achieves 18\% WER. In a separate experiment, newspapers were aligned to very widely reprinted texts such as State of the Union speeches, producing clusters with up to 58 witnesses. Beyond 20 witnesses, majority vote outperforms language model rescoring, though the gap between them is much less in this experiment. © 2017 IEEE.},
	language = {English},
	urldate = {2017-06-19},
	booktitle = {Proc. {ACM} {IEEE} {Joint} {Conf}. {Digit}. {Libr}.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Xu, S. and Smith, D.},
	collaborator = {{Elsevier; et al.; IEEE; Libraries Bloomington; University of Toronto Faculty of Information; University of Toronto Libraries}},
	year = {2017},
	note = {Journal Abbreviation: Proc. ACM IEEE Joint Conf. Digit. Libr.},
	keywords = {Character recognition, Computational linguistics, Decoding, Digital libraries, HEMDIG - SCOPUS, Historical newspapers, Language model, Majority vote, Newsprint, Noisy channel models, Optical character recognition, Optical character recognition (OCR), Processed images, SCOPUS, Simple majority, Speech recognition, Word error rate},
}

@book{van_erp_constructing_2018,
	series = {17th {International} {Semantic} {Web} {Conference}, {ISWC} 2018},
	title = {Constructing a recipe web from historical newspapers},
	volume = {11136 LNCS},
	isbn = {03029743 (ISSN); 9783030006709 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054854478&doi=10.1007%2f978-3-030-00671-6_13&partnerID=40&md5=5f589c78e990eeee4796aec1ee65b51d},
	abstract = {Historical newspapers provide a lens on customs and habits of the past. For example, recipes published in newspapers highlight what and how we ate and thought about food. The challenge here is that newspaper data is often unstructured and highly varied. Digitised historical newspapers add an additional challenge, namely that of fluctuations in OCR quality. Therefore, it is difficult to locate and extract recipes from them. We present our approach based on distant supervision and automatically extracted lexicons to identify recipes in digitised historical newspapers, to generate recipe tags, and to extract ingredient information. We provide OCR quality indicators and their impact on the extraction process. We enrich the recipes with links to information on the ingredients. Our research shows how natural language processing, machine learning, and semantic web can be combined to construct a rich dataset from heterogeneous newspapers for the historical analysis of food culture. © Springer Nature Switzerland AG 2018.},
	language = {English},
	urldate = {2018-10-08},
	publisher = {Springer Verlag},
	author = {van Erp, M. and Wevers, M. and Huurdeman, H.},
	editor = {{Suarez-Figueroa M.C.} and {Presutti V.} and {Kaffee L.} and {Simperl E.} and {Sabou M.} and {Vrandecic D.} and {Celino I.} and {Bontcheva K.}},
	year = {2018},
	doi = {10.1007/978-3-030-00671-6_13},
	note = {Journal Abbreviation: Lect. Notes Comput. Sci.
Pages: 232
Publication Title: Lect. Notes Comput. Sci.},
	keywords = {Artificial intelligence, Digital humanities, Digitised newspapers, Extraction process, Food history, HEMDIG - SCOPUS, Historical analysis, Historical newspapers, Information extraction, Information retrieval, Learning algorithms, Learning systems, Natural language processing, Natural language processing systems, Newsprint, Quality indicators, Recipe tags, SCOPUS, Semantic Web},
}

@inproceedings{dong_multi-input_2018,
	title = {Multi-input attention for unsupervised {OCR} correction},
	volume = {1},
	isbn = {9781948087322 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063100688&doi=10.18653%2fv1%2fp18-1220&partnerID=40&md5=693e2484b5f4a83f0910c524e3124915},
	doi = {10.18653/v1/p18-1220},
	abstract = {We propose a novel approach to OCR post-correction that exploits repeated texts in large corpora both as a source of noisy target outputs for unsupervised training and as a source of evidence when decoding. A sequence-to-sequence model with attention is applied for single-input correction, and a new decoder with multi-input attention averaging is developed to search for consensus among multiple sequences. We design two ways of training the correction model without human annotation, either training to match noisily observed textual variants or bootstrapping from a uniform error model. On two corpora of historical newspapers and books, we show that these unsupervised techniques cut the character and word error rates nearly in half on single inputs and, with the addition of multi-input decoding, can rival supervised methods. © 2018 Association for Computational Linguistics},
	language = {English},
	urldate = {2018-07-15},
	booktitle = {{ACL} - {Annu}. {Meet}. {Assoc}. {Comput}. {Linguist}., {Proc}. {Conf}. ({Long} {Pap}.)},
	publisher = {Association for Computational Linguistics (ACL)},
	author = {Dong, R. and Smith, D.A.},
	collaborator = {{Apple; ByteDance; et al.; Facebook; Google; Samsung Research}},
	year = {2018},
	note = {Journal Abbreviation: ACL - Annu. Meet. Assoc. Comput. Linguist., Proc. Conf. (Long Pap.)},
	keywords = {Computational linguistics, Correction models, Decoding, HEMDIG - SCOPUS, Historical newspapers, Human annotations, Multiple sequences, SCOPUS, Sequence modeling, Supervised methods, Unsupervised techniques, Unsupervised training},
	pages = {2363--2372},
}

@inproceedings{kettunen_open_2019,
	title = {Open source tesseract in re-{OCR} of {Finnish} {Fraktur} from 19th and early 20th century newspapers and journals – {Collected} notes on quality improvement},
	volume = {2364},
	isbn = {16130073 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066018333&partnerID=40&md5=737425205e3a539cc1fa6b12ad58954a},
	abstract = {This paper presents work that has been carried out in the National Library of Finland to improve optical character recognition (OCR) quality of a Finnish historical newspaper and journal collection 1771–1910. Work and results reported in the paper are based on a 500 000 word ground truth (GT) sample of the Finnish language part of the whole collection. The sample has three different parallel parts: a manually corrected ground truth version, original OCR with ABBYY FineReader v. 7 or v. 8, and an ABBYY FineReader v. 11 re-OCRed version. Based on this sample and its page image originals we have developed a re-OCRing process using the open source software package Tesseract1 v. 3.04.01. Our methods in the re-OCR include image preprocessing techniques, usage of morphological analyzers and a set of weighting rules for resulting candidate words. Besides results based on the GT sample we present also results of re-OCR for a 29 year period of one newspaper of our collection, Uusi Suometar. The paper describes the results of our re-OCR process including the latest results. We also state some of the main lessons learned during the development work. © 2019 CEUR-WS. All rights reserved.},
	language = {English},
	urldate = {2019-03-05},
	booktitle = {{CEUR} {Workshop} {Proc}.},
	publisher = {CEUR-WS},
	author = {Kettunen, K. and Koistinen, M.},
	editor = {{Navarretta C.} and {Agirrezabal M.} and {Maegaard B.}},
	year = {2019},
	note = {Journal Abbreviation: CEUR Workshop Proc.},
	keywords = {Finnish, HEMDIG - SCOPUS, Historical newspapers, Image preprocessing, Image processing, Morphological analyzer, National libraries, Newsprint, OCR, Open source software, Open systems, Optical character recognition, Optical character recognition (OCR), Quality improvement, SCOPUS, Tesseract},
	pages = {270--282},
}

@inproceedings{kettunen_tagging_2017,
	title = {Tagging {Named} {Entities} in 19th {Century} and {Modern} {Finnish} {Newspaper} {Material} with a {Finnish} {Semantic} {Tagger}},
	isbn = {9789176856017 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081759890&partnerID=40&md5=c36a8d1b1904e2ea5b1450cf3d115698},
	abstract = {Named Entity Recognition (NER), search, classification and tagging of names and name like informational elements in texts, has become a standard information extraction procedure for textual data during the last two decades. NER has been applied to many types of texts and different types of entities: newspapers, fiction, historical records, persons, locations, chemical compounds, protein families, animals etc. In general a NER system’s performance is genre and domain dependent. Also used entity categories vary a lot (Nadeau and Sekine, 2007). The most general set of named entities is usually some version of three part categorization of locations, persons and corporations. In this paper we report evaluation results of NER with two different data: digitized Finnish historical newspaper collection Digi and modern Finnish technology news, Digitoday. Historical newspaper collection Digi contains 1,960,921 pages of newspaper material from years 1771–1910 both in Finnish and Swedish. We use only material of Finnish documents in our evaluation. The OCRed newspaper collection has lots of OCR errors; its estimated word level correctness is about 70–75\%, and its NER evaluation collection consists of 75 931 words (Kettunen and Pääkkönen, 2016; Kettunen et al., 2016). Digitoday’s annotated collection consists of 240 articles in six different sections of the newspaper. Our new evaluated tool for NER tagging is non-conventional: it is a rule-based Finnish Semantic Tagger, the FST (Löfberg et al., 2005), and its results are compared to those of a standard rule-based NE tagger, FiNER. © 2017 Linköping University Electronic Press.},
	language = {English},
	urldate = {2017-05-23},
	booktitle = {{NoDaLiDa} - {Nordic} {Conf}. {Comput}. {Linguist}., {Proc}. {Conf}.},
	publisher = {Association for Computational Linguistics (ACL)},
	author = {Kettunen, K. and Löfberg, L.},
	editor = {{Tiedemann J.}},
	year = {2017},
	note = {Journal Abbreviation: NoDaLiDa - Nordic Conf. Comput. Linguist., Proc. Conf.},
	keywords = {19th century, Classification (of information), Computational linguistics, Data mining, Entity search, Entity tagging, Finnish, HEMDIG - SCOPUS, Historical newspapers, Named entities, Named entity recognition, Natural language processing systems, Newsprint, Rule based, SCOPUS, Semantic tags, Semantics, Textual data},
	pages = {29--36},
}

@inproceedings{koistinen_improving_2017,
	title = {Improving {Optical} {Character} {Recognition} of {Finnish} {Historical} {Newspapers} with a {Combination} of {Fraktur} \& {Antiqua} {Models} and {Image} {Preprocessing}},
	isbn = {9789176856017 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122977243&partnerID=40&md5=2f84c0d44a98dfcee69ccd2dbfd7536d},
	abstract = {In this paper we describe a method for improving the optical character recognition (OCR) toolkit Tesseract for Finnish historical documents. First we create a model for Finnish Fraktur fonts. Second we test Tesseract with the created Fraktur model and Antiqua model on single images and combinations of images with different image preprocessing methods. Against commercial ABBYY FineReader toolkit our method achieves 27.48\% (FineReader 7 or 8) and 9.16\% (FineReader 11) improvement on word level. © 2017 Linköping University Electronic Press.},
	language = {English},
	urldate = {2017-05-23},
	booktitle = {{NoDaLiDa} - {Nordic} {Conf}. {Comput}. {Linguist}., {Proc}. {Conf}.},
	publisher = {Association for Computational Linguistics (ACL)},
	author = {Koistinen, M. and Kettunen, K. and Pääkkönen, T.},
	editor = {{Tiedemann J.}},
	year = {2017},
	note = {Journal Abbreviation: NoDaLiDa - Nordic Conf. Comput. Linguist., Proc. Conf.},
	keywords = {Binarization, Binarizations, Digital image processing, Finnish, HEMDIG - SCOPUS, Historical documents, Historical newspapers, History, Image enhancement, Image preprocessing, Noise removal, Noises removal, OCR quality, Optical character recognition, Optical character recognition quality, Optical data processing, Pre-processing method, SCOPUS, Single images, Tesseract},
	pages = {277--283},
}

@book{milnovic_new_2018,
	series = {International {Workshop} on {Advances} in {Digital} {Cultural} {Heritage}, 2017},
	title = {New {Horizon} of {Digitization} in {Serbia} {Improvement} of {Digitization} {Through} {Cooperation} with {Leading} {World} {Institutions} and the {In}-{House} {Development} of {Digital} {Tools}},
	volume = {10754 LNCS},
	isbn = {03029743 (ISSN); 9783319757889 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042526259&doi=10.1007%2f978-3-319-75789-6_6&partnerID=40&md5=4eb7907aa57df8f5c1af9e112d5fb578},
	abstract = {In 2016 the University Library “Svetozar Markovic” in cooperation with eight partner institutions started a project entitled “New Horizon of Digitization in Serbia”, which was funded by the Ministry of Culture and Information of the Republic of Serbia. The aim of the project was to improve the main segments of digitization that were developed properly and to apply new digitization methods in as many institutions as possible in Serbia in a standardized way. The new digitization methods and approaches used in this project mostly come from the cooperation of the University Library “Svetozar Markovic” with leading world institutions or result from the in-house development of digital tools to improve segments of the digitization process. During 2016, the University Library “Svetozar Markovic” in cooperation with the British National Library carried out the project entitled “Safeguarding the fragile collection of the private archive of the Lazic family” within which it acquired rich experience in enhancing the digitization process. This paper presents several segments which show how the digitization process has been improved within the project. The paper also outlines the need for the full searchability of digital text in materials that are presented to patrons and that portray the University Library “Svetozar Markovic” experience through the example of a searchable collection of historical newspapers. Two methods for creating searchable texts are outlined: via the creation of METS-ALTO files in cooperation with the National Library of Luxembourg in the case of printed materials and via automatic recognition of the handwritten text in the Horizon 2020 READ project. Finally, the needs of patron groups in working with digital materials and possibilities for their display and promotion in the modern business environment of libraries are analyzed. This paper describes in detail the experience of using the promotional device Magic Box at the University Library “Svetozar Markovic” and outlines analysis and scenarios for its application in cultural institutions. This paper is an overview of the possibilities and first experiences of improving the digitization process which will mark the following development of the field in Serbia and the region. © 2018, Springer International Publishing AG, part of Springer Nature.},
	language = {English},
	urldate = {2017-06-28},
	publisher = {Springer Verlag},
	author = {Milnovic, V. and Jerkov, A.},
	editor = {{Zarnic R.} and {Ioannides M.} and {Lim V.} and {Martins J.}},
	year = {2018},
	doi = {10.1007/978-3-319-75789-6_6},
	note = {Journal Abbreviation: Lect. Notes Comput. Sci.
Pages: 88
Publication Title: Lect. Notes Comput. Sci.},
	keywords = {Automatic recognition, Business environments, Character recognition, Cultural institutions, Digital devices, Digital libraries, Digital tools, Digitization process, HEMDIG - SCOPUS, Historical newspapers, In-house development, Libraries, Partner institutions, SCOPUS, University libraries, University library “svetozar markovic”},
}

@inproceedings{kettunen_creating_2018,
	title = {Creating and using ground truth {OCR} sample data for {Finnish} historical newspapers and journals},
	volume = {2084},
	isbn = {16130073 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045343540&partnerID=40&md5=795b1cf5f8b0c64798e054437691780b},
	abstract = {The National Library of Finland (NLF) has digitized historical newspapers, journals and ephemera published in Finland since the late 1990s. The present collection consists of about 12.9 million pages mainly in Finnish and Swedish. Out of these about 7.36 million pages are freely available on the web site digi.kansalliskirjasto.fi. The copiright restricted part of the collection can be used at six legal deposit libraries in different parts of Finland. The time period of the open collection is from 1771 to 1929. The years 1920-1929 were opened in January 2018. This paper presents the ground truth Optical Character Recognition data of about 500 000 Finnish words that has been compiled at the NLF for development of a new OCR process for the collection. We discuss compilation of the data and show basic results of the new OCR process in comparison to current OCR using the ground truth data. © 2018 CEUR-WS. All rights reserved.},
	language = {English},
	urldate = {2018-03-07},
	booktitle = {{CEUR} {Workshop} {Proc}.},
	publisher = {CEUR-WS},
	author = {Kettunen, K. and Kervinen, J. and Koistinen, M.},
	editor = {{Tolonen M.} and {Tuominen J.} and {Makela E.}},
	year = {2018},
	note = {Journal Abbreviation: CEUR Workshop Proc.},
	keywords = {Basic results, Finnish, Ground truth, Ground truth data, HEMDIG - SCOPUS, Historical newspaper collections, Historical newspapers, National libraries, Newsprint, Optical character recognition, Sample data, SCOPUS, Time-periods},
	pages = {162--169},
}

@inproceedings{la_mela_finding_2019,
	title = {Finding nineteenth-century berry spots: {Recognizing} and linking place names in a historical newspaper berry-picking corpus},
	volume = {2364},
	isbn = {16130073 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066029263&partnerID=40&md5=8280628ebd238d5d3445ac4c0b78a77b},
	abstract = {The paper studies and improves methods of named entity recognition (NER) and linking (NEL) for facilitating historical research, which uses digitized newspaper texts. The specific focus is on a study about historical process of commodification. The named entity detection pipeline is discussed in three steps. First, the paper presents the corpus, which consists of newspaper articles on wild berry picking from the late nineteenth century. Second, the paper compares two named entity recognition tools: the trainable Stanford NER and the rule-based FiNER. Third, the linking and disambiguation of the recognized places is explored. In the linking process, information about the newspaper publication place is used to improve the identification of small places. The paper concludes that the pipeline performs well for mapping the commodification, and that specific problems relate to the recognition of place names (among named entities). It is shown how Stanford NER performs better in the task (F-score of 0.83) than the FiNER tool (F-score of 0.68). Concerning the linking of places, the use of newspaper metadata appears useful for disambiguation between small places. However, the historical language (with its OCR errors) recognized by the Stanford model poses challenges for the linking tool. The paper proposes that other information, for instance about the reuse of the newspaper articles, could be used to further improve the recognition and linking quality. © 2019 CEUR-WS. All rights reserved.},
	language = {English},
	urldate = {2019-03-05},
	booktitle = {{CEUR} {Workshop} {Proc}.},
	publisher = {CEUR-WS},
	author = {La Mela, M.L. and Tamper, M. and Kettunen, K.},
	editor = {{Agirrezabal M.} and {Maegaard B.} and {Navarretta C.}},
	year = {2019},
	note = {Journal Abbreviation: CEUR Workshop Proc.},
	keywords = {Berry picking, Commodification, Fruits, HEMDIG - SCOPUS, Historical newspapers, Named entities, Named entity linking, Named entity recognition, Natural language processing systems, Newsprint, Pipelines, SCOPUS},
	pages = {295--307},
}

@inproceedings{maegaard_b_ceur_2019,
	title = {{CEUR} {Workshop} {Proceedings}},
	volume = {2364},
	isbn = {16130073 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066055181&partnerID=40&md5=bd8373e579c96c8b3ec710da7f512e88},
	abstract = {The proceedings contain 45 papers. The topics discussed include: exploring the quality of the digital historical newspaper archive KubHist; creating vocabulary exercises through NLP; annotation of subtitle paraphrases using a new web tool; managing uncertainties: small-scale crowdsourcing of author letters; automatic dating of medieval charters from Denmark; new applications of gaze tracking in speech science; a corpus of regional American language from YouTube; faces, fights, and families: topic modeling and gendered themes in two corpora of Swedish prose fiction; evaluation and refinement of an enhanced OCR process for mass digitization; and distinguishing narration and speech in prose fiction dialogues.},
	language = {English},
	urldate = {2019-03-05},
	booktitle = {{CEUR} {Workshop} {Proc}.},
	publisher = {CEUR-WS},
	editor = {{Maegaard B.} and {Agirrezabal M.} and {Navarretta C.}},
	year = {2019},
	note = {Journal Abbreviation: CEUR Workshop Proc.},
	keywords = {HEMDIG - SCOPUS, SCOPUS},
}

@inproceedings{drobac_improving_2019,
	title = {Improving {OCR} of historical newspapers and journals published in {Finland}},
	isbn = {9781450371940 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074887718&doi=10.1145%2f3322905.3322914&partnerID=40&md5=092ae594104542e2128457bac13c0db4},
	doi = {10.1145/3322905.3322914},
	abstract = {This paper presents experiments on Optical character recognition (OCR) of historical newspapers and journals published in Finland. The corpus has two main languages: Finnish and Swedish and is written in both Blackletter and Antiqua fonts. Here we experiment with how much training data is enough to train high accuracy models, and try to train a joint model for both languages and all fonts. So far we have not been successful in getting one best model for all, but it is promising that with the mixed model we get the best results on the Finnish test set with 95 \% CAR, which clearly surpasses previous results on this data set. © 2019 Copyright held by the owner/author(s).},
	language = {English},
	urldate = {2019-05-08},
	booktitle = {{ACM} {Int}. {Conf}. {Proc}. {Ser}.},
	publisher = {Association for Computing Machinery},
	author = {Drobac, S. and Kauppinen, P. and Lindén, K.},
	year = {2019},
	note = {Journal Abbreviation: ACM Int. Conf. Proc. Ser.},
	keywords = {Best model, HEMDIG - SCOPUS, High-accuracy, Historical newspapers, Joint modeling, Mixed modeling, Newsprint, Optical character recognition, Optical character recognition (OCR), SCOPUS, Statistical tests, Test sets, Training data},
	pages = {97--102},
}

@article{kettunen_ground_2020,
	title = {Ground truth ocr sample data of finnish historical newspapers and journals in data improvement validation of a re-ocring process},
	volume = {30},
	issn = {14355205 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079483501&doi=10.18352%2flq.10322&partnerID=40&md5=dc21a7cff28c4e159b637a1d1d391cc2},
	doi = {10.18352/lq.10322},
	abstract = {The National Library of Finland (NLF) has digitized historical newspapers, journals and ephemera published in Finland since the late 1990s. The present collection consists of about 16.51 million pages mainly in Finnish and Swedish. Out of these about 7.64 million pages are freely available on the web site https:/ / digi.kansalliskirjasto.f / etusivu. The copyright restricted part of the collection can be used at six legal deposit libraries in different parts of Finland. The time period of the open collection is from 1771 to 1929. The last nine years, 1921–1929, were opened in January 2018. This paper presents brief y the ground truth Optical Character Recognition data of about 500,000 words that has been compiled at the NLF for development of an improved OCR process for the Finnish collection. We discuss compilation of the data generally and show results of the new OCR process in comparison to current OCR, using the ground truth data as an evaluation benchmark. We also show with real newspaper data of 30 years and 109 million words that the re-OCRing process is improving the quality of the OCRed data. © 2020, Igitur, Utrecht Publishing and Archiving Services. All rights reserved.},
	language = {English},
	number = {1},
	journal = {LIBER Quarterly},
	author = {Kettunen, K. and Koistinen, M. and Kervinen, J.},
	year = {2020},
	note = {Publisher: Igitur, Utrecht Publishing and Archiving Services},
	keywords = {Evaluation, Finnish historical newspapers, Ground truth data, HEMDIG - SCOPUS, Measurement, OCR quality, SCOPUS},
	pages = {1--20},
}

@inproceedings{dannells_supervised_2020,
	title = {Supervised {OCR} post-correction of historical {Swedish} texts: {What} role does the {OCR} system play?},
	volume = {2612},
	isbn = {16130073 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086073064&partnerID=40&md5=b79672182270dcdc923828917900373d},
	abstract = {Current approaches for post-correction of OCR errors offer solutions that are tailored to a specific OCR system. This can be problematic if the post-correction method was trained on a specific OCR system but have to be applied on the result of another system. Whereas OCR post-correction of historical text has received much attention lately, the question of what role does the OCR system play for the post-correction method has not been addressed. In this study we explore a dataset of 400 documents of historical Swedish text which has been OCR processed by three state-of-the-art OCR systems: Abbyy Finereader, Tesseract and Ocropus. We examine the OCR results of each system and present a supervised machine learning post-correction method that tries to approach the challenges exhibited by each system. We study the performance of our method by using three evaluation tools: PrimA, Språkbanken evaluation tool and Frontiers Toolkit. Based on the evaluation analysis we discuss the impact each of the OCR systems has on the results of the post-correction method. We report on quantitative and qualitative results showing varying degrees of OCR post-processing complexity that are important to consider when developing an OCR post-correction method. Copyright © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
	language = {English},
	urldate = {2020-10-21},
	booktitle = {{CEUR} {Workshop} {Proc}.},
	publisher = {CEUR-WS},
	author = {Dannélls, D. and Persson, S.},
	editor = {{Reinsone S.} and {Skadina I.} and {Baklane A.} and {Daugavietis J.}},
	year = {2020},
	note = {Journal Abbreviation: CEUR Workshop Proc.},
	keywords = {Correction method, Electromagnetic wave attenuation, Evaluation tool, HEMDIG - SCOPUS, Historical Newspaper Corpus, Machine Learning, OCR, OCR systems, Optical character recognition, Post processing, SCOPUS, State of the art, Supervised learning, Supervised machine learning, Swedishs, Tesseract},
	pages = {24--37},
}

@inproceedings{adesam_exploring_2019,
	title = {Exploring the quality of the digital historical newspaper archive kubhist},
	volume = {2364},
	isbn = {16130073 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066036703&partnerID=40&md5=649c96ef49fb24b1a8e7da1b8cc80eb5},
	abstract = {The KubHist Corpus is a massive corpus of Swedish historical newspapers, digitized by the Royal Swedish library, and available through the Språkbanken corpus infrastructure Korp. This paper contains a first overview of the KubHist corpus, exploring some of the difficulties with the data, such as OCR errors and spelling variation, and discussing possible paths for improving the quality and the searchability. © 2019 CEUR-WS. All rights reserved.},
	language = {English},
	urldate = {2019-03-05},
	booktitle = {{CEUR} {Workshop} {Proc}.},
	publisher = {CEUR-WS},
	author = {Adesam, Y. and Dannélls, D. and Tahmasebi, N.},
	editor = {{Maegaard B.} and {Navarretta C.} and {Agirrezabal M.}},
	year = {2019},
	note = {Journal Abbreviation: CEUR Workshop Proc.},
	keywords = {HEMDIG - SCOPUS, Historical newspaper corpus, Historical newspapers, Newsprint, OCR errors, SCOPUS, Searchability, Spelling normalization, Swedishs},
	pages = {9--17},
}

@inproceedings{riedl_clustering-based_2019,
	title = {Clustering-based article identification in historical newspapers},
	isbn = {9781950737000 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079286436&partnerID=40&md5=d1e6412d4811908c6ca4389d118344a9},
	abstract = {This article focuses on the problem of identifying articles and recovering their text from within and across newspaper pages when OCR just delivers one text file per page. We frame the task as a segmentation plus clustering step. Our results on a sample of 1912 New York Tribune magazine shows that performing the clustering based on similarities computed with word embeddings outperforms a similarity measure based on character n-grams and words. Furthermore, the automatic segmentation based on the text results in low scores, due to the low quality of some OCRed documents. © 2019 Association for Computational Linguistics.All right reserved.},
	language = {English},
	urldate = {2019-06-07},
	booktitle = {{LaTeCH}@{NAACL}-{HLT} - {Jt}. {SIGHUM} {Workshop} {Comput}. {Linguist}. {Cult}. {Herit}., {Soc}. {Sci}., {Humanit}. {Lit}., {Proc}.},
	publisher = {Association for Computational Linguistics (ACL)},
	author = {Riedl, M. and Betz, D. and Pado, S.},
	year = {2019},
	note = {Journal Abbreviation: LaTeCH@NAACL-HLT - Jt. SIGHUM Workshop Comput. Linguist. Cult. Herit., Soc. Sci., Humanit. Lit., Proc.},
	keywords = {Automatic segmentations, Clusterings, Computational linguistics, Embeddings, HEMDIG - SCOPUS, Historical newspapers, Low qualities, N-grams, New York, Newsprint, SCOPUS, Similarity measure, Text file},
	pages = {12--17},
}

@inproceedings{puthanveetil_satheesan_historical_2019,
	title = {A historical big data analysis to understand the social construction of juvenile delinquency in the {United} {States}},
	isbn = {9781728124513 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083209279&doi=10.1109%2feScience.2019.00094&partnerID=40&md5=68428db11a064b647adc2f245c9b3e1a},
	doi = {10.1109/eScience.2019.00094},
	abstract = {Social construction is a theoretical position that social reality is created through the humans' definition and interaction as opposed to something that exists by default. As one type of social reality, juvenile delinquency is perceived as part of social problems, deeply contextualized and socially constructed in American society. The social construction of juvenile delinquency started far earlier than the first juvenile court in 1899 in the U.S. Scholars have tried traditional historical analysis to explore the timeline of the social construction of juvenile delinquency in the past, but it is inefficient to examine hundred years of documents using traditional paper-pencil documenting method. We propose to research, develop and apply image and text analysis methods to analyze hundreds of years of newspaper data and show a clear development of social construction of juvenile delinquency in American society. The project aims to explore questions around how the media started depicting certain types of juvenile behavior as delinquency, how they described those behaviors; who are those juveniles (age, race, gender, family background, community background, etc.), how other social institutions treat those juveniles in those stories; how the depiction of juvenile delinquency has changed during the past 100 years; whether the analysis results support social construction perspective in terms of juvenile delinquency or not. In this paper, we present our ongoing work of doing image analysis on the newspaper collection from the Library of Congress Chronicling America website, initial results, observations, current conclusions, and future work. © 2019 IEEE.},
	language = {English},
	urldate = {2019-09-24},
	booktitle = {Proc. - {IEEE} {Int}. {Conf}. {eScience}, {eScience}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Puthanveetil Satheesan, S. and Craig, A.B. and Zhang, Y.},
	year = {2019},
	note = {Journal Abbreviation: Proc. - IEEE Int. Conf. eScience, eScience},
	keywords = {Big data, Digitized data, e-Science, HEMDIG - SCOPUS, Historical analysis, Historical newspaper collections, Image analysis, Juvenile delinquency, Library of congress, Newspaper article segmentation, Newsprint, Optical character recognition, SCOPUS, Social construction, Social constructions, Social problems, Social reality, Text analysis, Text-analysis methods},
	pages = {636--637},
}

@inproceedings{ruokolainen_name_2020,
	title = {Name the {Name} – {Named} {Entity} {Recognition} in {OCRed} 19th and {Early} 20th {Century} {Finnish} {Newspaper} and {Journal} {Collection} {Data}},
	volume = {2612},
	isbn = {16130073 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086078129&partnerID=40&md5=82770f258ce72cd6c26efb70cd819fc4},
	abstract = {Named Entity Recognition (NER), search, classification, and tagging of names and name like frequent informational elements in texts, has become a standard information extraction procedure for textual data. NER has been applied to many types of texts and different types of entities: newspapers, fiction, historical records, persons, locations, chemical compounds, protein families, animals etc. Performance of a NER system is usually quite heavily genre and domain dependent. Entity categories used in NER may also vary. The most used set of named entity categories is usually some version of three partite categorization of locations, persons, and organizations. In this paper we report evaluation results with data extracted from a digitized Finnish historical newspaper collection Digi using two statistical NER systems, namely, Stanford Named Entity Recognizer and LSTM-CRF NER model. The OCRed newspaper collection has lots of OCR errors; its estimated word level correctness is about 70–75\%. Our NER evaluation collection and training data are based on ca. 500 000 words which have been manually corrected from OCR output of ABBYY FineReader 11. We have also available evaluation data of new uncorrected OCR output of Tesseract 3.04.01. Our Stanford NER results are mostly satisfactory. With our ground truth data we achieve F-score of 0.89 with locations and 0.84 with persons. With organizations the result is 0.60. With re-OCRed Tesseract output the results are 0.79, 0.72, and 0.42, respectively. Results of LSTM-CRF are similar. Copyright © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
	language = {English},
	urldate = {2020-10-21},
	booktitle = {{CEUR} {Workshop} {Proc}.},
	publisher = {CEUR-WS},
	author = {Ruokolainen, T. and Kettunen, K.},
	editor = {{Reinsone S.} and {Skadina I.} and {Baklane A.} and {Daugavietis J.}},
	year = {2020},
	note = {Journal Abbreviation: CEUR Workshop Proc.},
	keywords = {Classification (of information), Data acquisition, Data mining, Evaluation, Evaluation results, Extraction procedure, Finnish, Ground truth data, HEMDIG - SCOPUS, Historical newspapers, Historical Newspapers, Historical records, Location, Long short-term memory, Named entities, Named entity recognition, Named Entity Recognition, Newsprint, OCR Data, Protein family, SCOPUS},
	pages = {137--156},
}

@article{kettunen_how_2020,
	title = {How to do lexical quality estimation of a large {OCRed} historical finnish newspaper collection with scarce resources},
	volume = {10},
	issn = {19183666 (ISSN)},
	doi = {10.16995/DSCN.315},
	abstract = {The National Library of Finland has digitized and made available the historical newspapers published in Finland between 1771 and 1910 (Bremer-Laamanen 2014; Kettunen et al. 2014). This collection contains approximately 1.95 million pages in Finnish and Swedish. The Finnish part of the collection consists of about 2.40 billion words. The National Library’s Digital Collections are offered via the digi.kansalliskirjasto.fi web service, also known as Digi. An open data package of the whole collection was released in early 2017 (Pääkkönen et al. 2016). Quality of OCRed collections is an important topic in digital humanities, as it affects general usability and searchability of collections. There is no single available method to assess quality of large collections, but different methods can be used to approximate quality. This paper discusses different corpus analysis style methods to approximate overall lexical quality of the Finnish part of the Digi collection. Methods include usage of parallel samples and word error rates, usage of morphological analysers, frequency analysis of words and comparisons to comparable edited lexical data. Our aim in the quality analysis is twofold: firstly to analyse the present state of the lexical data and secondly, to establish a set of assessment methods that build up a compact procedure for quality assessment after e.g. re-OCRing or post-correction of the material. © 2020 The Author(s).},
	language = {English},
	number = {1},
	journal = {Digital Studies/ Le Champ Numerique},
	author = {Kettunen, K.},
	year = {2020},
	note = {Publisher: Open Library of Humanities},
	keywords = {19th century Finnish newspaper collection, HEMDIG - SCOPUS, Lexical quality estimation, OCR quality, SCOPUS},
}

@book{koistinen_how_2020,
	series = {8th {Language} and {Technology} {Conference}: {Challenges} for {Computer} {Science} and {Linguistics}, {LTC} 2017},
	title = {How to {Improve} {Optical} {Character} {Recognition} of {Historical} {Finnish} {Newspapers} {Using} {Open} {Source} {Tesseract} {OCR} {Engine} – {Final} {Notes} on {Development} and {Evaluation}},
	volume = {12598 LNAI},
	isbn = {03029743 (ISSN); 9783030665265 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101563522&doi=10.1007%2f978-3-030-66527-2_2&partnerID=40&md5=08f62dba499a7dc943333e5037615d1f},
	abstract = {The current paper presents work that has been carried out in the National Library of Finland (NLF) to improve optical character recognition (OCR) quality of the historical Finnish newspaper collection 1771–1910. Evaluation results reported in the paper are based mainly on a 500 000 word sample of the Finnish language part of the whole collection. The sample has three different parallel parts: a manually corrected ground truth version, original OCR with ABBYY FineReader v. 7 or v. 8, and an ABBYY FineReader v. 11 re-OCRed version for comparison with Tesseract’s OCR. Using this sample and its page image originals we have developed a re-OCRing procedure using the open source software package Tesseract v. 3.04.01. Our method achieved initially 27.48\% improvement vs. ABBYY FineReader 7 or 8 and 9.16\% improvement vs. ABBYY FineReader 11 on document level. On word level our method achieved 36.25\% improvement vs. ABBYY FineReader 7 or 8 and 20.14\% improvement vs. ABBYY FineReader 11. Our final precision and recall results on word level show clear improvement in the quality: recall is 76.0 and precision 92.0 in comparison to GT OCR. Other measures, such as recognizability of words with a morphological analyzer and character accuracy rate, show also steady improvement after re-OCRing. © 2020, Springer Nature Switzerland AG.},
	language = {English},
	urldate = {2017-11-17},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Koistinen, M. and Kettunen, K. and Kervinen, J.},
	editor = {{Vetulani Z.} and {Paroubek P.} and {Kubis M.}},
	year = {2020},
	doi = {10.1007/978-3-030-66527-2_2},
	note = {Journal Abbreviation: Lect. Notes Comput. Sci.
Pages: 30
Publication Title: Lect. Notes Comput. Sci.},
	keywords = {Accuracy rate, Evaluation, Evaluation results, Finnish, Ground truth, HEMDIG - SCOPUS, Historical newspaper collections, Linguistics, Morphological analyzer, National libraries, Newsprint, Open source software, Open sources, Open systems, Optical character recognition, Optical character recognition (OCR), Precision and recall, SCOPUS},
}

@book{michael_icpr_2021,
	series = {25th {International} {Conference} on {Pattern} {Recognition} {Workshops}, {ICPR} 2020},
	title = {{ICPR} 2020 {Competition} on {Text} {Block} {Segmentation} on a {NewsEye} {Dataset}},
	volume = {12668 LNCS},
	isbn = {03029743 (ISSN); 9783030687922 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104450309&doi=10.1007%2f978-3-030-68793-9_30&partnerID=40&md5=708e73c0c0b5fc1a9524228c4ee4a363},
	abstract = {We present a competition on text block segmentation within the framework of the International Conference on Pattern Recognition (ICPR) 2020. The main goal of this competition is to automatically analyse the structure of historical newspaper pages with a subsequent evaluation of the participants’ algorithms performance. In contrast to many existing segmentation methods, instead of working on pixels, the present study has a focus on clustering baselines/text lines into text blocks. Therefore, we introduce a new measure based on a baseline detection evaluation scheme. But also common pixel-based approaches could participate without restrictions. Working on baseline level addresses directly the application scenario where for a given image the contained text should be extracted in blocks for further investigations. We present the results of three submissions. The experiments have shown that text blocks can be reliably detected both on pages with a simple layout and on pages with a complex layout. © 2021, Springer Nature Switzerland AG.},
	language = {English},
	urldate = {2021-01-10},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Michael, J. and Weidemann, M. and Laasch, B. and Labahn, R.},
	editor = {{Del Bimbo A.} and {Cucchiara R.} and {Sclaroff S.} and {Farinella G.M.} and {Mei T.} and {Bertini M.} and {Escalante H.J.} and {Vezzani R.}},
	year = {2021},
	doi = {10.1007/978-3-030-68793-9_30},
	note = {Journal Abbreviation: Lect. Notes Comput. Sci.
Pages: 418
Publication Title: Lect. Notes Comput. Sci.},
	keywords = {Application scenario, Baseline detection, Baseline levels, Block segmentations, Character recognition, Document image analysis, Evaluation scheme, HEMDIG - SCOPUS, Historical documents, Historical newspapers, Layout analysis, Pixel based approach, Pixels, SCOPUS, Segmentation methods, Text block segmentation},
}

@inproceedings{ito_extraction_2020,
	title = {Extraction of distinctive keywords and articles from untranscribed historical newspaper images},
	volume = {11515},
	isbn = {0277786X (ISSN); 9781510638358 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086628923&doi=10.1117%2f12.2566612&partnerID=40&md5=d5438781affedd4d6c417b7097acab60},
	doi = {10.1117/12.2566612},
	abstract = {This paper proposes a novel approach to extract distinctive keywords from historical newspaper images without using character recognition. We converted an image of the text block on an entire newspaper page into a sequence of codes based on discretization of the feature vectors, an approach that eliminated the errors introduced by optical character recognition (OCR). This conversion makes it possible to analyze untranscribed newspaper images by using text-processing methods. We examined the daily occurrence of every tri-gram string, and extracted strings with a dense appearance as distinctive keywords. In addition, we highlighted articles that contain distinctive keywords as distinctive articles. The proposed method was evaluated on an archive of Japanese newspaper images published in the 19th century, and the results were promising. © 2020 SPIE CCC.},
	language = {English},
	urldate = {2020-01-05},
	booktitle = {Proc {SPIE} {Int} {Soc} {Opt} {Eng}},
	publisher = {SPIE},
	author = {Ito, S. and Terasawa, K.},
	editor = {{Lau P.Y.} and {Shobri M.}},
	year = {2020},
	note = {Journal Abbreviation: Proc SPIE Int Soc Opt Eng},
	keywords = {19th century, Discretizations, Feature vectors, HEMDIG - SCOPUS, Historical documents, Historical newspapers, Imaging techniques, Keyword extraction, Newsprint, Optical character recognition, Optical character recognition (OCR), SCOPUS, Text processing, Tri grams},
}

@inproceedings{ehrmann_language_2020,
	title = {Language resources for historical newspapers: {The} impresso collection},
	isbn = {9791095546344 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095977149&partnerID=40&md5=5116d08004976b6db32e8cddf4bfb50b},
	abstract = {Following decades of massive digitization, an unprecedented amount of historical document facsimiles can now be retrieved and accessed via cultural heritage online portals. If this represents a huge step forward in terms of preservation and accessibility, the next fundamental challenge- and real promise of digitization- is to exploit the contents of these digital assets, and therefore to adapt and develop appropriate language technologies to search and retrieve information from this 'Big Data of the Past'. Yet, the application of text processing tools on historical documents in general, and historical newspapers in particular, poses new challenges, and crucially requires appropriate language resources. In this context, this paper presents a collection of historical newspaper data sets composed of text and image resources, curated and published within the context of the 'impresso - Media Monitoring of the Past' project. With corpora, benchmarks, semantic annotations and language models in French, German and Luxembourgish covering ca. 200 years, the objective of the impresso resource collection is to contribute to historical language resources, and thereby strengthen the robustness of approaches to non-standard inputs and foster efficient processing of historical documents. © European Language Resources Association (ELRA), licensed under CC-BY-NC},
	language = {English},
	urldate = {2020-05-11},
	booktitle = {{LREC} - {Int}. {Conf}. {Lang}. {Resour}. {Eval}., {Conf}. {Proc}.},
	publisher = {European Language Resources Association (ELRA)},
	author = {Ehrmann, M. and Romanello, M. and Clematide, S. and Ströbel, P.B. and Barman, R.},
	editor = {{Calzolari N.} and {Bechet F.} and {Blache P.} and {Choukri K.} and {Cieri C.} and {Declerck T.} and {Goggi S.} and {Isahara H.} and {Maegaard B.} and {Mariani J.} and {Mazo H.} and {Moreno A.} and {Odijk J.} and {Piperidis S.}},
	collaborator = {{Amazon AWS; Bertin; Lenovo; Ontotex; Vecsys; Vocapia}},
	year = {2020},
	note = {Journal Abbreviation: LREC - Int. Conf. Lang. Resour. Eval., Conf. Proc.},
	keywords = {Cultural heritages, Digital humanities, HEMDIG - SCOPUS, Historic preservation, Historical and multilingual language resources, Historical documents, Historical newspapers, Historical texts, Language resources, Language technology, Media monitoring, Multi-layered historical semantic annotations, Named entity processing, Newsprint, OCR, Resource collections, SCOPUS, Semantic annotations, Semantics, Text processing, Text reuse, Topic modeling},
	pages = {958--968},
}

@inproceedings{anderson_segmenting_2020,
	title = {Segmenting messy text: {Detecting} boundaries in text derived from historical newspaper images},
	isbn = {10514651 (ISSN); 9781728188089 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110522850&doi=10.1109%2fICPR48806.2021.9413279&partnerID=40&md5=36c913ef41ad50aa0202f5d15965e617},
	doi = {10.1109/ICPR48806.2021.9413279},
	abstract = {Text segmentation, the task of dividing a document into sections, is often a prerequisite for performing additional natural language processing tasks. Existing text segmentation methods have typically been developed and tested using clean, narrative-style text with segments containing distinct topics. Here we consider a challenging text segmentation task: dividing newspaper marriage announcement lists into units of one announcement each. In many cases the information is not structured into sentences, and adjacent segments are not topically distinct from each other. In addition, the text of the announcements, which is derived from images of historical newspapers via optical character recognition, contains many typographical errors. As a result, these announcements are not amenable to segmentation with existing techniques. We present a novel deep learning-based model for segmenting such text and show that it significantly outperforms an existing state-of-the-art method on our task. © 2020 IEEE},
	language = {English},
	urldate = {2021-01-10},
	booktitle = {Proc. {Int}. {Conf}. {Pattern} {Recognit}.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Anderson, C. and Crone, P.},
	year = {2020},
	note = {Journal Abbreviation: Proc. Int. Conf. Pattern Recognit.},
	keywords = {Deep learning, Document analysis, HEMDIG - SCOPUS, Historical newspapers, Information extraction, Learning Based Models, Natural language processing, NAtural language processing, Natural language processing systems, Newsprint, Optical character recognition, SCOPUS, State-of-the-art methods, Text mining, Text segmentation, Typographical errors},
	pages = {5543--5550},
}

@inproceedings{liebl_evaluation_2020,
	title = {An evaluation of {DNN} architectures for page segmentation of historical newspapers},
	isbn = {10514651 (ISSN); 9781728188089 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110525384&doi=10.1109%2fICPR48806.2021.9412571&partnerID=40&md5=fbd439eda9baace5fd91d820ea9af4aa},
	doi = {10.1109/ICPR48806.2021.9412571},
	abstract = {One important and particularly challenging step in the optical character recognition of historical documents with complex layouts, such as newspapers, is the separation of text from non-text content (e.g. page borders or illustrations). This step is commonly referred to as page segmentation. While various rule-based algorithms have been proposed, the applicability of Deep Neural Networks for this task recently has gained a lot of attention. In this paper, we perform a systematic evaluation of 11 different published backbone architectures and 9 different tiling and scaling configurations for separating text, tables or table column lines. We also show the influence of the number of labels and the number of training pages on the segmentation quality, which we measure using the Matthews Correlation Coefficient. Our results show that (depending on the task) Inception-ResNetv2 and EfficientNet backbones work best, vertical tiling is generally preferable to other tiling approaches, and training data that comprises 30 to 40 pages will be sufficient most of the time. © 2020 IEEE},
	language = {English},
	urldate = {2021-01-10},
	booktitle = {Proc. {Int}. {Conf}. {Pattern} {Recognit}.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Liebl, B. and Burghardt, M.},
	year = {2020},
	note = {Journal Abbreviation: Proc. Int. Conf. Pattern Recognit.},
	keywords = {Correlation coefficient, Deep neural networks, HEMDIG - SCOPUS, Historical documents, Historical newspapers, Image segmentation, Network architecture, Neural networks, Newsprint, Optical character recognition, Optical character recognition software, Page segmentation, Rule based algorithms, SCOPUS, Segmentation quality, Systematic evaluation, Training pages},
	pages = {5153--5160},
}

@inproceedings{provatorova_named_2020,
	title = {Named {Entity} {Recognition} and {Linking} on {Historical} {Newspapers}: {UvA}.{ILPS} \& {REL} at {CLEF} {HIPE} 2020},
	volume = {2696},
	isbn = {16130073 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121760479&partnerID=40&md5=b360b186cf8a91af0c19f6f22dd093e5},
	abstract = {This paper describes our submission to the CLEF HIPE 2020 shared task on identifying named entities in multi-lingual historical newspapers in French, German and English. The subtasks we addressed in our submission include coarse-grained named entity recognition, entity mention detection and entity linking. For the task of named entity recognition we used an ensemble of fine-tuned BERT models; entity linking was approached by three different methods: (1) a simple method relying on ElasticSearch retrieval scores, (2) an approach based on contextualised text embeddings, and (3) REL, a modular entity linking system based on several state-of-the-art components. Copyright © 2020 for this paper by its authors.},
	language = {English},
	urldate = {2020-09-22},
	booktitle = {{CEUR} {Workshop} {Proc}.},
	publisher = {CEUR-WS},
	author = {Provatorova, V. and Vakulenko, S. and Kanoulas, E. and Dercksen, K. and van Hulst, J.M.},
	editor = {{De Carolis B.} and {Gena C.} and {Lieto A.} and {Rossi S.} and {Sciutti A.}},
	year = {2020},
	note = {Journal Abbreviation: CEUR Workshop Proc.},
	keywords = {Character recognition, Coarse-grained, Digital libraries, Embeddings, HEMDIG - SCOPUS, Historical newspapers, Modulars, Named entities, Named entity linking, Named entity recognition, Newsprint, SCOPUS, SIMPLE method, State of the art, Subtask},
}

@inproceedings{skelbye_ocr_2021,
	title = {{OCR} {Processing} of {Swedish} {Historical} {Newspapers} {Using} {Deep} {Hybrid} {CNN}-{LSTM} {Networks}},
	isbn = {13138502 (ISSN); 9789544520724 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123612328&doi=10.26615%2f978-954-452-072-4_023&partnerID=40&md5=d67d45707cd2f36e1522e262617badc6},
	doi = {10.26615/978-954-452-072-4_023},
	abstract = {Deep CNN-LSTM hybrid neural networks have proven to improve the accuracy of Optical Character Recognition (OCR) models for different languages. In this paper we examine to what extent these networks improve the OCR accuracy rates on Swedish historical newspapers. By experimenting with the open source OCR engine Calamari, we are able to show that mixed deep CNN-LSTM hybrid models outperform previous models on the task of character recognition of Swedish historical newspapers spanning 1818-1848. We achieved an average character accuracy rate (CAR) of 97.43\% which is a new state-of-the-art result on 19th century Swedish newspaper text. Our data, code and models are released under CC BY licence. © 2021 Incoma Ltd. All rights reserved.},
	language = {English},
	urldate = {2021-09-01},
	booktitle = {Int. {Conf}. {Recent} {Adv}. {Nat}. {Lang}. {Proces}., {RANLP}},
	publisher = {Incoma Ltd},
	author = {Skelbye, M.B. and Dannélls, D.},
	editor = {{Angelova G.} and {Kunilovskaya M.} and {Mitkov R.} and {Nikolova-Koleva I.}},
	year = {2021},
	note = {Journal Abbreviation: Int. Conf. Recent Adv. Nat. Lang. Proces., RANLP},
	keywords = {Accuracy rate, HEMDIG - SCOPUS, Historical newspapers, Hybrid model, Hybrid neural networks, Long short-term memory, Newsprint, Open-source, Optical character recognition, Optical character recognition engines, Recognition accuracy, Recognition models, SCOPUS, State of the art, Swedishs},
	pages = {190--198},
}

@article{lu_bayesian_2021,
	title = {Bayesian damage recognition in document images based on a joint global and local homogeneity model},
	volume = {118},
	issn = {00313203 (ISSN)},
	doi = {10.1016/j.patcog.2021.108034},
	abstract = {Physical damages (such as torn-offs and scratches) are commonly seen in historical documents. Recognition of such damages is currently absent in digitization-and-information-extraction (DIE) systems but crucial for automatic document comprehension and exploitation. In this paper we propose a generic damage recognition (DR) method based on a joint global and local modeling of the text homogeneity (TH) pattern exhibited in document images. More specifically, a connected component (CC) based formulation is developed as a global homogeneity measure, where TH is characterized using a probabilistic graph model for a coarse recognition of damaged regions. A multi-resolution analysis (MRA) of TH is further developed for a granular within-CC recognition of damage pixels, where the disparity between damage and text pixels is characterized by exploiting neighborhood transitions. This enables the formulation of a local homogeneity measure, where the neighborhood transition around an individual pixel is modeled using the propagation of the approximation coefficients of a stationary wavelet transform (SWT). The proposed global and local homogeneity measures are integrated as a joint likelihood in a Bayesian model with a Markov random field (MRF) prior, where DR is formulated as a maximum a posterior (MAP) inference which is addressed using Markov Chain Monte Carlo (MCMC) sampling. The resulting algorithm is tested on a set of real-life historical newspaper images containing damages of varying size and shape. The performance of the algorithm is evaluated using both F-measures and the Intersection-over-Union (IoU) metric, where test results demonstrate the promising potential of the proposed method. © 2021 The Author(s)},
	language = {English},
	journal = {Pattern Recognition},
	author = {Lu, T. and Dooms, A.},
	year = {2021},
	note = {Publisher: Elsevier Ltd},
	keywords = {Bayesian, Bayesian inference, Bayesian networks, Character recognition, Connected component, Damage recognition, Document images, HEMDIG - SCOPUS, Image-based, Inference engines, Magnetorheological fluids, Markov processes, Neighborhood transition, Neighbourhood, Pixels, Propagation of wavelet approximation, SCOPUS, Structural frames, Text homogeneity, Wavelet transforms},
}

@article{pack_visual_2021,
	title = {Visual domain knowledge-based multimodal zoning for textual region localization in noisy historical document images},
	volume = {30},
	issn = {10179909 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122680308&doi=10.1117%2f1.JEI.30.6.063028&partnerID=40&md5=41c5af1b2f0dcef5c9a6d42b754cee6e},
	doi = {10.1117/1.JEI.30.6.063028},
	abstract = {Document layout analysis, or zoning, is important for textual content analysis such as optical character recognition. Zoning document images such as digitized historical newspaper pages are challenging due to noise and quality of the document images. Recently, effective data-driven approaches, such as leveraging deep learning, have been proposed, albeit with the concern of requiring larger training data and thus incurring additional cost of ground truthing. We propose a zoning solution by incorporating a knowledge-driven document representation, gravity map, into a multimodal deep learning framework to reduce the amount of time and data required for training. We first generate a gravity map for each image, considering the centroid distance and area between a cell in a Voronoi tessellation and its content to encode visual domain knowledge of a zoning task. Second, we inject the gravity maps into a deep convolution neural network (DCNN) during training, as an additional modality to boost performance. We report on two investigations using two state-of-the-art DCNN architectures and three datasets: two sets of historical newspapers and a set of born-digital contemporary documents. Evaluations show that our solution achieved comparable segmentation accuracy using fewer training epochs and less training data compared to a naïve training scheme. © 2021 SPIE and IS\&T.},
	language = {English},
	number = {6},
	journal = {Journal of Electronic Imaging},
	author = {Pack, C. and Soh, L.-K. and Lorang, E.},
	year = {2021},
	note = {Publisher: SPIE},
	keywords = {Deep learning, document image processing, Document image processing, Document images, Domain knowledge, Domain Knowledge, Gravity maps, HEMDIG - SCOPUS, Historical newspapers, image analysis, image decomposition, Image decomposition, image recognition, Image recognition, image segmentation, Image segmentation, Image-analysis, Images segmentations, Multi-modal, Newsprint, Optical character recognition, Optical data processing, SCOPUS, Training data, Zoning},
}

@inproceedings{di_nunzio_gm_ircdl_2022,
	title = {{IRCDL} 2022 - {Proceedings} of the 18th {Italian} {Research} {Conference} on {Digital} {Libraries}},
	volume = {3160},
	isbn = {16130073 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134254681&partnerID=40&md5=e7b93d735f69d855be3e45a8a62a09e1},
	abstract = {The proceedings contain 32 papers. The topics discussed include: OCR quality affects perceived usefulness of historical newspaper clippings – a user study; new information extracting and analysis methodology for the terminology research purposes: the field of biology; enhancement of scribal hands identification via self-supervised learning; the construction of the grammaticographical textual genre in grammars for Italian speakers with Spanish as a foreign language; negation detection for robust adverse drug event extraction from social media texts; toward reasoning-based recommendation of library items – a case study on the e-learning domain; artificial intelligence systems producing books: questions of agency; and European libraries and their virtual users: how the pandemic affected digital production and participation.},
	language = {English},
	urldate = {2022-02-24},
	booktitle = {{CEUR} {Workshop} {Proc}.},
	publisher = {CEUR-WS},
	editor = {{Di Nunzio G.M.} and {Portelli B.} and {Redavid D.} and {Silvello G.}},
	year = {2022},
	note = {Journal Abbreviation: CEUR Workshop Proc.},
	keywords = {HEMDIG - SCOPUS, SCOPUS},
}

@article{suissa_toward_2022,
	title = {Toward a {Period}-specific {Optimized} {Neural} {Network} for {OCR} {Error} {Correction} of {Historical} {Hebrew} {Texts}},
	volume = {15},
	issn = {15564673 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133692491&doi=10.1145%2f3479159&partnerID=40&md5=020f16aa9a65430283ee5d5795f5b866},
	doi = {10.1145/3479159},
	abstract = {Over the past few decades, large archives of paper-based historical documents, such as books and newspapers, have been digitized using the Optical Character Recognition (OCR) technology. Unfortunately, this broadly used technology is error-prone, especially when an OCRed document was written hundreds of years ago. Neural networks have shown great success in solving various text processing tasks, including OCR post-correction. The main disadvantage of using neural networks for historical corpora is the lack of sufficiently large training datasets they require to learn from, especially for morphologically rich languages like Hebrew. Moreover, it is not clear what are the optimal structure and values of hyperparameters (predefined parameters) of neural networks for OCR error correction in Hebrew due to its unique features. Furthermore, languages change across genres and periods. These changes may affect the accuracy of OCR post-correction neural network models. To overcome these challenges, we developed a new multi-phase method for generating artificial training datasets with OCR errors and hyperparameters' optimization for building an effective neural network for OCR post-correction in Hebrew. To evaluate the proposed approach, a series of experiments using several literary Hebrew corpora from various periods and genres were conducted. The obtained results demonstrate that (1) training a network on texts from a similar period dramatically improves the network's ability to fix OCR errors, (2) using the proposed error injection algorithm, based on character-level period-specific errors, minimizes the need for manually corrected data and improves the network accuracy by 9\%, (3) the optimized network design improves the accuracy by 3\% compared to the state-of-the-art network, and (4) the constructed optimized network outperforms neural machine translation models and industry-leading spellcheckers. The proposed methodology may have practical implications for digital humanities projects that aim to search and analyze OCRed documents in Hebrew and potentially other morphologically rich languages. © 2022 Association for Computing Machinery.},
	language = {English},
	number = {2},
	journal = {Journal on Computing and Cultural Heritage},
	author = {Suissa, O. and Zhitomirsky-Geffet, M. and Elmalech, A.},
	year = {2022},
	note = {Publisher: Association for Computing Machinery},
	keywords = {Character recognition errors, Computational linguistics, Computer aided language translation, dataset generation, Dataset generation, digital humanities, Digital humanities, DNN, Error correction, Errors correction, Hebrew, HEMDIG - SCOPUS, historical newspapers, Historical newspapers, Large dataset, neural machine translation, Neural machine translation, Neural-networks, Newsprint, OCR post-correction, Optical character recognition, Optical character recognition post-correction, SCOPUS, Structural optimization, Training dataset},
}

@inproceedings{satheesan_toward_2022,
	title = {Toward a big data analysis system for historical newspaper collections research},
	isbn = {9781450394109 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134839759&doi=10.1145%2f3539781.3539795&partnerID=40&md5=d975ea30002b9d0a6e230c6ccdc8cb30},
	doi = {10.1145/3539781.3539795},
	abstract = {The availability and generation of digitized newspaper collections have provided researchers in several domains with a powerful tool to advance their research. More specifically, digitized historical newspapers give us a magnifying glass into the past. In this paper, we propose a scalable and customizable big data analysis system that enables researchers to study complex questions about our society as depicted in news media for the past few centuries by applying cutting-edge text analysis tools to large historical newspaper collections. We discuss our experience with building a preliminary version of such a system, including how we have addressed the following challenges: processing millions of digitized newspaper pages from various publications worldwide, which amount to hundreds of terabytes of data; applying article segmentation and Optical Character Recognition (OCR) to historical newspapers, which vary between and within publications over time; retrieving relevant information to answer research questions from such data collections by applying human-in-the-loop machine learning; and enabling users to analyze topic evolution and semantic dynamics with multiple compatible analysis operators. We also present some preliminary results of using the proposed system to study the social construction of juvenile delinquency in the United States and discuss important remaining challenges to be tackled in the future. © 2022 ACM.},
	language = {English},
	urldate = {2022-06-27},
	booktitle = {Proc. {Platf}. {Adv}. {Sci}. {Comput}. {Conf}., {PASC}},
	publisher = {Association for Computing Machinery, Inc},
	author = {Satheesan, S.P. and {Bhavya} and Davies, A. and Craig, A.B. and Zhang, Y. and Zhai, C.},
	collaborator = {{Association for Computing Machinery�s (ACM) Special Interest Group on High Performance Computing (SIGHPC); Swiss National Supercomputing Centre (CSCS)}},
	year = {2022},
	note = {Journal Abbreviation: Proc. Platf. Adv. Sci. Comput. Conf., PASC},
	keywords = {Big data, Big data analyse system, big data analysis system, Cutting tools, Data analysis system, Data handling, data visualization, Data visualization, Deep learning, HEMDIG - SCOPUS, historical newspapers, Historical newspapers, image analysis, Image segmentation, Image-analysis, information retrieval, Information retrieval, juvenile delinquency, Juvenile delinquency, Language processing, Learning algorithms, natural language processing, Natural language processing, Natural language processing systems, Natural languages, newspaper article segmentation, Newspaper article segmentation, Newsprint, Optical character recognition, SCOPUS, Search engines, Semantics, social construction, Social constructions, social science research, Social science research, text analysis, Text analysis},
}

@inproceedings{pokrywka_challenging_2022,
	title = {Challenging {America}: {Modeling} language in longer time scales},
	isbn = {9781955917766 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137342060&partnerID=40&md5=36924565f59f619f65a879b9328f0305},
	abstract = {The aim of the paper is to apply, for historical texts, the methodology used commonly to solve various NLP tasks defined for contemporary data, i.e. pre-train and fine-tune large Transformer models. This paper introduces an ML challenge, named Challenging America (ChallAm), based on OCR-ed excerpts from historical newspapers collected from the Chronicling America portal. ChallAm provides a dataset of clippings, labeled with metadata on their origin, and paired with their textual contents retrieved by an OCR tool. Three, publicly available, ML tasks are defined in the challenge: to determine the article date, to detect the location of the issue, and to deduce a word in a text gap (cloze test). Strong baselines are provided for all three ChallAm tasks. In particular, we pretrained a RoBERTa model from scratch from the historical texts. We also discuss the issues of discrimination and hate-speech present in the historical American texts. © Findings of the Association for Computational Linguistics: NAACL 2022 - Findings.},
	language = {English},
	urldate = {2022-07-10},
	booktitle = {Find. {Assoc}. {Comput}. {Linguist}.: {NAACL}},
	publisher = {Association for Computational Linguistics (ACL)},
	author = {Pokrywka, J. and Gralinski, F. and Jassem, K. and Kaczmarek, K. and Jurkiewicz, K. and Wierzchon, P.},
	year = {2022},
	note = {Journal Abbreviation: Find. Assoc. Comput. Linguist.: NAACL},
	keywords = {Computational linguistics, HEMDIG - SCOPUS, Historical newspapers, Modeling languages, SCOPUS, Textual content, Time-scales, Transformer modeling},
	pages = {737--749},
}

@article{tanaka_corpus_2022,
	title = {Corpus {Construction} for {Historical} {Newspapers}: {A} {Case} {Study} on {Public} {Meeting} {Corpus} {Construction} {Using} {OCR} {Error} {Correction}},
	volume = {3},
	issn = {2662995X (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138759603&doi=10.1007%2fs42979-022-01393-6&partnerID=40&md5=144396185d0fe2a33d353a776d2e66a4},
	doi = {10.1007/s42979-022-01393-6},
	abstract = {Large text corpora are indispensable for natural language processing. However, in various fields such as literature and humanities, many documents to be studied are only scanned to images, but not converted to text data. Optical character recognition (OCR) is a technology to convert scanned document images into text data. However, OCR often misrecognizes characters due to the low quality of the scanned document images, which is a crucial factor that degrades the quality of constructed text corpora. This paper works on corpus construction for historical newspapers. We present a corpus construction method based on a pipeline of image processing, OCR, and filtering. To improve the quality, we further propose to integrate OCR error correction. To this end, we manually construct an OCR error correction dataset in the historical newspaper domain, propose methods to improve a neural OCR correction model and compare various OCR error correction models. We evaluate our corpus construction method on the accuracy of extracting articles of a specific topic to construct a historical newspaper corpus. As a result, our method improves the article extraction F score by 1.7 \% via OCR error correction comparing to previous work. This verifies the effectiveness of OCR error correction for corpus construction. © 2022, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd.},
	language = {English},
	number = {6},
	journal = {SN Computer Science},
	author = {Tanaka, K. and Chu, C. and Kajiwara, T. and Nakashima, Y. and Takemura, N. and Nagahara, H. and Fujikawa, T.},
	year = {2022},
	note = {Publisher: Springer},
	keywords = {Corpus construction, HEMDIG - SCOPUS, Historical newspapers, OCR error correction, Public meeting, SCOPUS},
}

@book{hamdi_assessing_2020,
	series = {24th {International} {Conference} on {Theory} and {Practice} of {Digital} {Libraries}, {TPDL} 2020},
	title = {Assessing and {Minimizing} the {Impact} of {OCR} {Quality} on {Named} {Entity} {Recognition}},
	volume = {12246 LNCS},
	isbn = {03029743 (ISSN); 9783030549558 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090099712&doi=10.1007%2f978-3-030-54956-5_7&partnerID=40&md5=8b1df2eaee13342237ff55c6526c9b7a},
	abstract = {In digital libraries, the accessibility of digitized documents is directly related to the way they are indexed. Named entities are one of the main entry points used to search and retrieve digital documents. However, most digitized documents are indexed through their OCRed version and OCR errors may hinder their accessibility. This paper aims to quantitatively estimate the impact of OCR quality on the performance of named entity recognition (NER). We tested state-of-the-art NER techniques over several evaluation benchmarks, and experimented with various levels and types of synthesised OCR noise so as to estimate the impact of OCR noise on NER performance. We share all corresponding datasets. To the best of our knowledge, no other research work has systematically studied the impact of OCR on named entity recognition over datasets in multiple languages. The final outcome of this study is an evaluation over historical newspaper data of the national library of Finland, resulting in an increase of around 11\% points in terms of F1-measure over the best-known results to this day. © 2020, Springer Nature Switzerland AG.},
	language = {English},
	urldate = {2020-08-25},
	publisher = {Springer},
	author = {Hamdi, A. and Jean-Caurant, A. and Sidère, N. and Coustaty, M. and Doucet, A.},
	editor = {{Hall M.} and {Mercun T.} and {Risse T.} and {Duchateau F.}},
	year = {2020},
	doi = {10.1007/978-3-030-54956-5_7},
	note = {Journal Abbreviation: Lect. Notes Comput. Sci.
Pages: 101
Publication Title: Lect. Notes Comput. Sci.},
	keywords = {Benchmarking, Digital Documents, Digital libraries, Digitized documents, Entry point, HEMDIG - SCOPUS, Historical newspapers, Indexing, Multiple languages, Named entities, Named entity recognition, National libraries, Natural language processing systems, OCR, SCOPUS, State of the art},
}
