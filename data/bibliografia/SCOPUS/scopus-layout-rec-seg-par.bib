
@inproceedings{neudecker_making_2016,
	title = {Making {Europe}'s {Historical} {Newspapers} {Searchable}},
	isbn = {9781509017928 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979556437&doi=10.1109%2fDAS.2016.83&partnerID=40&md5=8f07aa6473046bc026f95a822340502c},
	doi = {10.1109/DAS.2016.83},
	abstract = {This paper provides a rare glimpse into the overall approach for the refinement, i.e. the enrichment of scanned historical newspapers with text and layout recognition, in the Europeana Newspapers project. Within three years, the project processed more than 10 million pages of historical newspapers from 12 national and major libraries to produce the largest open access and fully searchable text collection of digital historical newspapers in Europe. In this, a wide variety of legal, logistical, technical and other challenges were encountered. After introducing the background issues in newspaper digitization in Europe, the paper discusses the technical aspects of refinement in greater detail. It explains what decisions were taken in the design of the large-scale processing workflow to address these challenges, what were the results produced and what were identified as best practices. © 2016 IEEE.},
	language = {English},
	urldate = {2016-04-11},
	booktitle = {Proc. - {IAPR} {Int}. {Workshop} {Doc}. {Anal}. {Syst}., {DAS}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Neudecker, C. and Antonacopoulos, A.},
	year = {2016},
	note = {Journal Abbreviation: Proc. - IAPR Int. Workshop Doc. Anal. Syst., DAS},
	keywords = {Best practices, Character recognition, digital libraries, Digital libraries, Digitisation, Electronic publishing, europeana, HEMDIG - PROJETOS SIMILARES, HEMDIG - SCOPUS, HEMDIG FRAMEWORK, historical newspapers, Historical newspapers, large-scale digitisation, Large-scale processing, layout analysis, Layout analysis, newspapers, Newsprint, OCR, OLR, Open Access, optical character recognition, Optical character recognition, SCOPUS, Technical aspects, Text collection},
	pages = {405--410},
	file = {neudecker_antonacopoulos_2016_making europe's historical newspapers searchable.pdf:/home/ebn/pCloudDrive/zot_library/IEEE/2016/neudecker_antonacopoulos_2016_making europe's historical newspapers searchable.pdf:application/pdf;Snapshot:/home/ebn/Zotero/storage/L8533WTD/7490152.html:text/html},
}

@inproceedings{klaus_can_2020,
	title = {Can umlauts ruin your research in digitized newspaper collections? {A} newseye case study on ‘{The} dark sides of war’ (1914–1918)},
	volume = {2612},
	isbn = {16130073 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086073635&partnerID=40&md5=e52a70aabcf2494dc7d25773a94a3164},
	abstract = {Digitized newspaper collections facilitate the access to historical newspapers. Even though they offer several useful possibilities regarding the research in historical newspapers and magazines, the (automatic) research in these collections is (still) full of limitations and pitfalls. Based on the research conducted on the platform AustriaN Newspapers Online (ANNO) for the NewsEye case study ‘the dark sides of war’, the main challenges of working with digitized newspaper collections will be discussed in this paper. Especially two aspects – the fire catastrophe at the munitions factory Wöllersdorf (1918/09/18) in Lower Austria and the Austrian press coverage about war widows during the First World War – will be used as specific examples. The discussed limitations include the Optical Character Recognition (OCR) quality, provided search options and metadata, as well as others. Furthermore, possible improvements regarding these challenges, e.g. Optical Layout Recognition (OLR), Named-entity Recognition (NER) and Named-entity Linking (NEL), will be presented in this paper. Copyright © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
	language = {English},
	urldate = {2020-10-21},
	booktitle = {{CEUR} {Workshop} {Proc}.},
	publisher = {CEUR-WS},
	author = {Klaus, B.},
	editor = {{Reinsone S.} and {Skadina I.} and {Baklane A.} and {Daugavietis J.}},
	year = {2020},
	note = {Journal Abbreviation: CEUR Workshop Proc.},
	keywords = {Austria, Digital Humanities, Digitized Newspaper Collections, First World War, HEMDIG - SCOPUS, Historical newspapers, Historical Research Interfaces, Military operations, Named entities, Named entity recognition, Newsprint, Optical character recognition, Optical character recognition (OCR), Optical layouts, SCOPUS},
	pages = {267--274},
}

@inproceedings{esposito_experimental_1990,
	address = {Piscataway, NJ, United States},
	title = {An experimental page layout recognition system for office document automatic classification: {An} integrated approach for inductive generalization},
	volume = {1},
	isbn = {0818620625 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025532061&partnerID=40&md5=0abd07a40956e17fc5baca27934fe02a},
	abstract = {A novel approach to automatic classification of digitized office documents, based on the inductive generalization of their layout style, is presented. It is supported by the observation that for a number of printed documents it is possible to find a set of relevant and invariant layout features. These are geometrical characteristics automatically detected through a segmentation and layout analysis process. The learning step, in which significant examples of document classes are used to train the classification system, involves the novel idea of integrating parametric (numerical) and conceptual (symbolic) learning methods.},
	language = {English},
	urldate = {1990-06-16},
	booktitle = {Proc {Int} {Conf} {Pattern} {Recognit}},
	publisher = {Publ by IEEE},
	author = {Esposito, Floriana and Malerba, Donato and Semeraro, Giovanni and Annese, Enrico and Scafuro, Giovanna},
	collaborator = {{Int Assoc for Pattern Recognition}},
	year = {1990},
	note = {Journal Abbreviation: Proc Int Conf Pattern Recognit},
	keywords = {HEMDIG - SCOPUS, Inductive Generalization, Learning Systems, Office Automation, Office Documents, Page Layouts, Pattern Recognition, Printed Documents, SCOPUS, Segmentation},
	pages = {557--562},
}

@inproceedings{watanabe_cooperative_1992,
	title = {A cooperative document understanding method among multiple recognition procedures},
	volume = {2},
	isbn = {10514651 (ISSN); 0818629150 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951784977&doi=10.1109%2fICPR.1992.201870&partnerID=40&md5=e319af6e2645e13f007445c4d876f255},
	doi = {10.1109/ICPR.1992.201870},
	abstract = {The main objective of document understanding is to extract and classify the meaningful data automatically from documents. Some researches, concerning this issue, have already been reported. However, these methods are not always successful because the recognition procedures analyze document images on the basis of only physical coordinate values of compositive items. In this paper, we propose a more advanced method based on the spatial relationships among neighboring segments of compositive items,in addition to the geometric aspects. In our method, the knowledge about documents is not a single layer,but organized as multilevel layers: knowledge about layout structures, knowledge about item sequences and knowledge about item properties. 3 kinds of knowledge are not only specified hierarchically, but also interrelated mutually between the layout recognition, item recognition and character recognition procedures. Additionally, this paper makes it clear through some experiments that our method for document understanding is very excellent in the adaptability, applicability and flexibility. © 1992 Institute of Electrical and Electronics Engineers Inc. All rights reserved.},
	language = {English},
	urldate = {1992-08-30},
	booktitle = {Proc. {Int}. {Conf}. {Pattern} {Recognit}.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Watanabe, T. and Qin, L.U.O. and Sugie, N.},
	collaborator = {{The International Association for Pattern Recognition (IAPR)}},
	year = {1992},
	note = {Journal Abbreviation: Proc. Int. Conf. Pattern Recognit.},
	keywords = {Character recognition, Compositive, Document images, Document understanding, HEMDIG - SCOPUS, Item recognition, Layout structure, Pattern recognition systems, SCOPUS, Single layer, Spatial relationships},
	pages = {689--692},
}

@article{watanabe_layout_1995,
	title = {Layout {Recognition} of {Multi}-{Kinds} of {Table}-{Form} {Documents}},
	volume = {17},
	issn = {01628828 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029293805&doi=10.1109%2f34.385976&partnerID=40&md5=257ade263f09b1c8421e776680bbb8bc},
	doi = {10.1109/34.385976},
	abstract = {Many approaches have reported that knowledge-based layout recognition methods are very successful to classify the meaningful data from document images automatically. However, these approaches are applicable to only the same kind of documents because they are based on the paradigm that specifies the structure definition information in advance so as to be able to analyze a particular class of documents intelligently. In this paper, we propose a method to recognize the layout structures of multi-kinds of table-form document images. For this purpose, we introduce a classification tree to manage the relationships among different classes of layout structures. Our recognition system has two modes: layout knowledge acquisition and layout structure recognition. In the layout knowledge acquisition mode, table-form document images are distinguished according to this classification tree and then the structure description trees which specify the logical structures of table-form documents are generated automatically. While, in the layout structure recognition mode, individual item fields in the table-form document images are extracted and classified successfully by searching the classification tree and interpreting the structure description tree. © 1995 IEEE},
	language = {English},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Watanabe, T. and Luo, Q. and Sugie, N.},
	year = {1995},
	keywords = {automatic acquisition of layout knowledge, Automation, classification tree, Classification tree, HEMDIG - SCOPUS, Image understanding, Knowledge acquisition, Knowledge based systems, Layout knowledge, Layout recognition, Layout structures, Pattern recognition, recognition of document classes, recognition of layout structures, Recognition paradigm for multi-kinds of table-form documents, SCOPUS, structure description tree, Structure description tree, Table form documents, Trees (mathematics)},
	pages = {432--445},
}

@article{eales_limit_1995,
	title = {Limit the scope of design rules to verify complex designs},
	volume = {12},
	issn = {07480016 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029276158&partnerID=40&md5=ef850cf2bd33132be05e889dc30518a3},
	abstract = {Design rules which are intended to provide the basis for any circuit-board layout can be both simple and complex. Most often, design-rule check DRC programs operate as external programs. However, given that most designers employ toolsets with limited online features, there techniques such as layout segmentation and running DRC in stages that can help them verify complex design requirements. Consequently, more-sophisticated tools allow checking for nets that might have over/undershoot problems and thus, don't settle within the desired time. However, when using these tools, it should be remembered that the edge rate of the driving device is the most important for settling-time calculations rather than clock speed.},
	language = {English},
	number = {3},
	journal = {Personal Engineering and Instrumentation News},
	author = {Eales, Ian},
	year = {1995},
	note = {Place: Pine Brook, NJ, United States
Publisher: PEC Inc},
	keywords = {Algorithms, Calculations, Computer aided design, Computer aided engineering, Crosstalk, Design rule check, Errors, HEMDIG - SCOPUS, Logic design, PC based layout tools, Personal computers, Printed circuit boards, Printed circuit design, SCOPUS, Specifications},
	pages = {63--65},
}

@inproceedings{ratha_fpga-based_1996,
	address = {Los Alamitos, CA, United States},
	title = {{FPGA}-based high performance page layout segmentation},
	isbn = {10661395 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029720295&partnerID=40&md5=645f1050465b1f83193aa0c7aa4732c1},
	abstract = {A page layout segmentation algorithm for locating text, background and halftone areas is presented. The algorithm has been implemented on Splash 2 - an FPGA-based array processor. The speed as determined by the Xilinx synthesis tools projects an application speed of 5 MHz. For documents of size 1.024 × 1.024 pixels, a significant speedup of two orders of magnitude compared to a SparcStation 20 has been achieved.},
	language = {English},
	urldate = {1996-03-22},
	booktitle = {Proc {IEEE} {Great} {Lakes} {Symp} {VLSI}},
	publisher = {IEEE},
	author = {Ratha, N.K. and Jain, A.K. and Rover, D.T.},
	collaborator = {{IEEE}},
	year = {1996},
	note = {Journal Abbreviation: Proc IEEE Great Lakes Symp VLSI},
	keywords = {Algorithms, Character recognition, Codes (symbols), Computer graphics, Design, HEMDIG - SCOPUS, Image segmentation, Page layout segmentation, Program processors, SCOPUS},
	pages = {29--34},
}

@article{watanabe_multilayer_1996,
	title = {A multilayer recognition method for understanding table-form documents},
	volume = {7},
	issn = {08999457 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030392123&doi=10.1002%2f%28SICI%291098-1098%28199624%297%3a4%3c279%3a%3aAID-IMA3%3e3.0.CO%3b2-5&partnerID=40&md5=8b89af04d86c1fd0653fb2258cc000cc},
	doi = {10.1002/(SICI)1098-1098(199624)7:4<279::AID-IMA3>3.0.CO;2-5},
	abstract = {The items in documents are composed under the mutual relationship between geometrical layout structure and logical structure. Thus, currently used documents may be categorized roughly on the basis of the geometrical layout structure and logical structure. Table-form documents are strictly defined by the geometrical layout structure in comparison with the other documents. Since the item fields are, in general, surrounded with vertical and horizontal line segments, with a view to understanding table-form documents it is better to recognize these line segments first and then identify individual item fields on the basis of the recognition results. At least, in table-form documents the vertical and horizontal line segments take important roles in recognizing the layout structures. In this article, we address a multilayer recognition method for understanding table-form documents. Our recognition layers are composed hierarchically of four different recognition processes: document class recognition, layout recognition, item recognition, and character recognition. In addition, our recognition method is organized on the basis of the interaction paradigm that the lower-layer recognition process verifies objects interpreted in the upper-layer recognition process with its own knowledge and then decomposes the objects into more elementary and meaningful objects. Moreover, we discuss a knowledge representation method from the viewpoints of physical and logical representations, and syntactic and semantic information. We also show the knowledge that is useful in understanding table-form documents with respect to such a representation method. © 1996 John Wiley \& Sons, inc.},
	language = {English},
	number = {4},
	journal = {International Journal of Imaging Systems and Technology},
	author = {Watanabe, T. and Luo, Q.},
	year = {1996},
	note = {Publisher: John Wiley and Sons Inc.},
	keywords = {Character recognition, Data processing, Document class recognition, HEMDIG - SCOPUS, Image understanding, Item recognition, Layout recognition, Multilayer recognition methods, Object recognition, Pattern recognition systems, SCOPUS, Table form documents},
	pages = {279--288},
}

@article{sherkat_descriptive_1996,
	title = {A descriptive retrieval engine for image databases},
	issn = {09633308 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-7044269336&partnerID=40&md5=efa0d73c7dc22997b7ec95f79a2fec78},
	abstract = {Electronic images are rapidly replacing conventional paper documents due to their handling advantages. As a result, there is a need for retrieval techniques where the level of man-machine interaction is closer to that enjoyed amongst the humans. This paper describes the work carried out to date in design and development of a descriptive retrieval engine. In such a system users may retrieve documents from an unorganised image database using 'high level' descriptions, such as: "..I'm looking for a tax form. It was mainly blue with a customs and excise logo on top right hand comer. I think it was a 95-96.." The automatic information extraction part of the system has been successfully tested on many widely varying documents segmenting them into pure text, pure graphic and mixed entities.},
	language = {English},
	number = {119},
	journal = {IEE Colloquium (Digest)},
	author = {Sherkat, N. and Mighlani, D. and Whitrow, R.J.},
	year = {1996},
	keywords = {Computer software, Database systems, Descriptive retrieval engine, HEMDIG - SCOPUS, High level languages, Human computer interaction, Image databases, Image processing, Image segmentation, Information retrieval, Layout segmentation, SCOPUS},
	pages = {11/1--11/6},
}

@article{suvichakorn_simple_2002,
	title = {Simple layout segmentation of gray-scale document images},
	volume = {2423},
	issn = {03029743 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947765865&doi=10.1007%2f3-540-45869-7_28&partnerID=40&md5=107477c448bd11e289b5b6d2db44b5df},
	doi = {10.1007/3-540-45869-7_28},
	abstract = {A simple yet effective layout segmentation of document images is proposed in this paper. First, n × n blocks are roughly labeled as background, line, text, images, graphics or mixed class. For blocks in mixed class, they are split into 4 sub-blocks and the process repeats until no mixed class is found. By exploiting Savitzky-Golay derivative filter in the classification, the computation of features is kept to the minimum. Next, the boundaries of each object are refined. The experimental results yields a satisfactory results as a pre-process prior to OCR. © 2002 Springer-Verlag Berlin Heidelberg.},
	language = {English},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Suvichakorn, A. and Watcharabusaracum, S. and Sinthupinyo, W.},
	year = {2002},
	keywords = {Artificial intelligence, Computers, Derivative filter, Document images, Gray scale, HEMDIG - SCOPUS, Image segmentation, Mixed class, Savitzky-Golay, SCOPUS, Sub-blocks},
	pages = {245--248},
}

@inproceedings{tian_layout_2002,
	address = {Beijing},
	title = {The layout recognition and reconstruction of {Chinese} documents based on gabor filter},
	volume = {4},
	isbn = {0780375084 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036929026&partnerID=40&md5=a6e02bfbe938e2dc41dca3235434b2e6},
	abstract = {The layout recognition of Chinese documents is one of the most difficult pattern recognition problems, because it concerns a great number of classes and many variations. In this paper, an intelligent layout recognition and reconstruction system is put forward on the basis of the analysis of the present ways. The algorithms of the pivotal processing steps such as layout analysis and character font recognition are discussed. Especially, a 2-D Gabor filter is used in character font recognition to improve the adaptability to the Chinese document. Experiments have been made and some promising results have been drawn.},
	language = {English},
	urldate = {2002-11-04},
	booktitle = {Proc. of 2002 {Internat}. {Conf}. on {Machine} {Learning} and {Cybernetics}},
	author = {Tian, X.-D. and Guo, B.-L.},
	collaborator = {Hebei University; IEEE systems, Man {and} Cybernetics technical Comm. on Cybernetics},
	year = {2002},
	note = {Journal Abbreviation: Proc. of 2002 Internat. Conf. on Machine Learning and Cybernetics},
	keywords = {Algorithms, Artificial intelligence, Character font recognition, Character recognition, Chinese documents, Digital filters, Digitalization of document, File organization, Font recognition, Gabor filter, HEMDIG - SCOPUS, Image compression, Image reconstruction, Layout analysis, Layout recognition, SCOPUS, Two dimensional},
	pages = {1707--1710},
}

@article{diwadkar_viewpoint_1997,
	title = {Viewpoint dependence in scene recognition},
	volume = {8},
	issn = {09567976 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347351067&doi=10.1111%2fj.1467-9280.1997.tb00442.x&partnerID=40&md5=f5c905d8dafacbe15c3f36c641c5214f},
	doi = {10.1111/j.1467-9280.1997.tb00442.x},
	abstract = {Two experiments investigated the viewpoint dependence of spatial memories. In Experiment 1, participants learned the locations of objects on a desktop from a single perspective and then took part in a recognition test; test scenes included familiar and novel views of the layout. Recognition latency was a linear function of the angular distance between a test view and the study view. In Experiment 2, participants studied a layout from a single view and then learned to recognize the layout from three additional training views. A final recognition test showed that the study view and the training views were represented in memory, and that latency was a linear function of the angular distance to the nearest study or training view. These results indicate that interobject spatial relations are encoded in a viewpoint-dependent manner, and that recognition of novel views requires normalization to the most similar representation in memory. These findings parallel recent results in visual object recognition.},
	language = {English},
	number = {4},
	journal = {Psychological Science},
	author = {Diwadkar, V.A. and McNamara, T.P.},
	year = {1997},
	note = {Publisher: Blackwell Publishing Ltd},
	keywords = {HEMDIG - SCOPUS, SCOPUS},
	pages = {302--307},
}

@inproceedings{mighlani_intelligent_1997,
	address = {Piscataway, NJ, United States},
	title = {Intelligent hierarchical layout segmentation of document images on the basis of colour content},
	volume = {1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031345878&partnerID=40&md5=7c771103f731fb57dda1fbbb72f158e9},
	abstract = {This paper proposes a general methodology for automatic layout segmentation of documents. We first use colour histograms for extracting dominant colours of an image. This information is then used to hierarchically segment documents into regions of interest represented as polygons. If a region of interest is a picture the algorithm intelligently refrains from segmenting it further, while coloured regions that contain text are sub-segmented. The method has been tested on 50 real life documents, such as office letters, brochures, and technical papers, scanned at 100×100 dpi resolution. Regions are detected with about 68\% reliability. A critical analysis of the results is presented.},
	language = {English},
	urldate = {1997-12-02},
	booktitle = {{IEEE} {Reg} 10 {Annu} {Int} {Conf} {Proc} {TENCON}},
	publisher = {IEEE},
	author = {Mighlani, D. and Hennig, A. and Sherkat, N. and Whitrow, R.J.},
	editor = {{Deriche M.} and {Moody M.} and {Bennamoun M.}},
	collaborator = {{IEEE}},
	year = {1997},
	note = {Journal Abbreviation: IEEE Reg 10 Annu Int Conf Proc TENCON},
	keywords = {Adaptive algorithms, Automatic layout segmentation, Color image processing, Feature extraction, HEMDIG - SCOPUS, Image segmentation, Intelligent hierarchical layout segmentation, SCOPUS},
	pages = {191--194},
}

@inproceedings{watanabe_guideline_1999,
	address = {Bellingham},
	title = {Guideline for specifying layout knowledge},
	volume = {3651},
	isbn = {0277786X (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032629301&doi=10.1117%2f12.335813&partnerID=40&md5=dc21383b9e23e0c596cf3f5573201fa4},
	doi = {10.1117/12.335813},
	abstract = {Until today, many layout recognition/analysis methods have been proposed, but the guideline for knowledge representation applicable to documents which should be analyzed newly is not always discussed directly. This paper addresses such a subject. Generally, the documents can be categorized into the appropriate document types on the basis of the features of layout structures. Then, the processing mechanisms are assessed with a view to establishing the criteria for selecting the knowledge representation means appropriate to the document types. First, we define the physical layout structure and logical layout structure in addition to the traditional concepts of layout structure and logical structure. Second, we define the document type on the basis of the relationship between the physical layout structure and logical layout structure. Third, we make the knowledge representation means and the processing mechanisms clear under the document types. Finally, we show a criterion or guideline for knowledge representation means and processing mechanisms with respect to the logical layout structure and physical layout structure. For this discussion, our basic view is derived from our document understanding methods which we have developed for several different documents.},
	language = {English},
	urldate = {1999-01-27},
	booktitle = {Proc {SPIE} {Int} {Soc} {Opt} {Eng}},
	publisher = {Society of Photo-Optical Instrumentation Engineers},
	author = {Watanabe, Toyohide},
	collaborator = {{IS and T; SPIE}},
	year = {1999},
	note = {Journal Abbreviation: Proc SPIE Int Soc Opt Eng},
	keywords = {Data structures, HEMDIG - SCOPUS, Image analysis, Knowledge representation, Layout knowledge representation, Layout structure, Mathematical models, Optical character recognition, SCOPUS},
	pages = {162--172},
}

@article{watanabe_document_1999,
	title = {Document analysis and recognition},
	volume = {E82-D},
	issn = {09168532 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033350174&partnerID=40&md5=0fe0408d0f13fb6fea99c186ea7c00c2},
	abstract = {SUMMARY The subject about document image understanding is to extract and classify individual data meaningfully from paper-based documents. Until today, many methods/approaches have been proposed with regard to recognition of various kinds of documents, various technical problems for extensions of OCR, and requirements for practical usages. Of course, though the technical research issues in the early stage are looked upon as complementary attacks for the traditional OCR which is dependent on character recognition techniques, the application ranges or related issues are widely investigated or should be established progressively. This paper addresses current topics about document image understanding from a technical point of view as a survey.},
	language = {English},
	number = {3},
	journal = {IEICE Transactions on Information and Systems},
	author = {Watanabe, T.},
	year = {1999},
	note = {Publisher: Institute of Electronics, Information and Communication, Engineers, IEICE},
	keywords = {Bottom-up, Character recognition techniques, Data structures, Database systems, Document image understanding, Document model, Document types, Feature extraction, HEMDIG - SCOPUS, Image analysis, Image understanding, Knowledge representation, Layout recognition, Layout structure, Logical structure, Optical character recognition, Paper based documents, SCOPUS, Top-down},
	pages = {601--610},
}

@inproceedings{minguillon_progressive_1999,
	address = {Bellingham, WA, United States},
	title = {Progressive classification scheme for document layout recognition},
	volume = {3816},
	isbn = {0277786X (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033359405&partnerID=40&md5=094c3898ca3859bac8af0866375ef8ed},
	abstract = {In this paper we present a progressive classification scheme for a document layout recognition system (background, text and images) using three stages. The first stage, preprocessing, extracts statistical information that may be used for background detection and removal. The second stage, a tree based classifier, uses a variable block size and a set of probabilistic rules to classify segmented blocks. When a block cannot be classified at a given block size because contains more than one class, it is split in four sub-blocks that are independently classified. The third stage, postprocessing, uses the label map generated in the second stage with a set of context rules to label unclassified blocks, trying also to solve some of the misclassification errors that may have been generated during the previous stage. The progressive scheme used in the second and third stages allows the user to stop the classification process at any block size, depending on his requirements. Experiments show that a progressive scheme combined with a set of postprocessing rules increases the percentage of correctly classified blocks and reduces the number of block computations.},
	language = {English},
	urldate = {1999-07-21},
	booktitle = {Proc {SPIE} {Int} {Soc} {Opt} {Eng}},
	publisher = {Society of Photo-Optical Instrumentation Engineers},
	author = {Minguillon, Julia and Pujol, Jaume and Zeger, Kenneth},
	collaborator = {{SPIE}},
	year = {1999},
	note = {Journal Abbreviation: Proc SPIE Int Soc Opt Eng},
	keywords = {Document analysis, Document layout recognition, HEMDIG - SCOPUS, Image coding, Image compression, Image quality, Information analysis, Information retrieval systems, Pattern recognition systems, Probability, Progressive classification tree, SCOPUS, Wavelet packets},
	pages = {241--251},
}

@article{maderlechner_extraction_2000,
	title = {Extraction of relevant information from document images using measures of visual attention},
	volume = {15},
	issn = {10514651 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750906044&partnerID=40&md5=9e5fb6e90b20bc2fb3d22ac3b9105d5a},
	abstract = {This paper describes an approach to attention based layout segmentation using general principles of the human visual perception to achieve this goal. The text is considered as texture in different resolution levels. A new measure of attractiveness is introduced. The segmentation is generic and not limited to specific document classes and models. The resulting regions of interest may be used for further interpretation. The overall speed of browsing and searching large volumes of scanned documents can be increased considerably. © 2000 IEEE.},
	language = {English},
	number = {4},
	journal = {Proceedings - International Conference on Pattern Recognition},
	author = {Maderlechner, G. and Schreyer, A. and Suda, P.},
	year = {2000},
	keywords = {Behavioral research, Copying, Different resolutions, Document images, HEMDIG - SCOPUS, Human visual perception, Large volumes, Pattern recognition systems, Regions of interest, SCOPUS, Visual Attention},
	pages = {385--388},
}

@inproceedings{cinque_system_2003,
	address = {Mantova},
	title = {A system for the automatic layout segmentation and classification of digital documents},
	isbn = {0769519482 (ISBN); 9780769519487 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649152439&doi=10.1109%2fICIAP.2003.1234050&partnerID=40&md5=d82804346f8d2452921f12efafdabc82},
	doi = {10.1109/ICIAP.2003.1234050},
	abstract = {Paper document recognition is fundamental for office automation becoming every day a more powerful tool in those fields where information is still on paper. Document recognition follows from data acquisition, from both journals and entire books, in order to transform them into digital objects. We present a new system for document recognition that follows the open source methodologies, XML description for document segmentation and classification, which turns out to be beneficial in terms of classification precision, and general-purpose availability. © 2003 IEEE.},
	language = {English},
	urldate = {2003-09-17},
	booktitle = {Proc. - {Int}. {Conf}. {Image} {Anal}. {Process}., {ICIAP}},
	author = {Cinque, L. and Levialdi, S. and Malizia, A.},
	year = {2003},
	note = {Journal Abbreviation: Proc. - Int. Conf. Image Anal. Process., ICIAP},
	keywords = {Automatic layout, Classification precision, Digital Documents, Digital Objects, Document analysis, Document Analysis, Document recognition, Document segmentation, HEMDIG - SCOPUS, Image analysis, Image processing and segmentation, Information retrieval systems, Office automation, Open systems, Paper documents, Pattern recognition, Pattern Recognition, SCOPUS},
	pages = {201--206},
}

@inproceedings{golebiowski_automated_2004,
	address = {Washington, DC},
	title = {Automated layout recognition},
	isbn = {1581139764 (ISBN); 9781581139761 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-20444479896&doi=10.1145%2f1031442.1031449&partnerID=40&md5=9459d5ba46422452b088390338ecb51c},
	doi = {10.1145/1031442.1031449},
	abstract = {To develop document image layout classifiers, each document image is represented by a set of labeled polygons corresponding to the pair-wise relationships between objects on the page. "Wanted" and "Unwanted" training sets are used to generate a polygon weight based on frequency of occurrence in both sets (term frequency). Unknown documents are scored by comparing polygons to those occurring in the wanted set. A score, weighted by the term frequency for the matching polygons, is computed. Experiments are performed against the NIST Structured Forms Database based on single and multiple layout collections using a variety of training samples. Copyright 2004 ACM.},
	language = {English},
	urldate = {2004-11-12},
	booktitle = {{HDP} {Proc}. {First} {ACM} {Hardcopy} {Doc}. {Process}. {Workshop}},
	publisher = {Association for Computing Machinery},
	author = {Golebiowski, L.},
	collaborator = {{ACM SIGIR}},
	year = {2004},
	note = {Journal Abbreviation: HDP Proc. First ACM Hardcopy Doc. Process. Workshop},
	keywords = {Algorithms, Database systems, Degradation, Document image clustering, Document image processing, Document Image Processing, Feature extraction, HEMDIG - SCOPUS, Image processing, Image retrieval, Image segmentation, Page layout recognition, Page Layout Recognition, Page segmentation, Pattern recognition, SCOPUS},
	pages = {41--45},
}

@inproceedings{takiguchi_fundamental_2005,
	address = {Seoul},
	title = {A fundamental study of output translation from layout recognition and semantic understanding system for mathematical formulae},
	volume = {2005},
	isbn = {15205363 (ISSN); 0769524206 (ISBN); 9780769524207 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947360285&doi=10.1109%2fICDAR.2005.10&partnerID=40&md5=b469479340d8a26a56f00a6bd8986866},
	doi = {10.1109/ICDAR.2005.10},
	abstract = {In this paper we propose an implementation method for an off-line layout recognition and semantic understanding system for mathematical formulae. This off-line system aims at higher order coding of mathematical formulae in scientific articles as an application in document analysis. The system has two intermediate output codes: a layout tree holding information of geometrical structure of the formula and character recognized code of the symbols, and a semantic tree holding information of semantics of symbols. From the structure tree and the semantic tree after layout recognition and semantic understanding, various useful output can be generated at the translating part. This paper mainly describes implementation techniques for LATEX source output for high quality typesetting and gnuplot script output for drawing a function as a method for visual representation. © 2005 IEEE.},
	language = {English},
	urldate = {2005-08-31},
	booktitle = {Proc. {Int}. {Conf}. {Doc}. {Anal}. {Recognit}.},
	author = {Takiguchi, Y. and Okada, M. and Miyake, Y.},
	collaborator = {{TC10 (Graph. Recog.) and TC11 (Read. Syst.) of the (IAPR)}},
	year = {2005},
	note = {Journal Abbreviation: Proc. Int. Conf. Doc. Anal. Recognit.},
	keywords = {Computational geometry, Encoding (symbols), Geometrical structure, HEMDIG - SCOPUS, Layout recognition, Mathematical formulae, Pattern recognition, SCOPUS, Semantics, Translation (languages), Visual representation},
	pages = {745--749},
}

@inproceedings{esposito_intelligent_2005,
	address = {Seoul},
	title = {Intelligent document processing},
	volume = {2005},
	isbn = {15205363 (ISSN); 0769524206 (ISBN); 9780769524207 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947413581&doi=10.1109%2fICDAR.2005.144&partnerID=40&md5=2593cd8304f09ed0c5cc62c147785ca2},
	doi = {10.1109/ICDAR.2005.144},
	abstract = {Digital repositories raise the need for an effective and efficient retrieval of the stored material. In this paper we propose the intensive application of intelligent techniques to the steps of document layout analysis, document image classification and understanding on digital documents. Specifically, the complex interrelation existing among layout components, that are fundamental to assign them the proper semantic role, suggest the exploitation of first-order representations in some learning steps. Results obtained in a prototypical system for scientific conference management prove that the proposed approach can be beneficial both for the layout recognition and for the selection of interesting components of the document, from which extracting the text for categorizing the document according to its topic. © 2005 IEEE.},
	language = {English},
	urldate = {2005-08-31},
	booktitle = {Proc. {Int}. {Conf}. {Doc}. {Anal}. {Recognit}.},
	author = {Esposito, F. and Ferilli, S. and Basile, T.M.A. and Di Mauro, N.},
	collaborator = {{TC10 (Graph. Recog.) and TC11 (Read. Syst.) of the (IAPR)}},
	year = {2005},
	note = {Journal Abbreviation: Proc. Int. Conf. Doc. Anal. Recognit.},
	keywords = {Classification (of information), Data processing, Digital documents, HEMDIG - SCOPUS, Information management, Information retrieval, Intelligent agents, Intelligent document processing, Layout recognition, SCOPUS, Semantics},
	pages = {1100--1104},
}

@inproceedings{gaceb_application_2008,
	title = {Application of graph coloring in physical layout segmentation},
	isbn = {10514651 (ISSN); 9781424421756 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957964928&doi=10.1109%2ficpr.2008.4761641&partnerID=40&md5=f0a1f0ef9c1d5ee90ffb19cf8787f87a},
	doi = {10.1109/icpr.2008.4761641},
	abstract = {Every-day, the postal sorting systems diffuse several tons of mails. It is noted that the principal origin of mail rejection is related to the failure of address-block localization task, particularly, of the physical layout segmentation stage. The bottom-up and top-down segmentation methods bring different knowledge that should not be ignored when we need to increase the robustness. Hybrid methods combine the two strategies in order to take advantages of one strategy to the detriment of other. Starting from these remarks, our proposal makes use of a hybrid segmentation strategy more adapted to the postal mails. The high level stages are based on the hierarchical graphs coloring. Today, no other work in this context has make use of the powerfulness of this tool. The performance evaluation of our approach was tested on a corpus of 10000 envelope images. The processing times and the rejection rate were considerably reduced. © 2008 IEEE.},
	language = {English},
	booktitle = {Proc. {Int}. {Conf}. {Pattern} {Recognit}.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Gaceb, D. and Eglin, V. and Lebourgeois, F. and Emptoz, H.},
	year = {2008},
	note = {Journal Abbreviation: Proc. Int. Conf. Pattern Recognit.},
	keywords = {Bottom-up and top-down, Graph colorings, HEMDIG - SCOPUS, Hierarchical graphs, Hybrid segmentation, Pattern recognition, Physical layout, Processing time, Rejection rates, SCOPUS, Segmentation methods, Software engineering},
}

@article{cheng_layout_2008,
	title = {Layout analysis based on multi-feature and {SVM}},
	volume = {23},
	issn = {10049037 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-56549088885&partnerID=40&md5=c0b0500fc7280b0fdb694555cfb46ee9},
	abstract = {The gray feature, the shape feature, and the texture feature are analyzed. The differences among the text, the table, the figure, and the image on document images are studied. Aimed at the Chinese/English layout, the projection profile and the connected component analyses are effectively combined to segment images. Then, the feature vector of seventeen dimensions is abstracted for representing block information, and SVM is introduced to classify document images as four types (text, table, figure, and image). Experimental results show that the method can segment document images for various kinds of the layout segmentation.},
	language = {Chinese},
	number = {5},
	journal = {Shuju Caiji Yu Chuli/Journal of Data Acquisition and Processing},
	author = {Cheng, J. and Ping, X. and Zhou, G.},
	year = {2008},
	keywords = {Gray feature, HEMDIG - SCOPUS, Page segmentation, SCOPUS, Shape feature, Support vector machine, Texture feature},
	pages = {569--574},
}

@inproceedings{lubbes_k_hdp_2004,
	address = {Washington, DC},
	title = {{HDP} 2004: {Proceedings} of the {First} {ACM} {Hardcopy} {Document} {Processing} {Workshop}},
	isbn = {1581139764 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-20444482147&partnerID=40&md5=cc8c322972e0f026b5f567bbea5a834b},
	abstract = {The proceedings contain 10 papers from the HDP 2004: Proceedings of the First ACM Hardcopy Document Processing Workshop. The topics discussed include: information access in the presence of OCR errors; robust document image understanding technologies; low resolution character recognition by dual eigenspace and synthetic degraded patterns; a filter based post-OCR accuracy boost system; a text image enhancement system based on segmentation and classification methods; automated layout recognition; dynamic local connectivity and its application to page segmentation; pictographic matching: a graph-based approach towards a language independent document exploitation platform; and quantifying information leakage in document redaction.},
	language = {English},
	urldate = {2004-11-12},
	booktitle = {{HDP} {Proc}. {First} {ACM} {Hardcopy} {Doc}. {Process}. {Workshop}},
	editor = {{Lubbes K.} and {Ronthaler M.}},
	collaborator = {{ACM SIGIR}},
	year = {2004},
	note = {Journal Abbreviation: HDP Proc. First ACM Hardcopy Doc. Process. Workshop},
	keywords = {Algorithms, Character shaping code (CSC), Database systems, Document analysis, Document image analysis, Document image clustering, Document redaction, EiRev, Errors, HEMDIG - SCOPUS, Hidden Markov Model (HMM), Image analysis, Image coding, Image enhancement, Image processing, Image quality, Image retrieval, Image segmentation, Imaging techniques, Information retrieval, Markov processes, Mathematical models, Mathematical transformations, OCR engines, Optical character recognition, Optical word recognition (OWR), Page segmentation, Pattern recognition, SCOPUS},
}

@article{kyriacou_vision-based_2005,
	title = {Vision-based urban navigation procedures for verbally instructed robots},
	volume = {51},
	issn = {09218890 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-22544440593&doi=10.1016%2fj.robot.2004.08.011&partnerID=40&md5=48971da18919c7c3deb996cf3ad03fbc},
	doi = {10.1016/j.robot.2004.08.011},
	abstract = {When humans explain a task to be executed by a robot they decompose it into chunks of actions. These form a chain of search-and-act sensory-motor loops that exit when a condition is met. In this paper we investigate the nature of these chunks in an urban visual navigation context, and propose a method for implementing the corresponding robot primitives such as "take the nth turn right/left". These primitives make use of a "short-lived" internal map updated as the robot moves along. The recognition and localisation of intersections is done in the map using task-guided template matching. This approach takes advantage of the content of human instructions to save computation time and improve robustness. © 2004 Elsevier B.V. All rights reserved.},
	language = {English},
	number = {1},
	journal = {Robotics and Autonomous Systems},
	author = {Kyriacou, T. and Bugmann, G. and Lauria, S.},
	year = {2005},
	keywords = {Computer vision, HEMDIG - SCOPUS, Mobile robots, Natural language processing systems, Navigation, Pattern recognition, Road layout recognition, Robot learning, Robot primitives, Robotics, Route instructions, SCOPUS, Template matching, Urban navigation, Urban planning},
	pages = {69--80},
}

@inproceedings{zhang_mask_2005,
	address = {San Jose, CA},
	title = {Mask cost analysis via write time estimation},
	volume = {5756},
	isbn = {0277786X (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-25144436352&doi=10.1117%2f12.598884&partnerID=40&md5=aa6606ca7f7be6a0869bd43f38cd27cc},
	doi = {10.1117/12.598884},
	abstract = {Long write times have been an industry wide concern regarding rising mask costs. The purpose of this study is to develop a simple model that can predict mask write time precisely, without an e-beam writer. With a good understanding of the trade-offs between design complexity and write time, mask makers can work with mask designers more closely to simplify design and minimize mask cost. This work compared several basic models including calculations based on write area with a fixed e-beam shot size, a software estimation with a pre-set exposure, and a mask stage settling time. Our proposed model uses a completely different approach to examine the correlation between layout complexity (vertices count, total line edge, figure, etc.) through a CATS layout segmentation and actual write time. It is found that write time is a strong function of layout figure, vertex count and total line edge. Errors between actual write time and estimated write time from the new model reduced from 7\% on average on the current production software to 3\%. Additionally, the new model can operate independent of the writer type and without fractured data being transferred onto a writer. Also provided are a few case studies to evaluate the interaction between write time and basic shape/OPC (optical proximity correction). Using a simple design shape and a better data snapping strategy can reduce write time up to 10 fold for applications in nano-imprint template manufacturing. Several strategies to reduce mask cost are proposed.},
	language = {English},
	urldate = {2005-03-03},
	booktitle = {Proc {SPIE} {Int} {Soc} {Opt} {Eng}},
	author = {Zhang, Y. and Gray, R. and Chou, S. and Rockwell, B. and Xiao, G. and Kamberian, H. and Cottle, R. and Wolleben, A. and Progler, C.},
	editor = {{Liebmann L.W.}},
	collaborator = {{SPIE}},
	year = {2005},
	note = {Journal Abbreviation: Proc SPIE Int Soc Opt Eng},
	keywords = {CATS, Cost benefit analysis, Data acquisition, Design optimization, HEMDIG - SCOPUS, Image segmentation, Mask write time, OPC, Optical correlation, Optical design, Optical engineering, Optical systems, SCOPUS, Vertex},
	pages = {313--318},
}

@article{mukherjee_statistical_2005,
	title = {Statistical analysis and diagnosis methodology for {RF} circuits in {LCP} substrates},
	volume = {53},
	issn = {00189480 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-28144451013&doi=10.1109%2fTMTT.2005.855735&partnerID=40&md5=5c3447f342ee566ce6fbf235698cee71},
	doi = {10.1109/TMTT.2005.855735},
	abstract = {This paper presents the application of a fast and accurate layout-level statistical analysis methodology for the diagnosis of RF circuit layouts with embedded passives in liquid crystalline polymer substrates. The approach is based on layout-segmentation, lumped-element modeling, sensitivity analysis, and extraction of probability density function using convolution methods. The statistical analyses were utilized as a diagnosis tool to estimate distributed design parameter variations and yield of RF circuit layouts for a given measured performance. The results of statistical analysis and diagnosis were compared with measurement results of fabricated filters. Statistical methods were also applied for design space exploration to improve system performance, as well as estimation of yield and diagnosis of faults during batch fabrication. © 2005 IEEE.},
	language = {English},
	number = {11},
	journal = {IEEE Transactions on Microwave Theory and Techniques},
	author = {Mukherjee, S. and Swaminathan, M. and Matoglu, E.},
	year = {2005},
	keywords = {Bandpass filter, Bandpass filters, Frequency converter circuits, HEMDIG - SCOPUS, Integrated circuit layout, Liquid crystal polymers, Liquid crystalline polymer (LCP), Lumped-element modeling, Mathematical models, Parametric yield, Passive networks, RF synthesis, SCOPUS, Statistical diagnosis, Statistical methods, Substrates},
	pages = {3621--3630},
}

@inproceedings{erol_smart_2005,
	address = {Amsterdam},
	title = {Smart handouts: {Personalized} e-presentation documents},
	volume = {2005},
	isbn = {0780393325 (ISBN); 9780780393325 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750549032&doi=10.1109%2fICME.2005.1521451&partnerID=40&md5=b7e1dd287b1d1186e944f524e731b581},
	doi = {10.1109/ICME.2005.1521451},
	abstract = {A novel system is described that significantly enhances the usefulness of handwritten notes taken during a presentation by creating a multimedia document that includes scanned images of handouts, personal notes, and links to a multimedia recording of the presentation. Notes are linked to the e-presentation media with automatic content analysis without any special notes capture device. Layout segmentation and template matching automatically detects the presence of presentation handouts during scanning. Presentation-level and slide-level linking of handouts to e-media use text and image features from slides. Experimental results show 95\% accuracy in linking of the scanned handouts to the e-presentation media. © 2005 IEEE.},
	language = {English},
	urldate = {2005-07-06},
	booktitle = {{IEEE} {Int}. {Conf}. {Multimedia} {Expo}},
	author = {Erol, B. and Lee, D.-S. and Hull, J.J.},
	year = {2005},
	note = {Journal Abbreviation: IEEE Int. Conf. Multimedia Expo},
	keywords = {Automatic content analysis, Electronic document exchange, Feature extraction, Handouts, HEMDIG - SCOPUS, Image recording, Multimedia documents, Multimedia recording, Multimedia systems, Pattern matching, SCOPUS, Telecommunication links, Text processing},
	pages = {426--429},
}

@inproceedings{takiguchi_study_2006,
	address = {Hong Kong},
	title = {A study on character recognition error correction at higher level recognition step for mathematical formulae understanding},
	volume = {2},
	isbn = {10514651 (ISSN); 0769525210 (ISBN); 9780769525211 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047223470&doi=10.1109%2fICPR.2006.180&partnerID=40&md5=df4aafba3e70262bdd526b57f01ccf13},
	doi = {10.1109/ICPR.2006.180},
	abstract = {In this paper we propose a method for correcting character recognition errors at the higher level recognition step in an understanding system for mathematical formulae. The system consists of two-level recognition steps: the low level recognition including character recognition, and the higher level recognition including layout recognition. We use the layout information recognized in the latter step to correct the character recognition errors by using two sources of information. One is based on some keywords such as mathematical function names, and the other is based on a cost tree and co-occurrence probabilities between symbols. The efficacy of the proposed method is verified by some experimental results, and the character recognition rate increased from 80.2\% to 89.2\%. © 2006 IEEE.},
	language = {English},
	urldate = {2006-08-20},
	booktitle = {Proc. {Int}. {Conf}. {Pattern} {Recognit}.},
	author = {Takiguchi, Y. and Okada, M. and Miyake, Y.},
	year = {2006},
	note = {Journal Abbreviation: Proc. Int. Conf. Pattern Recognit.},
	keywords = {Character recognition, Character recognition errors, Computational efficiency, Error correction, Functions, HEMDIG - SCOPUS, Information analysis, Keywords, Mathematical formulae, Probability, SCOPUS, Trees (mathematics)},
	pages = {966--969},
}

@inproceedings{gaceb_physical_2008,
	address = {Nara},
	title = {Physical layout segmentation of mail application dedicated to automatic postal sorting system},
	isbn = {9780769533377 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649210479&doi=10.1109%2fDAS.2008.56&partnerID=40&md5=18ee74d82acb5e9d7ca1b5cf2d05698d},
	doi = {10.1109/DAS.2008.56},
	abstract = {Every-day, the postal sorting systems diffuse several tons of mails. It is noted that the principal origin of mail rejection is related to the failure of address-block localization task, particularly, of the physical layout segmentation stage. The bottom-up and top-down segmentation methods bring different knowledge that should not be ignored when we need to increase the robustness. Hybrid methods combine the two strategies in order to take advantages of one strategy to the detriment of other. Starting from these remarks, our proposal makes use of a hybrid segmentation strategy more adapted to the postal malls. The high level stages are based on the hierarchical graphs coloring, allowing managing through a pyramidal data organization, the complex rules leading the interpretation of the connected components decomposition of interest zones. Today, no other work in this context has make use of the powerfulness of this tool. The performance evaluation of our approach was tested on a corpus of 10000 envelope Images. The processing times and the rejection rate were considerably reduced. © 2008 IEEE.},
	language = {English},
	urldate = {2008-09-16},
	booktitle = {{DAS} 2008 - {Proc}. {IAPR} {Int}. {Workshop} {Document} {Anal}. {Syst}.},
	author = {Gaceb, D. and Eglin, V. and Lebourgeois, F. and Emptoz, H.},
	collaborator = {Osaka Prefecture University; International Association for Pattern Recognition, Ltd.; Hitachi Computer Peripherals Co., Ltd., IAPR; Japan Society for the Promotion of Science; International Information Science Foundation; Hitachi},
	year = {2008},
	note = {Journal Abbreviation: DAS 2008 - Proc. IAPR Int. Workshop Document Anal. Syst.},
	keywords = {Bottom-up, Connected components, Data organizations, Electronic warfare, HEMDIG - SCOPUS, Hierarchical graphs, Hybrid methods, Hybrid segmentations, Intelligent vehicle highway systems, Military electronic countermeasures, Performance evaluations, Processing times, Rejection rates, SCOPUS, Segmentation methods, Sorting, Sorting systems, Technical presentations, Top downs},
	pages = {408--414},
}

@article{leydier_towards_2009,
	title = {Towards an omnilingual word retrieval system for ancient manuscripts},
	volume = {42},
	issn = {00313203 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-67349245536&doi=10.1016%2fj.patcog.2009.01.026&partnerID=40&md5=3c60d67964c4e975c186bec41cf8500e},
	doi = {10.1016/j.patcog.2009.01.026},
	abstract = {In this article, we introduce the first method that allows the indexation of ancient manuscripts of any language and alphabet. We describe a word retrieval engine inspired by recent word-spotting advances on ancient manuscripts. Our approach does not need any layout segmentation and makes use of features fitted to any type of alphabet (Latin, Arabic, Chinese, etc.) and writing. The engine is tested on numerous documents and in several use-cases. © 2009 Elsevier Ltd. All rights reserved.},
	language = {English},
	number = {9},
	journal = {Pattern Recognition},
	author = {Leydier, Y. and Ouji, A. and LeBourgeois, F. and Emptoz, H.},
	year = {2009},
	note = {Publisher: Elsevier Ltd},
	keywords = {Ancient documents, Data mining, Document indexing, Engines, HEMDIG - SCOPUS, Information retrieval, Omnilingual, SCOPUS, Segmentation-free, Word retrieval, Word Spotting, Word-spotting},
	pages = {2089--2105},
}

@inproceedings{misra_system_2009,
	address = {Arlington, VA},
	title = {A system for automated extraction of metadata from scanned documents using layout recognition and string pattern search models},
	volume = {1509 STP},
	isbn = {9780892082841 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952373668&partnerID=40&md5=a8788b74b11a46045ecc73126c07556a},
	abstract = {One of the most expensive aspects of archiving digital documents is the manual acquisition of context-semitive metadata useful for the subsequent discovery of, and access to, the archived items. For certain types of textual documents, such as journal arlicles, pamphlets, official government records, eie, where the metadata is contained within the body of the documents, a cost effective method is to identify and extract the metadata in an automated way, applying machine learning and string pattern search techniques. Al the U. S. National Library of Medicine (NLM) we have developed an automated metadata extraction (AME) system that employs layout classification and recognition models with a metadata pattern search model for a text corpus with structured or semi-structured Information. A combination of Support Vector Machine and Hidden Markov Model is used to create the layout recognition models from a training set of the corpus, following which a rule-based metadata search model is used to extract the embedded metadata by analyzing the string patterns within and surrounding each field in the recognized layouts. In this paper, we describe the design of our AME system, with focus on the metadata search model. We present the extraction results for a historic collection from the Food and Drug Administration, and outline how the system may be adapted for similar collections. Finally, we discuss seme ongoing enhancements to our AME system.},
	language = {English},
	urldate = {2009-05-04},
	booktitle = {Arch. - {Preserv}. {Strateg}. {Imaging} {Technol}. {Cult}. {Herit}. {Inst}. {Mem}. {Organ}. - {Final} {Program} {Proc}.},
	author = {Misra, D. and Chen, S. and Thoma, G.R.},
	collaborator = {{Society for Imaging Science and Technology}},
	year = {2009},
	note = {Journal Abbreviation: Arch. - Preserv. Strateg. Imaging Technol. Cult. Herit. Inst. Mem. Organ. - Final Program Proc.},
	keywords = {Automated extraction, Automation, Character recognition, Classification and recognition, Copying, Cost-effective methods, Digital Documents, Food and Drug Administration, Government records, HEMDIG - SCOPUS, Hidden Markov models, Historic preservation, Imaging techniques, Machine-learning, Metadata, Metadata extraction, National library of medicines, Pattern recognition systems, Pattern search, Recognition models, Rule based, SCOPUS, Search models, Semi-structured information, Societies and institutions, Text corpora, Text processing, Textual documents, Training sets},
	pages = {107--112},
}

@inproceedings{ouji_advertisement_2011,
	address = {Barcelona},
	title = {Advertisement detection in digitized press images},
	isbn = {19457871 (ISSN); 9781612843490 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155136740&doi=10.1109%2fICME.2011.6011890&partnerID=40&md5=7c6ceb0cc50fa7a46e01aaf31af00725},
	doi = {10.1109/ICME.2011.6011890},
	abstract = {This paper presents the first method for detecting advertisements in digitized press. The system aims at locating and recognizing ads. A color segmentation approach which is robust against digitization noise is introduced. The color separation output is used to carry out layout segmentation in document pages and to compute visual features. Block classification results, given with a variety of magazine and newspaper pages, are presented and discussed. © 2011 IEEE.},
	language = {English},
	urldate = {2011-07-11},
	booktitle = {Proc. - {IEEE} {Int}. {Conf}. {Multimedia} {Expo}},
	publisher = {IEEE Computer Society},
	author = {Ouji, A. and Leydier, Y. and Lebourgeois, F.},
	collaborator = {{IEEE Circuit and System Society; IEEE Communication Society; IEEE Computer Society; IEEE Signal Processing Society}},
	year = {2011},
	note = {Journal Abbreviation: Proc. - IEEE Int. Conf. Multimedia Expo},
	keywords = {Advertisement detection, Advertisement detections, Block classification, Color segmentation, Color separation, document image, Document images, HEMDIG - SCOPUS, Image segmentation, Presses (machine tools), SCOPUS, segmentation, Visual feature},
}

@inproceedings{clausner_scenario_2011,
	address = {Beijing},
	title = {Scenario driven in-depth performance evaluation of document layout analysis methods},
	isbn = {15205363 (ISSN); 9780769545202 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-82355192070&doi=10.1109%2fICDAR.2011.282&partnerID=40&md5=cd44cf6fe531857cd9ca6fcd90102778},
	doi = {10.1109/ICDAR.2011.282},
	abstract = {This paper presents an advanced framework for evaluating the performance of layout analysis methods. It combines efficiency and accuracy by using a special interval based geometric representation of regions. A wide range of sophisticated evaluation measures provides the means for a deep insight into the analysed systems, which goes far beyond simple benchmarking. The support of user-defined profiles allows the tuning for practically any kind of evaluation scenario related to real world applications. The framework has been successfully delivered as part of a major EU-funded project (IMPACT) to evaluate large-scale digitisation projects and has been validated using the dataset from the ICDAR2009 Page Segmentation Competition. © 2011 IEEE.},
	language = {English},
	urldate = {2011-09-18},
	booktitle = {Proc. {Int}. {Conf}. {Doc}. {Anal}. {Recognit}.},
	author = {Clausner, C. and Pletschacher, S. and Antonacopoulos, A.},
	collaborator = {{TC10 (Graph. Recogn.) TC11 (Read. Syst.) (IAPR); Chinese Academy of Sciences; NSFC; FUJITSU; Hanvon Technology}},
	year = {2011},
	note = {Journal Abbreviation: Proc. Int. Conf. Doc. Anal. Recognit.},
	keywords = {Analog to digital conversion, Data processing, Data sets, dataset, digitization, document layout analysis, Document layout analysis, evaluation scenario, ground truth, Ground truth, HEMDIG - SCOPUS, historical document, Historical documents, layout segmentation, measures, performance evaluation, Performance evaluation, SCOPUS},
	pages = {1404--1408},
}

@book{gordo_diagonal_2009,
	address = {Povoa de Varzim},
	series = {4th {Iberian} {Conference} on {Pattern} {Recognition} and {Image} {Analysis}, {IbPRIA} 2009},
	title = {The diagonal split: {A} pre-segmentation step for page layout analysis and classification},
	volume = {5524 LNCS},
	isbn = {03029743 (ISSN); 3642021719 (ISBN); 9783642021718 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-68749093090&doi=10.1007%2f978-3-642-02172-5_38&partnerID=40&md5=2646f26dbee726d1112ce634b211177b},
	abstract = {Document classification is an important task in all the processes related to document storage and retrieval. In the case of complex documents, structural features are needed to achieve a correct classification. Unfortunately, physical layout analysis is error prone. In this paper we present a pre-segmentation step based on a divide \& conquer strategy that can be used to improve the page segmentation results, independently of the segmentation algorithm used. This pre-segmentation step is evaluated in classification and retrieval using the selective CRLA algorithm for layout segmentation together with a clustering based on the voronoi area diagram, and tested on two different databases, MARG and Girona Archives. © 2009 Springer Berlin Heidelberg.},
	language = {English},
	urldate = {2009-06-10},
	author = {Gordo, A. and Valveny, E.},
	translator = {{International Association for Pattern Recognition (IAPR); Fundacao para a Ciencia e a Tecnologia (FCT)}},
	year = {2009},
	doi = {10.1007/978-3-642-02172-5_38},
	note = {Journal Abbreviation: Lect. Notes Comput. Sci.
Pages: 297
Publication Title: Lect. Notes Comput. Sci.},
	keywords = {Complex documents, Document Classification, Document storage and retrieval, Error prones, HEMDIG - SCOPUS, Image analysis, Image segmentation, Information retrieval systems, Page layout analysis, Page segmentation, Pattern recognition, Physical layout, Pre-segmentation, SCOPUS, Segmentation algorithms, Structural feature, Voronoi},
}

@inproceedings{zhu_layout_2011,
	address = {Hangzhou},
	title = {Layout recognition of multi-page document based on {Naive} {Bayes}},
	isbn = {9781612847740 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052932041&doi=10.1109%2fICMT.2011.6002154&partnerID=40&md5=03435247b5d45ba8767e0bf1c4bee95e},
	doi = {10.1109/ICMT.2011.6002154},
	abstract = {A layout recognition method for multi-page document image is proposed in this paper.Because there exists of spacings in vertical and horizontal direction in this kind of document, vertical and horizontal projection are used to extract the layout feature and Naive Bayes classifier is generated to realize the layout recognition of multi-page document.Experimental results show that the method of this paper has higher accuracy and efficiency,and has practicality and value to some extent. © 2011 IEEE.},
	language = {English},
	urldate = {2011-07-26},
	booktitle = {Int. {Conf}. {Multimedia} {Technol}., {ICMT}},
	author = {Zhu, L. and Chen, Y. and Zhu, M.},
	collaborator = {{University of Louisville; Ningbo University; Zhejiang Sci-Tech University; Communication University of China; Georgia State University}},
	year = {2011},
	note = {Journal Abbreviation: Int. Conf. Multimedia Technol., ICMT},
	keywords = {Classifiers, Feature extraction, HEMDIG - SCOPUS, Image preprocessing, Multi-page documents, Naive Bayes, Naive Bayes classification, Naive Bayes classifiers, Recognition methods, SCOPUS},
	pages = {568--571},
}

@inproceedings{malleron_mixed_2011,
	address = {Beijing},
	title = {A mixed approach for handwritten documents structural analysis},
	isbn = {15205363 (ISSN); 9780769545202 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-82355182603&doi=10.1109%2fICDAR.2011.62&partnerID=40&md5=09ef69db2ab991e1b86612f18dceb83d},
	doi = {10.1109/ICDAR.2011.62},
	abstract = {In this paper we propose a new method for document pages segmentation. First dedicated to handwritten documents, our method is designed to extract the different text zones, paragraph and fragment in unconstrained documents. The proposed approach is a mixed one, using both the advantages of top-down and bottom-up approaches. In this paper we proposed and evaluation of our methods on a 183 documents database, taken from a 19th century handwritten corpus : the «dossiers de Bouvard et Pécuchet» from Flaubert. With this evaluation we demonstrate that the combination of the top-down and the bottom-up approach allow to improve the obtained results. © 2011 IEEE.},
	language = {English},
	urldate = {2011-09-18},
	booktitle = {Proc. {Int}. {Conf}. {Doc}. {Anal}. {Recognit}.},
	author = {Malleron, V. and Eglin, V.},
	collaborator = {{TC10 (Graph. Recogn.) TC11 (Read. Syst.) (IAPR); Chinese Academy of Sciences; NSFC; FUJITSU; Hanvon Technology}},
	year = {2011},
	note = {Journal Abbreviation: Proc. Int. Conf. Doc. Anal. Recognit.},
	keywords = {19th century, Bottom up approach, Character recognition, handwritten, Handwritten document, HEMDIG - SCOPUS, layout segmentation, logical structure, Logical structure, Mixed approach, physical structure, Physical structures, SCOPUS, Topdown},
	pages = {269--273},
}

@inproceedings{zhang_algorithm_2012,
	address = {Hangzhou, Zhejiang},
	title = {An algorithm for scanned document image segmentation based on voronoi diagram},
	volume = {1},
	isbn = {9780769546476 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861039154&doi=10.1109%2fICCSEE.2012.144&partnerID=40&md5=a8a5d06139422e90cd94f35d69591070},
	doi = {10.1109/ICCSEE.2012.144},
	abstract = {With the emergence of complex layout, the layout isno longer confined to rectangular. This makes the traditional layout segmentation algorithm no longer applies, and new method dealing with complex layout emerge. This article proposed an algorithm for the scanned document image segmentation based on Voronoi diagram. The algorithm does not need additional tilt detection to oblique layout and pretreatment process for tilt correction, Voronoi diagram generated directly on the outer edge of the connected element, not related to the pixel processing, The algorithm does not need the processing to delete redundancy Voronoi edges and merger Voronoi edges, greatly reducing excessive segmentation of the other algorithm. The algorithm can fully identify each area of the document image, and the segmentation is accurate with very little information lost. In addition, because the structural elements were performed using statistical features, the adaptability of the algorithm is better. © 2012 IEEE.},
	language = {English},
	urldate = {2012-03-23},
	booktitle = {Proc. - {Int}. {Conf}. {Comput}. {Sci}. {Electron}. {Eng}., {ICCSEE}},
	author = {Zhang, J. and Dong, W. and Zhang, Y.},
	collaborator = {{Xi'an Technological University; Shaanxi New Network and Monitoring Control Engineering Laboratory}},
	year = {2012},
	note = {Journal Abbreviation: Proc. - Int. Conf. Comput. Sci. Electron. Eng., ICCSEE},
	keywords = {Algorithms, Computational geometry, Computer science, Digital image storage, Document images, Electronics engineering, Graphic methods, HEMDIG - SCOPUS, Image segmentation, Information lost, Pixel processing, Pretreatment process, scanned document image, Scanned document images, SCOPUS, segmentation algorithm, Segmentation algorithms, Statistical features, Structural elements, Tilt correction, Tilt detection, Voronoi, Voronoi diagram, Voronoi diagrams},
	pages = {156--159},
}

@inproceedings{mehri_old_2013,
	address = {Burlingame, CA},
	title = {Old document image segmentation using the autocorrelation function and multiresolution analysis},
	volume = {8658},
	isbn = {0277786X (ISSN); 9780819494313 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875824773&doi=10.1117%2f12.2002365&partnerID=40&md5=345ff43ec8ee0d94a6fb96535dcc3db3},
	doi = {10.1117/12.2002365},
	abstract = {Recent progress in the digitization of heterogeneous collections of ancient documents has rekindled new challenges in information retrieval in digital libraries and document layout analysis. Therefore, in order to control the quality of historical document image digitization and to meet the need of a characterization of their content using intermediate level metadata (between image and document structure), we propose a fast automatic layout segmentation of old document images based on five descriptors. Those descriptors, based on the autocorrelation function, are obtained by multiresolution analysis and used afterwards in a specific clustering method. The method proposed in this article has the advantage that it is performed without any hypothesis on the document structure, either about the document model (physical structure), or the typographical parameters (logical structure). It is also parameter-free since it automatically adapts to the image content. In this paper, firstly, we detail our proposal to characterize the content of old documents by extracting the autocorrelation features in the different areas of a page and at several resolutions. Then, we show that is possible to automatically find the homogeneous regions defined by similar indices of autocorrelation without knowledge about the number of clusters using adapted hierarchical ascendant classification and consensus clustering approaches. To assess our method, we apply our algorithm on 316 old document images, which encompass six centuries (1200-1900) of French history, in order to demonstrate the performance of our proposal in terms of segmentation and characterization of heterogeneous corpus content. Moreover, we define a new evaluation metric, the homogeneity measure, which aims at evaluating the segmentation and characterization accuracy of our methodology. We find a 85\% of mean homogeneity accuracy. Those results help to represent a document by a hierarchy of layout structure and content, and to define one or more signatures for each page, on the basis of a hierarchical representation of homogeneous blocks and their topology. © 2013 SPIE-IS\&T.},
	language = {English},
	urldate = {2013-02-05},
	booktitle = {Proc {SPIE} {Int} {Soc} {Opt} {Eng}},
	author = {Mehri, M. and Gomez-Krämer, P. and Héroux, P. and Mullot, R.},
	collaborator = {{The Society for Imaging Science and Technology (IS and T); The Society of Photo-Optical Instrumentation Engineers (SPIE); Qualcomm Inc.; Google Inc.}},
	year = {2013},
	note = {Journal Abbreviation: Proc SPIE Int Soc Opt Eng},
	keywords = {autocorrelation, Autocorrelation, Autocorrelation features, Autocorrelation functions, Characterization, Cluster analysis, Consensus clustering, consensus clustering., Digital image storage, Digital libraries, directional rose, Directional rose, Document layout analysis, HEMDIG - SCOPUS, Heterogeneous collections, Hierarchical representation, Image segmentation, Metadata, Multi-resolutions, multiresolution, Multiresolution analysis, Optical character recognition, Quality control, Regression analysis, SCOPUS, Segmentation},
}

@inproceedings{svendsen_document_2013,
	address = {Burlingame, CA},
	title = {Document segmentation via oblique cuts},
	volume = {8658},
	isbn = {0277786X (ISSN); 9780819494313 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875834336&doi=10.1117%2f12.2003351&partnerID=40&md5=c43481deebc1e3545cf418aece80359d},
	doi = {10.1117/12.2003351},
	abstract = {This paper presents a novel solution for the layout segmentation of graphical elements in Business Intelligence documents. We propose a generalization of the recursive X-Y cut algorithm, which allows for cutting along arbitrary oblique directions. An intermediate processing step consisting of line and solid region removal is also necessary due to presence of decorative elements. The output of the proposed segmentation is a hierarchical structure which allows for the identification of primitives in pie and bar charts. The algorithm was tested on a database composed of charts from business documents. Results are very promising. © 2013 SPIE-IS\&T.},
	language = {English},
	urldate = {2013-02-05},
	booktitle = {Proc {SPIE} {Int} {Soc} {Opt} {Eng}},
	author = {Svendsen, J. and Branzan-Albu, A.},
	collaborator = {{The Society for Imaging Science and Technology (IS and T); The Society of Photo-Optical Instrumentation Engineers (SPIE); Qualcomm Inc.; Google Inc.}},
	year = {2013},
	note = {Journal Abbreviation: Proc SPIE Int Soc Opt Eng},
	keywords = {Algorithms, Business documents, Chart Recognition, Document analysis, Document Analysis and Recognition, Document segmentation, Document Segmentation, Engineering, Graphical elements, HEMDIG - SCOPUS, Hierarchical structures, Molecular physics, Oblique direction, Processing steps, SCOPUS},
}

@inproceedings{baechler_text_2013,
	address = {Washington, DC},
	title = {Text line extraction using {DMLP} classifiers for historical manuscripts},
	isbn = {15205363 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889582038&doi=10.1109%2fICDAR.2013.206&partnerID=40&md5=94165d512203b615e4110187deb9f5a3},
	doi = {10.1109/ICDAR.2013.206},
	abstract = {This paper proposes a novel text line extraction method for historical documents. The method works in two steps. In the first step, layout analysis is performed to recognize the physical structure of a given document using a classification technique, more precisely the pixels of a coloured document image are classified into five classes: text-block, core-text-line, decoration, background, and periphery. This layout recognition is achieved by a cascade of two Dynamic Multilayer Perceptron (DMLP) classifiers and works without binarisation. In the second step, an algorithm takes the layout recognition results as an input, extracts the text lines, and groups them into blocks using the connected components approach. Finally, the algorithm refines the boundaries of the text lines using the binary image and the layout recognition results. Our system is evaluated on three historical manuscripts with a test set of 49 pages. The best obtained hit rate for text lines is 96.3\%. © 2013 IEEE.},
	language = {English},
	urldate = {2013-08-25},
	booktitle = {Proc. {Int}. {Conf}. {Doc}. {Anal}. {Recognit}.},
	author = {Baechler, M. and Liwicki, M. and Ingold, R.},
	collaborator = {{Raytheon BBN Technologies; ABBYY; VisionObjects; Google; HITACHI}},
	year = {2013},
	note = {Journal Abbreviation: Proc. Int. Conf. Doc. Anal. Recognit.},
	keywords = {Algorithms, Character recognition, Classification (of information), Classification technique, Connected component, Document images, HEMDIG - SCOPUS, Historical documents, History, Information retrieval systems, Layout analysis, Multi layer perceptron, Physical structures, SCOPUS, Text processing, Text-line extractions},
	pages = {1029--1033},
}

@inproceedings{mehri_pixel_2013,
	address = {Washington, DC},
	title = {A pixel labeling approach for historical digitized books},
	isbn = {15205363 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889588853&doi=10.1109%2fICDAR.2013.167&partnerID=40&md5=7f0fa5416d9c0dbafb2cc28b47468f60},
	doi = {10.1109/ICDAR.2013.167},
	abstract = {In the context of historical collection conservation and worldwide diffusion, this paper presents an automatic approach of historical book page layout segmentation. In this article, we propose to search the homogeneous regions from the content of historical digitized books with little a priori knowledge by extracting and analyzing texture features. The novelty of this work lies in the unsupervised clustering of the extracted texture descriptors to find homogeneous regions, i.e. graphic and textual regions, by performing the clustering approach on an entire book instead of processing each page individually. We propose firstly to characterize the content of an entire book by extracting the texture information of each page, as our goal is to compare and index the content of digitized books. The extraction of texture features, computed without any hypothesis on the document structure, is based on two non-parametric tools: the autocorrelation function and multiresolution analysis. Secondly, we perform an unsupervised clustering approach on the extracted features in order to classify automatically the homogeneous regions of book pages. The clustering results are assessed by internal and external accuracy measures. The overall results are quite satisfying. Such analysis would help to construct a computer-aided categorization tool of pages. © 2013 IEEE.},
	language = {English},
	urldate = {2013-08-25},
	booktitle = {Proc. {Int}. {Conf}. {Doc}. {Anal}. {Recognit}.},
	author = {Mehri, M. and Heroux, P. and Gomez-Kramer, P. and Boucher, A. and Mullot, R.},
	collaborator = {{Raytheon BBN Technologies; ABBYY; VisionObjects; Google; HITACHI}},
	year = {2013},
	note = {Journal Abbreviation: Proc. Int. Conf. Doc. Anal. Recognit.},
	keywords = {autocorrelation, Autocorrelation, Cluster analysis, Clustering accuracy, clustering accuracy metrics, Clustering algorithms, Computer aided analysis, consensus clustering, Consensus clustering, HEMDIG - SCOPUS, Historical books, homogeneity, multiresolution, Multiresolution, pixel labeling, Pixel labeling, Pixels, SCOPUS, texture, Textures, Tools},
	pages = {817--821},
}

@book{mujibiya_glassnage_2015,
	series = {3rd {International} {Conference} on {Distributed}, {Ambient} and {Pervasive} {Interactions}, {DAPI} 2015 {Held} as {Part} of 17th {International} {Conference} on {Human} {Computer} {Interaction}, {HCI} {International} 2015},
	title = {Glassnage: {Layout} recognition for dynamic content retrieval in multi-section digital signage},
	volume = {9189},
	isbn = {03029743 (ISSN); 9783319208039 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947277440&doi=10.1007%2f978-3-319-20804-6_31&partnerID=40&md5=f9c515d4bf2c47ae8f792074151b13ad},
	abstract = {We report our approach to support dynamic content transfer from publicly available large display digital signage to users’ private display, specifically Glass-like wearable devices. We aim to address issues concerning dynamic multimedia signage where the content are divided into several sections. This type of signage has become increasingly popular due to optimal content exposures. In contrast to prior research, our approach excludes computer vision based object recognition, and instead took an approach to identify how contents are being laid-out in a digital signage. We incorporate techniques to recognize basic layout features including corners, lines, edges, and line segments; which are obtained from the camera frame taken by the user using their own device. Consequently, these layout features are combined to generate signage layout map, which is then compared to pre-learned layout map for position detection and perspective correction using homography estimation. To grab a specific content, users are able to choose a section within the captured layout using the device’s interface, which in turn creates a request to contents server to send respective content information based on a timestamp and a unique section ID. In this paper, we describe implementation details, report user study results, and conclude with discussion of our experiences in implementation as well as highlighting future work. © Springer International Publishing Switzerland 2015.},
	language = {English},
	urldate = {2015-08-02},
	publisher = {Springer Verlag},
	author = {Mujibiya, A.},
	editor = {{Markopoulos P.} and {Streitz N.}},
	year = {2015},
	doi = {10.1007/978-3-319-20804-6_31},
	note = {Journal Abbreviation: Lect. Notes Comput. Sci.
Pages: 348
Publication Title: Lect. Notes Comput. Sci.},
	keywords = {Computer vision, Digital devices, Digital signage, Display devices, Electronic document exchange, Glass, HEMDIG - SCOPUS, Human computer interaction, Layout recognition, Line segment, Multi section, Multi-section, Object recognition, Public display, Public-to-private, SCOPUS, User study, Visual feature, Visual features},
}

@article{keefer_survey_2015,
	title = {A {Survey} on {Document} {Image} {Processing} {Methods} {Useful} for {Assistive} {Technology} for the {Blind}},
	volume = {15},
	issn = {02194678 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013150028&doi=10.1142%2fS0219467815500059&partnerID=40&md5=b50cb1d0e014f54ce9de25a9d195e675},
	doi = {10.1142/S0219467815500059},
	abstract = {This paper offers a review of the state-of-the-art document image processing methods and their classification by identifying new trends for automatic document processing and understanding. Document image processing (DIP) is an important problem related with most of the challenges coming from the image processing field and with applications to digital document summarization, readers for the visually impaired etc. Difficulties in the processing of documents can arise from lighting conditions, page curl, page rotation in 3D, and page layout segmentation. Document image processing is usually performed in the context of higher-level applications that require an undistorted document image such as optical character recognition and document restoration/preservation. Typically, assumptions are made to constrain the processing problem in the context of a particular application. In this survey, we categorize document image processing methods on the basis of the technique, provide detailed descriptions of representative methods in each category, and examine their pros and cons. It important to notice here that the DIP field is broad, thus we try to provide a top-down/horizontal survey rather a bottom up. At the same time, we target the area of document readers for the blind, and use this application to guide us in a top-down survey of DIP. Moreover, we present a comparative survey based on important aspects of a marketable system that is dependent on document image processing techniques. © 2015 World Scientific Publishing Company.},
	language = {English},
	number = {1},
	journal = {International Journal of Image and Graphics},
	author = {Keefer, R. and Bourbakis, N.},
	year = {2015},
	note = {Publisher: World Scientific},
	keywords = {Assistive technology, Automatic document processing, Digital image storage, Document image processing, document image segmentation, Document image segmentation, Document restoration, HEMDIG - SCOPUS, Image segmentation, Information retrieval systems, Lighting conditions, Optical character recognition, Optical data processing, page curl correction, Processing, Processing problems, SCOPUS, skew correction, Skew corrections, Surveys},
}

@inproceedings{komatsu_area_2015,
	title = {Area detection technology for air conditioner},
	isbn = {9784907764487 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960158187&doi=10.1109%2fSICE.2015.7285402&partnerID=40&md5=6228e69bab9578dd02804f70f8a46e82},
	doi = {10.1109/SICE.2015.7285402},
	abstract = {We developed room layout detection technology for air conditioners. This technology uses an image camera that is equipped with the air conditioners. The technology controls the direction of the wind and their air capacity. It has been installed in commercial products as a 'layout search.'. © 2015 The Society of Instrument and Control Engineers-SICE.},
	language = {English},
	urldate = {2015-07-28},
	booktitle = {Annu. {Conf}. {Soc}. {Instr}. {Control} {Eng}. {Japan}, {SICE}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Komatsu, Y. and Hamada, K. and Nukaga, N. and Kagehiro, T. and Ueda, Y. and Matsubara, E. and Jinno, N.},
	year = {2015},
	note = {Journal Abbreviation: Annu. Conf. Soc. Instr. Control Eng. Japan, SICE},
	keywords = {Air capacity, Air conditioner, Air conditioning, Area detection, Camera, Cameras, Commercial products, Detection technology, Domestic appliances, HEMDIG - SCOPUS, Layout recognition, Partition recognition, SCOPUS, Technology use},
	pages = {1094--1100},
}

@article{lou_extracting_2015,
	title = {Extracting {3D} {Layout} {From} a {Single} {Image} {Using} {Global} {Image} {Structures}},
	volume = {24},
	issn = {10577149 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933056746&doi=10.1109%2fTIP.2015.2431443&partnerID=40&md5=f6fe8de073fdd311dd6584ddc9f0d0db},
	doi = {10.1109/TIP.2015.2431443},
	abstract = {Extracting the pixel-level 3D layout from a single image is important for different applications, such as object localization, image, and video categorization. Traditionally, the 3D layout is derived by solving a pixel-level classification problem. However, the image-level 3D structure can be very beneficial for extracting pixel-level 3D layout since it implies the way how pixels in the image are organized. In this paper, we propose an approach that first predicts the global image structure, and then we use the global structure for fine-grained pixel-level 3D layout extraction. In particular, image features are extracted based on multiple layout templates. We then learn a discriminative model for classifying the global layout at the image-level. Using latent variables, we implicitly model the sublevel semantics of the image, which enrich the expressiveness of our model. After the image-level structure is obtained, it is used as the prior knowledge to infer pixel-wise 3D layout. Experiments show that the results of our model outperform the state-of-the-art methods by 11.7\% for 3D structure classification. Moreover, we show that employing the 3D structure prior information yields accurate 3D scene layout segmentation. © 1992-2012 IEEE.},
	language = {English},
	number = {10},
	journal = {IEEE Transactions on Image Processing},
	author = {Lou, Z. and Gevers, T. and Hu, N.},
	year = {2015},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {3D layout, 3D layouts, Discriminative models, HEMDIG - SCOPUS, Image analysis, Image processing, Image Structures, Object localization, Pixels, Prior information, SCOPUS, Semantics, Stage classification, State-of-the-art methods, Structural SVM, Three dimensional computer graphics, Video categorization},
	pages = {3098--3108},
}

@article{lin_fast_2015,
	title = {Fast layout segmentation algorithm based on sparsity difference},
	volume = {43},
	issn = {16714512 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945276099&doi=10.13245%2fj.hust.15S1127&partnerID=40&md5=222bfbdbb1ba5e2cd0647e16c64188d3},
	doi = {10.13245/j.hust.15S1127},
	abstract = {A fast page segmentation method based on the region sparsity difference was proposed to simplify the layout segmentation method and improve the efficiency of layout detection. The sparse degree was employed as the feature description of page regions. Then the threshold segmentation was applied to obtain the category labels according to the difference between contents and the backgrounds. Our approach was tested on a complex layout image set. The experimental results, accurate rate 74.1\% and the computation time 1.61 seconds, show the effectiveness and rapidity of our method. Comparing with the improved connectivity-based layout segmentation method, our method has simpler process, easier computation and better applicability. ©, 2015, Huazhong University of Science and Technology. All right reserved.},
	language = {Chinese},
	journal = {Huazhong Keji Daxue Xuebao (Ziran Kexue Ban)/Journal of Huazhong University of Science and Technology (Natural Science Edition)},
	author = {Lin, L. and Zhang, R. and Zhao, G. and Liu, G.},
	year = {2015},
	note = {Publisher: Huazhong University of Science and Technology},
	keywords = {Character recognition, Computation time, Feature description, Feature extraction, HEMDIG - SCOPUS, Image processing, Image segmentation, Layout analysis, Layout segmentation, Page segmentation, Pattern recognition, SCOPUS, Segmentation algorithms, Segmentation methods, Sparsity, Text layout analysis, Threshold segmentation},
	pages = {533--535 and 544},
}

@article{shruti_novel_2016,
	title = {Novel technique for layout and handwritten character recognition in {OCR}},
	volume = {9},
	issn = {09745572 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006489792&partnerID=40&md5=4b435b4978d43187b1b6d7ef6cf495fe},
	abstract = {As the result of enhancement and improvement in optical character recognition, various techniques have been implemented for handwritten character recognition. Document analysis and recognition plays a vital role in transferring the data between human being and computer. Segmentation of document is the very foremost step in document image analysis. Document segmentation is defined as the technique in which we chunk or partition our both homogeneous or heterogeneous data. Heterogeneous data is called as the data which contains printed text or handwritten text or graph or all of these together in one single document .Segmentation of the document is required because OCR is not able to recognize the whole document which contains multiple data types. Therefore first of all document segmentation is to be applied on handwritten bills which contain heterogeneous data type in order to differentiate between printed text and handwritten text. © 2016 International Science Press.},
	language = {English},
	number = {18},
	journal = {International Journal of Control Theory and Applications},
	author = {{Shruti} and Kapoor, R.},
	year = {2016},
	note = {Publisher: Serials Publications},
	keywords = {Documentation. Layout segmentation, HEMDIG - SCOPUS, SCOPUS, Segmentation, Text segmentation},
	pages = {8877--8881},
}

@article{chherawala_arabic_2014,
	title = {Arabic word descriptor for handwritten word indexing and lexicon reduction},
	volume = {47},
	issn = {00313203 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902372575&doi=10.1016%2fj.patcog.2014.04.025&partnerID=40&md5=98d011fd3855a5eb8f7b1fffe9394281},
	doi = {10.1016/j.patcog.2014.04.025},
	abstract = {Word recognition systems use a lexicon to guide the recognition process in order to improve the recognition rate. However, as the lexicon grows, the computation time increases. In this paper, we present the Arabic word descriptor (AWD) for Arabic word shape indexing and lexicon reduction in handwritten documents. It is formed in two stages. First, the structural descriptor (SD) is computed for each connected component (CC) of the word image. It describes the CC shape using the bag-of-words model, where each visual word represents a different local shape structure, extracted from the image with filters of different patterns and scales. Then, the AWD is formed by sorting and normalizing the SDs. This emphasizes the symbolic features of Arabic words, such as subwords and diacritics, without performing layout segmentation. In the context of lexicon reduction, the AWD is used to index a reference database. Given a query image, the reduced lexicon is obtained from the labels of the first entries in the indexed database. This framework has been tested on Arabic word databases. It has a low computational overhead, while providing a compact descriptor, with state-of-the-art results for lexicon reduction on the Ibn Sina and IFN/ENIT databases. © 2014 Elsevier Ltd.},
	language = {English},
	number = {10},
	journal = {Pattern Recognition},
	author = {Chherawala, Y. and Cheriet, M.},
	year = {2014},
	note = {Publisher: Elsevier Ltd},
	keywords = {Arabic handwritten documents, Arabic word descriptor, Database systems, Descriptors, HEMDIG - SCOPUS, Holistic representation, Ibn Sina database, IFN/ENIT, Indexing (of information), Information retrieval, Lexicon reduction, Lexicon reductions, Query processing, SCOPUS, Shape indexing, Vocabulary control},
	pages = {3477--3486},
}

@article{duan_layout_2017,
	title = {Layout analysis algorithm of questionnaire image},
	volume = {28},
	issn = {10009825 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014436410&doi=10.13328%2fj.cnki.jos.005032&partnerID=40&md5=9031b2776464a284678425d15d1e5ae3},
	doi = {10.13328/j.cnki.jos.005032},
	abstract = {The recognition of the information area with common format in the non-fixed format questionnaire is the major problem in existing questionnaire layout recognition algorithm. To address those problems, a new approach for questionnaire layout analysis based on regional connectivity and neural networks is proposed. First, a center valid graphics is generated by preprocessing the scanned image firstly. Then, a rapid skew correction algorithm is applied for questionnaire images. Next, many questionnaire rows are obtained by using horizontal projection profile segmentation algorithms. After that, the first connected region for each row is extracted to estimate the existence of form region. Based on the analysis of general questionnaire row and table row, a large amount of possible answers region are generated. Finally, the neural network is used to determine the type of possible information areas. Experiments show that the proposed algorithm can automatically identify common questionnaire. © Copyright 2017, Institute of Software, the Chinese Academy of Sciences. All rights reserved.},
	language = {Chinese},
	number = {2},
	journal = {Ruan Jian Xue Bao/Journal of Software},
	author = {Duan, L. and Song, Y.-H. and Zhang, Y.-L.},
	year = {2017},
	note = {Publisher: Chinese Academy of Sciences},
	keywords = {Connected region, Connective region, Edge detection, HEMDIG - SCOPUS, Horizontal projection profile, Image segmentation, Layout analysis, Line split, Neural network, Neural networks, Recognition algorithm, SCOPUS, Skew corrections, Surveys, Table recognition},
	pages = {234--245},
}

@inproceedings{zhu_segmentation_2017,
	title = {A segmentation algorithm based on image projection for complex text layout},
	volume = {1890},
	isbn = {0094243X (ISSN); 9780735415683 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031097249&doi=10.1063%2f1.5005199&partnerID=40&md5=79e4d70dccc9cde0902e9bdd2c8cde29},
	doi = {10.1063/1.5005199},
	abstract = {Segmentation algorithm is an important part of layout analysis, considering the efficiency advantage of the top-down approach and the particularity of the object, a breakdown of projection layout segmentation algorithm. Firstly, the algorithm will algorithm first partitions the text image, and divided into several columns, then for each column scanning projection, the text image is divided into several sub regions through multiple projection. The experimental results show that, this method inherits the projection itself and rapid calculation speed, but also can avoid the effect of arc image information page segmentation, and also can accurate segmentation of the text image layout is complex. © 2017 Author(s).},
	language = {English},
	urldate = {2017-10-27},
	booktitle = {{AIP} {Conf}. {Proc}.},
	publisher = {American Institute of Physics Inc.},
	author = {Zhu, W. and Chen, Q. and Wei, C. and Li, Z.},
	year = {2017},
	note = {Journal Abbreviation: AIP Conf. Proc.},
	keywords = {HEMDIG - SCOPUS, projection, SCOPUS, Text image, Text layout segmentation, top-down breakdown},
}

@inproceedings{ma_time_2018,
	title = {From {Time} to {Space}: {Automatic} {Annotation} of {Unmarked} {Traffic} {Scene} {Based} on {Trajectory} {Data}},
	isbn = {9781728103761 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064140224&doi=10.1109%2fROBIO.2018.8665110&partnerID=40&md5=61c385ca89ac80d162d6ac953a7ea228},
	doi = {10.1109/ROBIO.2018.8665110},
	abstract = {Research on autonomous driving is requiring more affordance information from the environment, which can be efficiently used by the subject vehicle. Road layout parsing is fundamentally important to this, and has attracted deserved attention on the urban road net where traffic signs are well established. However, for the unmarked traffic scenes, like campus and residence community, the large variation on individual surroundings leads to a reduced attention on public research and a lack of both available dataset and generic methods. To explore this issue, we present a novel automatic annotation method to analyze road semantics, which regards the prior trajectories of a vehicle as multi-dimensional space series and extends the traditional time series methods to the space domain to process the data. In order to achieve this, we first propose Incremental Dynamic Space Warping (IDSW) to synchronize space series and extract statistical road feature, then a Bayesian nonparametric method based on the Hierarchical Dirichlet Process-Hidden Markov Model (HDP-HMM) is introduced to generate road semantic sections without explicitly specifying the number of class. Experiment results demonstrate the proposed method can produce road semantics in different level and annotate not only the physical road position but also the vision observations: based on the extracted road modes, the method can annotate corresponding vision image with a positive class label and a rough distance to the road intersection. © 2018 IEEE.},
	language = {English},
	urldate = {2018-12-12},
	booktitle = {{IEEE} {Int}. {Conf}. {Robot}. {Biomimetics}, {ROBIO}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Ma, H. and Wang, Y. and Xiong, R.},
	collaborator = {et al.; Guangdong University of Technology; Guangzhou University; IEEE; IEEE Robotics {and} Automation Society (RAS); Shenzhen Han's Robot Co., Ltd.},
	year = {2018},
	note = {Journal Abbreviation: IEEE Int. Conf. Robot. Biomimetics, ROBIO},
	keywords = {Automatic annotation, Autonomous driving, Autonomous vehicles, Biomimetics, HEMDIG - SCOPUS, Hidden Markov models, Hierarchical dirichlet process hidden markov model (HDP HMM), Large dataset, Multi-dimensional space, Nonparametric methods, Number of class, Road intersections, Roads and streets, Robotics, SCOPUS, Semantics, Time series method, Traffic signs},
	pages = {1177--1182},
}

@inproceedings{sharma_unified_2016,
	title = {A unified framework for semantic matching of architectural floorplans},
	volume = {0},
	isbn = {10514651 (ISSN); 9781509048472 (ISBN)},
	doi = {10.1109/ICPR.2016.7899999},
	abstract = {An automatic lookup tool, which matches and retrieves similar floorplans from a large repository of digitized architectural floorplans can prove to be of immense help for the architects while designing new projects. In this paper, we have proposed a framework for the matching and retrieval of similar architectural floorplans under the query by example paradigm. We propose a room layout segmentation and adjacent room detection algorithm to represent layouts as an undirected graph. We have also proposed a novel graph spectral embedding feature to uniquely represent the layout of the architectural floorplan. This helps in effective and efficient matching of the room layouts. Room semantics in terms of both the room structures and room decor is used to retrieve similar floorplans from the repository. To match the semantic similarity between a pair of floorplans, we have proposed a two stage matching technique. We have validated the effectiveness of our proposed framework by performing experiments on publicly available floorplan dataset and achieved high retrieval accuracy. © 2016 IEEE.},
	language = {English},
	urldate = {2016-12-04},
	booktitle = {Proc. {Int}. {Conf}. {Pattern} {Recognit}.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Sharma, D. and Chattopadhyay, C. and Harit, G.},
	year = {2016},
	note = {Journal Abbreviation: Proc. Int. Conf. Pattern Recognit.},
	keywords = {Detection algorithm, Graph spectral, HEMDIG - SCOPUS, Pattern recognition, Query-by example, Retrieval accuracy, SCOPUS, Semantic matching, Semantic similarity, Semantics, Undirected graph, Unified framework},
	pages = {2422--2427},
}

@inproceedings{corbelli_historical_2016,
	title = {Historical document digitization through layout analysis and deep content classification},
	volume = {0},
	isbn = {10514651 (ISSN); 9781509048472 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019108141&doi=10.1109%2fICPR.2016.7900272&partnerID=40&md5=b82347e3a1e032e014b5a885733962de},
	doi = {10.1109/ICPR.2016.7900272},
	abstract = {Document layout segmentation and recognition is an important task in the creation of digitized documents collections, especially when dealing with historical documents. This paper presents an hybrid approach to layout segmentation as well as a strategy to classify document regions, which is applied to the process of digitization of an historical encyclopedia. Our layout analysis method merges a classic top-down approach and a bottom-up classification process based on local geometrical features, while regions are classified by means of features extracted from a Convolutional Neural Network merged in a Random Forest classifier. Experiments are conducted on the first volume of the 'Enciclopedia Treccani', a large dataset containing 999 manually annotated pages from the historical Italian encyclopedia. © 2016 IEEE.},
	language = {English},
	urldate = {2016-12-04},
	booktitle = {Proc. {Int}. {Conf}. {Pattern} {Recognit}.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Corbelli, A. and Baraldi, L. and Grana, C. and Cucchiara, R.},
	year = {2016},
	note = {Journal Abbreviation: Proc. Int. Conf. Pattern Recognit.},
	keywords = {Classification process, Content classification, Convolutional neural network, Decision trees, Document layouts, Geometrical features, HEMDIG - SCOPUS, Historical documents, History, Information retrieval systems, Neural networks, Pattern recognition, Random forest classifier, SCOPUS, Top down approaches},
	pages = {4077--4082},
}

@inproceedings{cote_layered_2016,
	title = {Layered ground truth: {Conveying} structural and statistical information for document image analysis and evaluation},
	volume = {0},
	isbn = {10514651 (ISSN); 9781509048472 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019160635&doi=10.1109%2fICPR.2016.7900137&partnerID=40&md5=548726d07d4e9e14000e49c61ab1eb05},
	doi = {10.1109/ICPR.2016.7900137},
	abstract = {This paper addresses the problem of semantic overlap across document objects in the context of ground truth representation for document layout analysis. Document object categories often share primitives from a low-level perspective (e.g. regions inside bars in a bar chart resemble background), making it difficult to evaluate document layout segmentation methods based on pixel classification, as most datasets and ground truth models focus on document objects. We propose a novel ground truth model that utilizes structural and statistical pattern recognition concepts. Statistical pixel-based data derived from low-level elemental patterns are layered onto high-level structural object-based data. We also present evaluation metrics that take advantage of the layered ground truth model, allowing a contextual evaluation of pixel classification algorithms. We apply the proposed model to two recent pixel classification approaches, evaluated on business document images that exhibit a challenging mixture of textual, graphical, and pictorial elements through varied layouts. The proposed model allows to obtain very detailed, comprehensive, and intuitive information on the strengths and limitations of the evaluated approaches that would be impossible to obtain through other models. © 2016 IEEE.},
	language = {English},
	urldate = {2016-12-04},
	booktitle = {Proc. {Int}. {Conf}. {Pattern} {Recognit}.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Cote, M. and Albu, A.B.},
	year = {2016},
	note = {Journal Abbreviation: Proc. Int. Conf. Pattern Recognit.},
	keywords = {Classification (of information), Digital image storage, Document image dataset, Document images, Document layout analysis, Ground truth, HEMDIG - SCOPUS, Image analysis, Image segmentation, Information retrieval systems, Pattern recognition, Performance evaluation, Performance evaluations, Pixels, SCOPUS, Semantics, Statistical pattern, Structural analysis, Structural pattern},
	pages = {3258--3263},
}

@inproceedings{kassis_scribble_2016,
	title = {Scribble based interactive page layout segmentation using gabor filter},
	volume = {0},
	isbn = {21676445 (ISSN); 9781509009817 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012914756&doi=10.1109%2fICFHR.2016.0016&partnerID=40&md5=2547068225a54593edfef6964d9fad67},
	doi = {10.1109/ICFHR.2016.0016},
	abstract = {This paper presents an interactive approach for fast and accurate page layout segmentation. It is a scribble-based interactive segmentation approach, where the user draws scribbles on the various regions and the system performs page layout segmentation. The user can correct and refine the resulting segmentation by drawing new scribbles. To classify the various regions of the page, we apply a bank of Gabor filters, in several orientations and multiple frequencies, to capture the orientation, the stroke width, and size of the text. These properties also implicitly encode the writing style of the document. After combining the responses of the Gabor filter into a feature matrix, we classify various regions of the document by applying graph cuts, while taking into account the user made scribbles. The presented approach is very fast, easy to use, robust to user interaction, and provides accurate results. © 2016 IEEE.},
	language = {English},
	urldate = {2016-10-23},
	booktitle = {Proc. {Int}. {Conf}. {Front}. {Handwrit}. {Recognit}., {ICFHR}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Kassis, M. and El-Sana, J.},
	collaborator = {{CASIA; Fujitsu; Master Learner; Micropattern; TC11 (Reading Systems) of the International Association of Pattern Recognition (IAPR); Tsinghua University}},
	year = {2016},
	note = {Journal Abbreviation: Proc. Int. Conf. Front. Handwrit. Recognit., ICFHR},
	keywords = {Character recognition, Feature matrices, Gabor filter, Gabor filters, Graphic methods, HEMDIG - SCOPUS, Interactive approach, Interactive pages, Interactive segmentation, Interactive system, Multiple frequency, Page segmentation, SCOPUS, Scribble-based},
	pages = {13--18},
}

@book{corbelli_layout_2017,
	series = {12th {Italian} {Research} {Conference} on {Digital} {Libraries}, {IRCDL} 2016},
	title = {Layout analysis and content classification in digitized books},
	volume = {701},
	isbn = {18650929 (ISSN); 9783319562995 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018670416&doi=10.1007%2f978-3-319-56300-8_14&partnerID=40&md5=5a6d2e565df550237252802a4797d2d9},
	abstract = {Automatic layout analysis has proven to be extremely important in the process of digitization of large amounts of documents. In this paper we present amixed approach to layout analysis, introducing a SVMaided layout segmentation process and a classification process based on local and geometrical features. The final output of the automatic analysis algorithm is a complete and structured annotation in JSON format, containing the digitalized text aswell as all the references to the illustrations of the input page, and which can be used by visualization interfaces as well as annotation interfaces. We evaluate our algorithm on a large dataset built upon the first volume of the “Enciclopedia Treccani”. © Springer International Publishing AG 2017.},
	language = {English},
	urldate = {2016-02-04},
	publisher = {Springer Verlag},
	author = {Corbelli, A. and Baraldi, L. and Balducci, F. and Grana, C. and Cucchiara, R.},
	editor = {{Bertini M.} and {Marinai S.} and {Ferilli S.} and {Agosti M.} and {Orio N.}},
	translator = {{MICC Center of Excellence; University of Florence}},
	year = {2017},
	doi = {10.1007/978-3-319-56300-8_14},
	note = {Journal Abbreviation: Commun. Comput. Info. Sci.
Pages: 165
Publication Title: Commun. Comput. Info. Sci.},
	keywords = {Annotation interfaces, Automatic analysis, Automatic layout, Classification process, Computer science, Computers, Content classification, Digital libraries, Geometrical features, HEMDIG - SCOPUS, Large amounts, Layout analysis, SCOPUS, Segmentation process, SVM},
}

@inproceedings{casia_fujitsu_master_learner_micropattern_tc11_reading_systems_of_the_international_association_of_pattern_recognition_iapr_tsinghua_university_proceedings_2017,
	title = {Proceedings of {International} {Conference} on {Frontiers} in {Handwriting} {Recognition}, {ICFHR}},
	isbn = {21676445 (ISSN); 9781509009817 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012902138&partnerID=40&md5=9713f925df264cb45eb0a71ec738da1f},
	abstract = {The proceedings contain 107 papers. The topics discussed include: learning text-line localization with shared and local regression neural networks; discovering visual element evolutions for historical document dating; scribble based interactive page layout segmentation using Gabor filter; automatic signature segmentation using hyper-spectral imaging; Fourier coefficients for fraud handwritten document classification through age analysis; word spotting using radial descriptor graph; new tampered features for scene and caption text classification in video frame; multiple generation of Bengali static signatures; recognizing off-line flowcharts by reconstructing strokes and using on-line recognition techniques; a connection reduced network for similar handwritten Chinese character discrimination; and exploiting existing modern transcripts for historical handwritten text recognition.},
	language = {English},
	urldate = {2016-10-23},
	booktitle = {Proc. {Int}. {Conf}. {Front}. {Handwrit}. {Recognit}., {ICFHR}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	collaborator = {{CASIA; Fujitsu; Master Learner; Micropattern; TC11 (Reading Systems) of the International Association of Pattern Recognition (IAPR); Tsinghua University}},
	year = {2017},
	note = {Journal Abbreviation: Proc. Int. Conf. Front. Handwrit. Recognit., ICFHR},
	keywords = {HEMDIG - SCOPUS, SCOPUS},
}

@inproceedings{cu_watermarking_2018,
	title = {Watermarking for security issue of handwritten documents with fully convolutional networks},
	volume = {2018-August},
	isbn = {21676445 (ISSN); 9781538658758 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060059726&doi=10.1109%2fICFHR-2018.2018.00060&partnerID=40&md5=c713b12c2f1ed6f3b09b66712b181e21},
	doi = {10.1109/ICFHR-2018.2018.00060},
	abstract = {To prevent falsification of handwriting document images, the methods of forensic document examination are widely used to determine the origin and authenticity of a given document. In this paper, we propose an effective approach for security issue of handwriting documents in spatial domain by making use of watermarking technique. To begin with, the handwritten document is pre-processed by replacing gray level values holding high intensity with the mean value of document content. The document is then transformed into standard form to minimize geometric distortion. Next, fully convolutional networks (FCN) is leveraged to detect document's watermarking regions used for hiding secret information wherein an approach of FCN for document layout segmentation is adjusted to solve the problem of watermarking region detection. Lastly, the data hiding process is conducted by dividing gray level values of each connected object situated within watermarking regions into two sets for carrying one watermark bit. The experiments are performed on various handwritten documents, and our approach achieves high performance regarding such properties as imperceptibility and robustness against distortions caused by JPEG compression, geometric transformation and print-and-scan process. © 2018 IEEE.},
	language = {English},
	urldate = {2018-08-05},
	booktitle = {Proc. {Int}. {Conf}. {Front}. {Handwrit}. {Recognit}., {ICFHR}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Cu, V.L. and Burie, J.-C. and Ogier, J.-M.},
	year = {2018},
	note = {Journal Abbreviation: Proc. Int. Conf. Front. Handwrit. Recognit., ICFHR},
	keywords = {Character recognition, Convolution, Convolutional networks, Digital watermarking, document analysis, Document analysis, Document examinations, FCN, Geometric transformations, Handwriting documents, Handwritten document, handwritten document security, HEMDIG - SCOPUS, Image compression, Mathematical transformations, Network security, Print and scan process, SCOPUS, watermarking, Watermarking, Watermarking algorithms, watermarking regions},
	pages = {303--308},
}

@inproceedings{xu_layout_2019,
	title = {Layout recognition attacks on split manufacturing},
	isbn = {9781450360074 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061118278&doi=10.1145%2f3287624.3287698&partnerID=40&md5=29af44051974fad487d442a8cee4505c},
	doi = {10.1145/3287624.3287698},
	abstract = {One technique to prevent attacks from an untrusted foundry is split manufacturing, where only a part of the layout is sent to the untrusted high-end foundry, and the rest is manufactured at a trusted low-end foundry. The untrusted foundry has front-end-of-line (FEOL) layout and the original circuit netlist and attempts to identify critical components on the layout for Trojan insertion. Although defense methods for this scenario have been developed, the corresponding attack technique is not well explored. For instance, Boolean satisfiability (SAT) based bijective mapping attack is mentioned without detailed research. Hence, the defense methods are mostly evaluated with the k-security metric without actual attacks. We provide the first systematic study, to the best of our knowledge, on attack techniques in this scenario. Besides of implementing SAT-based bijective mapping attack, we develop a new attack technique based on structural pattern matching. Experimental comparison with bijective mapping attack shows that the new attack technique achieves about the same success rate with much faster speed for cases without the k-security defense, and has a much better success rate at the same runtime for cases with k-security defense. The results offer an alternative and practical interpretation for k-security in split manufacturing. © 2019 Association for Computing Machinery.},
	language = {English},
	urldate = {2019-01-21},
	booktitle = {Proc {Asia} {South} {Pac} {Des} {Autom} {Conf}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Xu, W. and Feng, L. and Rajendran, J.J.V. and Hu, J.},
	collaborator = {ACM SIGDA; Cadence Design Systems, Inc.; CEDA; EIC; IEEE CAS; IPSJ},
	year = {2019},
	note = {Journal Abbreviation: Proc Asia South Pac Des Autom Conf},
	keywords = {Bijective mapping, Boolean satisfiability, Computer aided design, Critical component, Experimental comparison, Foundries, Front end of lines, Hardware security, HEMDIG - SCOPUS, Layout recognition attack, Malware, Manufacture, Mapping, Network security, Pattern matching, Recognition attacks, SCOPUS, Security metrices, Split manufacturing, Structural pattern matching},
	pages = {45--50},
}

@article{park_extended_2019,
	title = {An {Extended} {Agent} {Communication} {Framework} for {Rapid} {Reconfiguration} of {Distributed} {Manufacturing} {Systems}},
	volume = {15},
	issn = {15513203 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057893926&doi=10.1109%2fTII.2018.2883409&partnerID=40&md5=c636e384d6fb5c2ac5c10d953ec8ae0c},
	doi = {10.1109/TII.2018.2883409},
	abstract = {For proactive responses to customer requirements and rapidly advancing technologies, current manufacturing systems are required to be increasingly adaptable and reconfigurable. We present a communication framework between the coordinator, workstation agents, and executors to facilitate the reconfiguration process of manufacturing systems. The message structure of interagent information exchange is extended for the initiation, self-layout recognition, and control program update phases of reconfiguration. To provide more autonomy to a manufacturing system and to synchronize the updated shop floor condition, a self-layout recognition mechanism was developed based on infrared communication that enables each workstation to identify directly linked workstations and share the identified information with other shop floor entities for automatic layout change detection. Furthermore, we propose a message-based reprogramming approach to quickly update, insert, or delete command lines in the existing control program for a specific device in a workstation. We demonstrate a reconfiguration process via the communication framework in a factory testbed consisting of nine main workstations in a laboratory environment. © 2018 IEEE.},
	language = {English},
	number = {7},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Park, J.W. and Shin, M. and Kim, D.Y.},
	year = {2019},
	note = {Publisher: IEEE Computer Society},
	keywords = {Agent communication, Agent communications, Communication framework, Computer aided manufacturing, Computer workstations, Distributed manufacturing systems, Floors, HEMDIG - SCOPUS, IEC standards, Information dissemination, Manufacture, Network protocols, Rapidly advancing technology, Reconfigurable manufacturing system, reconfigurable manufacturing system (RMS), Reconfiguration process, SCOPUS, self-layout recognition},
	pages = {3845--3855},
}

@inproceedings{prusty_indiscapes_2019,
	title = {Indiscapes: {Instance} segmentation networks for layout parsing of historical indic manuscripts},
	isbn = {15205363 (ISSN); 9781728128610 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079901565&doi=10.1109%2fICDAR.2019.00164&partnerID=40&md5=1a89ae188c80c494a24cecb8b7dfa366},
	doi = {10.1109/ICDAR.2019.00164},
	abstract = {Historical palm-leaf manuscript and early paper documents from Indian subcontinent form an important part of the world's literary and cultural heritage. Despite their importance, large-scale annotated Indic manuscript image datasets do not exist. To address this deficiency, we introduce Indiscapes, the first ever dataset with multi-regional layout annotations for historical Indic manuscripts. To address the challenge of large diversity in scripts and presence of dense, irregular layout elements (e.g. text lines, pictures, multiple documents per image), we adapt a Fully Convolutional Deep Neural Network architecture for fully automatic, instance-level spatial layout parsing of manuscript images. We demonstrate the effectiveness of proposed architecture on images from the Indiscapes dataset. For annotation flexibility and keeping the non-technical nature of domain experts in mind, we also contribute a custom, web-based GUI annotation tool and a dashboard-style analytics portal. Overall, our contributions set the stage for enabling downstream applications such as OCR and word-spotting in historical Indic manuscripts at scale. © 2019 IEEE.},
	language = {English},
	urldate = {2019-09-20},
	booktitle = {Proc. {Int}. {Conf}. {Doc}. {Anal}. {Recognit}.},
	publisher = {IEEE Computer Society},
	author = {Prusty, A. and Aitha, S. and Trivedi, A. and Sarvadevabhatla, R.K.},
	collaborator = {{ACS; Adobe; et al.; Hitachi; Wacom; Wiris}},
	year = {2019},
	note = {Journal Abbreviation: Proc. Int. Conf. Doc. Anal. Recognit.},
	keywords = {Convolutional neural networks, Cultural heritages, Deep Networks, Deep neural networks, Document Layout Parsing, Document layouts, Downstream applications, HEMDIG - SCOPUS, Indian subcontinents, Indic, Large dataset, Multiple documents, Network architecture, Palm-leaf manuscripts, Proposed architectures, SCOPUS, Semantic Instance Segmentation, Semantics},
	pages = {999--1006},
}

@article{ma_segmentation_2020,
	title = {Segmentation and {Recognition} for {Historical} {Tibetan} {Document} {Images}},
	volume = {8},
	issn = {21693536 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082514305&doi=10.1109%2fACCESS.2020.2975023&partnerID=40&md5=e368df6705e144eff817b559dae4c422},
	doi = {10.1109/ACCESS.2020.2975023},
	abstract = {As a shining pearl in traditional Tibetan culture, historical Tibetan documents have received extensive attention from historians, linguists and Buddhist scholars. These documents are converted into digital form using Tibetan document segmentation and recognition methods. The document digitization is of great significance for the research, protection and inheritance of Tibetan history. This paper proposes an overall segmentation and recognition framework for historical Tibetan document images. Firstly, the historical Tibetan document image is preprocessed to correct imbalanced illumination, tilt and noises, and is further transformed into the binarized image. Secondly, we propose a layout segmentation method based on block projection to segment Tibetan document images into texts, lines and frames. Thirdly, in order to solve the problems of touching strokes between text-lines and curvilinear text-lines, we present a text-line segmentation method based on graph model for historical Tibetan text-line segmentation. Lastly, we present a touching segmentation method to segment touching Tibetan character string, and then recognize Tibetan characters. Experimental results show our proposed methods on layout segmentation, text-line segmentation and touching character string segmentation, achieve the satisfactory performance. The proposed methods can also be applied to other fonts in Tibetan font family. © 2013 IEEE.},
	language = {English},
	journal = {IEEE Access},
	author = {Ma, L. and Long, C. and Duan, L. and Zhang, X. and Li, Y. and Zhao, Q.},
	year = {2020},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {block projection, Character recognition, Character strings, Document segmentation, HEMDIG - SCOPUS, Historical Tibetan document, History, Image segmentation, layout segmentation, Recognition methods, SCOPUS, Segmentation methods, Text line segmentation, text-line segmentation, Tibetans, Touching character, touching character string segmentation},
	pages = {52641--52651},
}

@book{aveiro_d_8th_2019,
	series = {8th {International} {Joint} {Conference} on {Knowledge} {Discovery}, {Knowledge} {Engineering} and {Knowledge} {Management}, {IC3K} 2016},
	title = {8th {International} {Joint} {Conference} on {Knowledge} {Discovery}, {Knowledge} {Engineering} and {Knowledge} {Management}, {IC3K} 2016},
	volume = {914},
	isbn = {18650929 (ISSN); 9783319997001 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057195814&partnerID=40&md5=7b4f23d880950c8fd5d5c82b263dbd2b},
	abstract = {The proceedings contain 18 papers. The special focus in this conference is on Knowledge Discovery, Knowledge Engineering and Knowledge Management. The topics include: Conceptual software design: Modularity matrix as source of conceptual integrity; Extraction of patterns using NLP: US and European patents domain; cloud-based management of machine learning generated knowledge for fleet data refinement; knowledge management in enterprise architecture projects; Empowering SMEs to make better decisions with business intelligence: A case study; congestion control supported dual-mode video transfer; reframing coordination in knowledge transfer: A sociomaterial perspective; records systems and information systems: Connecting in organizations; exercises in unstyling texts: Formalisation and visualisation of a narrative’s [space, time, actors, motion] components; multi-layer and Co-learning Systems for semantic textual similarity, semantic relatedness and recognizing textual entailment; cell Classification for layout recognition in spreadsheets; computing data lineage and business semantics for data warehouse; introducing a vector space model to perform a proactive credit scoring; text mining for word sentiment detection; a smart system for haptic quality control: A knowledge-based approach to formalize the sense of touch; the mereologies of upper ontologies.},
	language = {English},
	urldate = {2016-11-09},
	publisher = {Springer Verlag},
	editor = {{Aveiro D.} and {Fred A.} and {Dietz J.} and {Bernardino J.} and {Liu K.} and {Filipe J.}},
	year = {2019},
	note = {Journal Abbreviation: Commun. Comput. Info. Sci.
Publication Title: Commun. Comput. Info. Sci.},
	keywords = {HEMDIG - SCOPUS, SCOPUS},
}

@book{koci_cell_2019,
	series = {8th {International} {Joint} {Conference} on {Knowledge} {Discovery}, {Knowledge} {Engineering} and {Knowledge} {Management}, {IC3K} 2016},
	title = {Cell {Classification} for layout recognition in spreadsheets},
	volume = {914},
	isbn = {18650929 (ISSN); 9783319997001 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057205078&doi=10.1007%2f978-3-319-99701-8_4&partnerID=40&md5=742146381afb46a2069453c604b40d31},
	abstract = {Spreadsheets compose a notably large and valuable dataset of documents within the enterprise settings and on the Web. Although spreadsheets are intuitive to use and equipped with powerful functionalities, extracting and reusing data from them remains a cumbersome and mostly manual task. Their greatest strength, the large degree of freedom they provide to the user, is at the same time also their greatest weakness, since data can be arbitrarily structured. Therefore, in this paper we propose a supervised learning approach for layout recognition in spreadsheets. We work on the cell level, aiming at predicting their correct layout role, out of five predefined alternatives. For this task we have considered a large number of features not covered before by related work. Moreover, we gather a considerably large dataset of annotated cells, from spreadsheets exhibiting variability in format and content. Our experiments, with five different classification algorithms, show that we can predict cell layout roles with high accuracy. Subsequently, in this paper we focus on revising the classification results, with the aim of repairing misclassifications. We propose a sophisticated approach, composed of three steps, which effectively corrects a reasonable number of inaccurate predictions. © Springer Nature Switzerland AG 2019.},
	language = {English},
	urldate = {2016-11-09},
	publisher = {Springer Verlag},
	author = {Koci, E. and Thiele, M. and Romero, O. and Lehner, W.},
	editor = {{Aveiro D.} and {Fred A.} and {Dietz J.} and {Bernardino J.} and {Liu K.} and {Filipe J.}},
	year = {2019},
	doi = {10.1007/978-3-319-99701-8_4},
	note = {Journal Abbreviation: Commun. Comput. Info. Sci.
Pages: 100
Publication Title: Commun. Comput. Info. Sci.},
	keywords = {Analysis, Cells, Classification, Classification (of information), Cytology, Data mining, Degrees of freedom (mechanics), Document, Forecasting, HEMDIG - SCOPUS, Information retrieval systems, Knowledge engineering, Knowledge management, Layout, Recognition, SCOPUS, Speadsheet, Spreadsheets, Table, Tabular},
}

@inproceedings{kiessling_escriptorium_2019,
	title = {{EScriptorium}: {An} open source platform for historical document analysis},
	volume = {2019-January},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097789248&doi=10.1109%2fICDARW.2019.10032&partnerID=40&md5=7e977ada73d514fca0a6a3d9534c8eb3},
	doi = {10.1109/ICDARW.2019.10032},
	abstract = {We describe the new open source document analysis and annotation platform eScriptorium. It allows to upload document collections, transcribe and segment them manually or automatically with the help of the kraken OCR engine. © 2019 IEEE.},
	language = {English},
	urldate = {2019-09-21},
	booktitle = {Int. {Conf}. {Doc}. {Anal}. {Recog}. {Workshops}, {ICDARW}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Kiessling, B. and Tissot, R. and Stokes, P. and Ezra, D.S.B.},
	year = {2019},
	note = {Journal Abbreviation: Int. Conf. Doc. Anal. Recog. Workshops, ICDARW},
	keywords = {Document analysis, Document collection, GUI, Handwritten text recognition, HEMDIG - SCOPUS, Historical documents, Layout segmentation, OCR engines, Open source platforms, Open sources, SCOPUS},
	pages = {19--24},
}

@inproceedings{trivedi_hindola_2019,
	title = {Hindola: {A} unified cloud-based platform for annotation, visualization and machine learning-based layout analysis of historical manuscripts},
	volume = {2019-January},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114243327&doi=10.1109%2fICDARW.2019.10035&partnerID=40&md5=6c28f20834d46bafdab474998a0ddb03},
	doi = {10.1109/ICDARW.2019.10035},
	abstract = {Palm-leaf manuscripts are one of the oldest medium of inscription in many Asian countries. Especially, manuscripts from the Indian subcontinent form an important part of the world's literary and cultural heritage. Despite their significance, large-scale datasets for layout parsing and targeted annotation systems do not exist. Addressing this, we propose a web-based layout annotation and analytics system. Our system, called HInDoLA, features an intuitive annotation GUI, a graphical analytics dashboard and interfaces with machine-learning based intelligent modules on the backend. HInDoLA has successfully helped us create the first ever large-scale dataset for layout parsing of Indic palm-leaf manuscripts. These manuscripts, in turn, have been used to train and deploy deep-learning based modules for fully automatic and semi-automatic instance-level layout parsing. © 2019 IEEE.},
	language = {English},
	urldate = {2019-09-21},
	booktitle = {Int. {Conf}. {Doc}. {Anal}. {Recog}. {Workshops}, {ICDARW}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Trivedi, A. and Sarvadevabhatla, R.K.},
	year = {2019},
	note = {Journal Abbreviation: Int. Conf. Doc. Anal. Recog. Workshops, ICDARW},
	keywords = {Analytics systems, Annotation systems, Cloud based platforms, Cultural heritages, Deep learning, HEMDIG - SCOPUS, Historic preservation, Indian subcontinents, Intelligent modules, Large dataset, Large-scale dataset, Large-scale datasets, Learning systems, SCOPUS},
	pages = {31--35},
}

@article{parinov_layout_2020,
	title = {Layout logical labelling and finding the semantic relationships between citing and cited paper content},
	volume = {14},
	issn = {17442621 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087985924&doi=10.1504%2fIJMSO.2020.107796&partnerID=40&md5=b4348a7fe22a4483604225e0f65fc20f},
	doi = {10.1504/IJMSO.2020.107796},
	abstract = {Currently, large data sets of in-text citations and citation contexts are becoming available for research and developing tools. Using the “topic model” method to analyse these data, one can characterise thematic relationships between citation contexts from citing and the cited paper content. However, to build relevant topic models and to compare them accurately for papers linked by citation relationships we have to know the semantic labels of PDF papers' layout such as section titles, paragraph boundaries, etc. Recent achievements in papers' conversion from a PDF form into a rich attributed JSON format allow us to develop new approaches for the logical labelling of the papers' layout. This paper presents a re-usable method and open source software for the logical labelling of PDF papers, which gave good quality of a layout element's recognition for a set of research papers. Using these semantic labels we made a precise comparison of topic models built for citing and cited papers and we found some level of similarity between them. Copyright © 2020 Inderscience Enterprises Ltd.},
	language = {English},
	number = {1},
	journal = {International Journal of Metadata, Semantics and Ontologies},
	author = {Parinov, S. and Bakarov, A. and Vodolazsky, D.},
	year = {2020},
	note = {Publisher: Inderscience Publishers},
	keywords = {Cirtec project, Citation contexts, Computer software reusability, HEMDIG - SCOPUS, Hierarchical topic models, In-text citation, Large datasets, Logical labelling, New approaches, Open source software, Open systems, Research paper layout recognition, Research papers, SCOPUS, Semantic labels, Semantic relationships, Semantics, Topic Modeling},
	pages = {54--62},
}

@inproceedings{he_residential_2020,
	title = {Residential floor plan recognition and information extraction based on image segmentation},
	isbn = {9781713823605 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101607816&partnerID=40&md5=3d6dbe66cee3c21c0d27792bb36564b1},
	abstract = {Layout design of residential building have deep influence on indoor environment and performance. Practically floor plan images are widely used and easy to access compared with original design models, and enable learning from data that may provide a new perspective for understanding the effect of layout on building performance and make design more scientific and objective. This research focuses on automated recognition of residential floor plan images for further performance analysis and optimization of layout. An algorithm for floor plan image recognition is proposed based on Optical Character Recognition (OCR) and Line Seeded Region Growing (LSRG) and tested with cases of real residential projects in China. Geographic information of rooms and openings are extracted and described in vector, based on which quantitative indicators of building environment and performance such as natural ventilation and daylight could be calculated for evaluation and optimization in future work. © 2020 16th Conference of the International Society of Indoor Air Quality and Climate: Creative and Smart Solutions for Better Built Environments, Indoor Air 2020. All rights reserved.},
	language = {English},
	urldate = {2020-11-01},
	booktitle = {Conf. {Int}. {Soc}. {Indoor} {Air} {Qual}. {Clim}.: {Creat}. {Smart} {Solut}. {Better} {Built} {Environ}., {Indoor} {Air}},
	publisher = {International Society of Indoor Air Quality and Climate},
	author = {He, Q. and Chen, H. and Lin, B.},
	year = {2020},
	note = {Journal Abbreviation: Conf. Int. Soc. Indoor Air Qual. Clim.: Creat. Smart Solut. Better Built Environ., Indoor Air},
	keywords = {Air quality, Architectural design, Automated recognition, Building performance, Floors, Geographic information, HEMDIG - SCOPUS, Housing, Image recognition, Image segmentation, Indoor air pollution, Layout recognition, Optical character recognition, Optical character recognition (OCR), Performance analysis and optimizations, Quantitative indicators, Residential building, SCOPUS, Seeded region growing, Structural design, Ventilation},
}

@article{zhang_pattern_2020,
	title = {Pattern understanding and synthesis based on layout tree descriptor},
	volume = {36},
	issn = {01782789 (ISSN)},
	doi = {10.1007/s00371-019-01723-5},
	abstract = {Synthesis from existing examples is a promising way to generate new patterns. However, pattern synthesis is challenging because it is difficult to understand and generate complex structures in patterns. In this paper, we propose an approach based on the layout tree descriptor (LTD) to understand and synthesize patterns from existing ones. The LTD is a binary tree that parametrically describes all primitives, layouts, their dependencies and hierarchies in a pattern. The LTD can be constructed automatically with proposed instance grouping, layout recognition, hyper-primitive matching and tree merging algorithms to realize pattern understanding. To meet specialists’ requirements for detailed modification and recombination of patterns, we designed LTD operations including add, remove, replace and grafting operations to allow users to get new patterns by simply adjusting the LTDs. For stylized synthesis, we gave the computing method of LTD similarity. Therefore, the styles of results and input can be compared and users can control generated serialized results by setting the input pattern weights. To meet user’s implicit preferences and provide novelty in creative design, we propose an evolutionary approach to creative synthesis. The system generates new patterns continuously based on LTD grafting, meanwhile user selection of preferred patterns will guide the direction of evolution. Experiments using the developed prototype system show that our approach can synthesize novel and complex patterns effectively, meeting different requirements in practice and providing plenty of digital textures for products. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.},
	language = {English},
	number = {6},
	journal = {Visual Computer},
	author = {Zhang, X. and Wang, J. and Lu, G. and Zhang, X.},
	year = {2020},
	note = {Publisher: Springer},
	keywords = {Binary trees, Design space exploration, Design tool, Design tools, GraphicaL model, Graphical models, HEMDIG - SCOPUS, Layouts, Patterns, SCOPUS, Synthesis, Synthesis (chemical), Textures},
	pages = {1141--1155},
}

@article{qiao_design_2020,
	title = {Design and verification of dual modular redundancy hamming code},
	volume = {52},
	issn = {03676234 (ISSN)},
	shorttitle = {双模冗余汉明码的设计与验证},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092261330&doi=10.11918%2f202001018&partnerID=40&md5=9724ecb0ed322354f1c589e1a00df01a},
	doi = {10.11918/202001018},
	abstract = {Integrated circuit chips work in complicated electromagnetic environments, which is susceptible to soft errors caused by high-energy particles. In a chip, the memory accounts for more than half of the total area, making it important to improve the reliability of the processor by hardening the memory. Therefore, a dual modular redundancy hamming (DMRH) code is designed and proposed in this paper, which can mitigate one-bit and two-bit upset in memory. First, logic optimization was carried out in the hamming code encoder to reduce the delay of the circuit, and the parity generated by this module was processed with dual modular redundancy technology, which was used as the output of the DMRH encoder. Then, the combinations of each parity and original code were processed according to the hamming decoding rules, and the revised data and two-bit upset flag were obtained. Through analysis, it was found that when the two-bit upset did not happen in the original code at the same time, the correct output could be obtained according to the two-bit upset flag. Finally, the layout segmentation technology was used to suppress the two-bit upset in the original code, which further improved the reliability of the memory. In this study, three types of DMRH codes with word lengths of 4, 8, and 11 were realized. Compared with other correction codes, results show that the circuit delay of the codes obtained in this study was 85\%, 89\%, and 96\% of the eight-bit hamming code, which was lower than the BCH codes. Copyright ©2020 Journal of Harbin Institute of Technology.All rights reserved.},
	language = {Chinese},
	number = {10},
	journal = {Harbin Gongye Daxue Xuebao/Journal of Harbin Institute of Technology},
	author = {Qiao, B. and Wu, X. and Liu, H. and Wang, Z. and Dong, Y.},
	year = {2020},
	note = {Publisher: Harbin Institute of Technology},
	keywords = {Block codes, Circuit delays, Correction codes, Delay circuits, Dual modular redundancy, Electromagnetic environments, Error correction code, Hamming code, HEMDIG - SCOPUS, High reliability, High-energy particles, Integrated circuit chips, Integrated circuit design, Logic optimization, Logic Synthesis, Memory, Radiation hardening, Redundancy, SCOPUS, Signal encoding, Two-bit upset},
	pages = {161--166},
}

@article{posch_lima_2020,
	title = {Lima or cima? {Structure} recognition and {OCR} in building the corpus of the {Austrian} {Alpine} {Club} {Journal}},
	volume = {25},
	issn = {13846655 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097218873&doi=10.1075%2fijcl.19094.pos&partnerID=40&md5=e6d481b80d05c74c0a56e7a4d5e35f87},
	doi = {10.1075/ijcl.19094.pos},
	abstract = {This paper outlines the construction of the corpus Alpenwort, a large, genre-based corpus of German texts on alpinism. We report on issues related to building the corpus from the Austrian Alpine Club Journal (1869–2010). First, a general description of our data and the project phases from digitization and annotation to publication is given. We focus on the most interesting challenges that the diverse layouts and the extensive use of Fraktur typefacing posed for optical layout recognition and optical character recognition (OCR) as well as post correction. The corrected data was lemmatized and annotated with part-of-speech information including named entities as well as TEI-conformant metadata. The resulting 19.9-million-word corpus is designed to be queried using CQPweb and Hyperbase and can be accessed freely online. Lastly, we give a short roadmap of current and future expansions and improvements as corpus data has been and is being enhanced in follow-up projects. © John Benjamins Publishing Company},
	language = {English},
	number = {4},
	journal = {International Journal of Corpus Linguistics},
	author = {Posch, C. and Rampl, G.},
	year = {2020},
	note = {Publisher: John Benjamins Publishing Company},
	keywords = {Alpinism, Document structure recognition, German Fraktur typeface, HEMDIG - SCOPUS, OCR, SCOPUS, Specialized corpora},
	pages = {489--503},
}

@book{sharan_palmira_2021,
	series = {16th {International} {Conference} on {Document} {Analysis} and {Recognition}, {ICDAR} 2021},
	title = {Palmira: {A} {Deep} {Deformable} {Network} for {Instance} {Segmentation} of {Dense} and {Uneven} {Layouts} in {Handwritten} {Manuscripts}},
	volume = {12822 LNCS},
	isbn = {03029743 (ISSN); 9783030863302 (ISBN)},
	abstract = {Handwritten documents are often characterized by dense and uneven layout. Despite advances, standard deep network based approaches for semantic layout segmentation are not robust to complex deformations seen across semantic regions. This phenomenon is especially pronounced for the low-resource Indic palm-leaf manuscript domain. To address the issue, we first introduce Indiscapes2, a new large-scale diverse dataset of Indic manuscripts with semantic layout annotations. Indiscapes2 contains documents from four different historical collections and is 150 \% larger than its predecessor, Indiscapes. We also propose a novel deep network Palmira for robust, deformation-aware instance segmentation of regions in handwritten manuscripts. We also report Hausdorff distance and its variants as a boundary-aware performance measure. Our experiments demonstrate that Palmira provides robust layouts, outperforms strong baseline approaches and ablative variants. We also include qualitative results on Arabic, South-East Asian and Hebrew historical manuscripts to showcase the generalization capability of Palmira. © 2021, Springer Nature Switzerland AG.},
	language = {English},
	urldate = {2021-09-05},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Sharan, S.P. and Aitha, S. and Kumar, A. and Trivedi, A. and Augustine, A. and Sarvadevabhatla, R.K.},
	editor = {{Llados J.} and {Lopresti D.} and {Uchida S.}},
	year = {2021},
	doi = {10.1007/978-3-030-86331-9_31},
	note = {Journal Abbreviation: Lect. Notes Comput. Sci.
Pages: 491
Publication Title: Lect. Notes Comput. Sci.},
	keywords = {Complex deformation, Computer vision, Convolutional networks, Dataset, Deformable convolutional network, Deformation, Document image segmentation, Documents analysis, Handwritten document, HEMDIG - SCOPUS, Historical document analyse, Historical document analysis, Historical documents, History, Image segmentation, Instance segmentation, Instance Segmentation, Large dataset, Network-based approach, SCOPUS, Semantics},
}

@book{lehenmeier_layout_2020,
	series = {24th {International} {Conference} on {Theory} and {Practice} of {Digital} {Libraries}, {TPDL} 2020},
	title = {Layout {Detection} and {Table} {Recognition} – {Recent} {Challenges} in {Digitizing} {Historical} {Documents} and {Handwritten} {Tabular} {Data}},
	volume = {12246 LNCS},
	isbn = {03029743 (ISSN); 9783030549558 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090095153&doi=10.1007%2f978-3-030-54956-5_17&partnerID=40&md5=63ae02b1ca2e99bea2680c18daa7ed82},
	abstract = {In this paper, we discuss the computer-aided processing of handwritten tabular records of historical weather data. The observationes meteorologicae, which are housed by the Regensburg University Library, are one of the oldest collections of weather data in Europe. Starting in 1771, meteorological data was consistently documented in a standardized form over almost 60 years by several writers. The tabular structure, as well as the unconstrained textual layout of comments and the use of historical characters, propose various challenges in layout and text recognition. We present a customized strategy to digitize tabular and handwritten data by combining various state-of-the-art methods for OCR processing to fit the collection. Since the recognition of historical documents still poses major challenges, we provide lessons learned from experimental testing during the first project stages. Our results show that deep learning methods can be used for text recognition and layout detection. However, they are less efficient for the recognition of tabular structures. Furthermore, a tailored approach had to be developed for the historical meteorological characters during the manual creation of ground truth data. The customized system achieved an accuracy rate of 82\% for the text recognition of the heterogeneous handwriting and 87\% accuracy for layout recognition of the tables. © 2020, Springer Nature Switzerland AG.},
	language = {English},
	urldate = {2020-08-25},
	publisher = {Springer},
	author = {Lehenmeier, C. and Burghardt, M. and Mischka, B.},
	editor = {{Hall M.} and {Mercun T.} and {Risse T.} and {Duchateau F.}},
	year = {2020},
	doi = {10.1007/978-3-030-54956-5_17},
	note = {Journal Abbreviation: Lect. Notes Comput. Sci.
Pages: 242
Publication Title: Lect. Notes Comput. Sci.},
	keywords = {Deep learning, Digital libraries, Document recognition, Experimental testing, Ground truth data, Handwritten text recognition, HEMDIG - SCOPUS, Historical characters, Historical documents, Historical weather datum, History, Learning systems, Meteorological data, Meteorology, Optical character recognition, SCOPUS, State-of-the-art methods, Table recognition, University libraries},
}

@article{kim_modular_2020,
	title = {A modular factory testbed for the rapid reconfiguration of manufacturing systems},
	volume = {31},
	issn = {09565515 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064196111&doi=10.1007%2fs10845-019-01471-2&partnerID=40&md5=f6c59cffae8b6f225bd3e30900a2ac67},
	doi = {10.1007/s10845-019-01471-2},
	abstract = {The recent manufacturing trend toward mass customization and further personalization of products requires factories to be smarter than ever before in order to: (1) quickly respond to customer requirements, (2) resiliently retool machinery and adjust operational parameters for unforeseen system failures and product quality problems, and (3) retrofit old systems with upcoming new technologies. Furthermore, product lifecycles are becoming shorter due to unbounded and unpredictable customer requirements, thereby requiring reconfigurable and versatile manufacturing systems that underpin the basic building blocks of smart factories. This study introduces a modular factory testbed, emphasizing transformability and modularity under a distributed shop-floor control architecture. The main technologies and methods, being developed and verified through the testbed, are presented from the four aspects of rapid factory transformation: self-layout recognition, rapid workstation and robot reprogramming, inter-layer information sharing, and configurable software for shop-floor monitoring. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.},
	language = {English},
	number = {3},
	journal = {Journal of Intelligent Manufacturing},
	author = {Kim, D.-Y. and Park, J.-W. and Baek, S. and Park, K.-B. and Kim, H.-R. and Park, J.-I. and Kim, H.-S. and Kim, B.-B. and Oh, H.-Y. and Namgung, K. and Baek, W.},
	year = {2020},
	note = {Publisher: Springer},
	keywords = {Basic building block, Closed loop control systems, Customer requirements, Distributed control, Distributed parameter control systems, Floors, HEMDIG - SCOPUS, Information sharing, Life cycle, Machinery, Mass customization, Operational parameters, Product life cycles, Quality control, Reconfigurable, SCOPUS, Smart factory, Systems engineering, Testbed, Testbeds},
	pages = {661--680},
}

@article{liu_scut-autoalp_2020,
	title = {{SCUT}-{AutoALP}: {A} diverse benchmark dataset for automatic architectural layout parsing},
	volume = {E103D},
	issn = {09168532 (ISSN)},
	doi = {10.1587/transinf.2020EDL8076},
	abstract = {Computer aided design (CAD) technology is widely used for architectural design, but current CAD tools still require high-level design specifications from human. It would be significant to construct an intelligent CAD system allowing automatic architectural layout parsing (AutoALP), which generates candidate designs or predicts architectural attributes without much user intervention. To tackle these problems, many learning-based methods were proposed, and benchmark dataset become one of the essential elements for the data-driven AutoALP. This paper proposes a new dataset called SCUT-AutoALP for multi-paradigm applications. It contains two subsets: 1) Subset-I is for floor plan design containing 300 residential floor plan images with layout, boundary and attribute labels; 2) Subset-II is for urban plan design containing 302 campus plan images with layout, boundary and attribute labels. We analyzed the samples and labels statistically, and evaluated SCUT-AutoALP for different layout parsing tasks of floor plan/urban plan based on conditional generative adversarial networks (cGAN) models. The results verify the effectiveness and indicate the potential applications of SCUT-AutoALP. The dataset is available at https://github.com/designfuturelab702/SCUT-AutoALP-DatabaseRelease. © 2020 The Institute of Electronics, Information and Communication Engineers},
	language = {English},
	number = {12},
	journal = {IEICE Transactions on Information and Systems},
	author = {Liu, Y. and Lai, Y. and Chen, J. and Liang, L. and Deng, Q.},
	year = {2020},
	note = {Publisher: Institute of Electronics, Information and Communication, Engineers, IEICE},
	keywords = {Adversarial networks, Architectural attributes, Benchmark datasets, Computer aided design, Essential elements, Floor plan, Floor plan designs, Floors, GAN, HEMDIG - SCOPUS, High-level design, Image parsing, Learning-based methods, SCOPUS, Urban plan, User intervention},
	pages = {2725--2729},
}

@article{deng_design_2020,
	title = {The design of tourism product {CAD} threedimensional modeling system using {VR} technology},
	volume = {15},
	issn = {19326203 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098936481&doi=10.1371%2fjournal.pone.0244205&partnerID=40&md5=95aba59d73e143e3fbfac90b280e2958},
	doi = {10.1371/journal.pone.0244205},
	abstract = {In view of the high homogeneity of tourism products all over the country, an attempt is made to design virtual visit tourism products with cultural experience background, which can reflect the characteristics of culture + tourism in different scenic spots, so that tourists can deeply experience the local culture. Combined with computer aided design (CAD), the virtual three-dimensional (3D) modeling system of scenic spots is designed, and VR real scene visit interactive tourism products suitable for different scenic spots are designed. 360° VR panoramic display technology is used for 360° VR panoramic video shooting and visiting system display production of Elephant Trunk Hill park scenery. A total of 157 images are collected and 720 cloud panoramic interactive H5 tool is selected to produce a display system suitable for 360° VR panoramic display of scenic spots. Meanwhile, based on single view RGB-D image, the latest convolutional neural network (CNN) algorithm and point cloud processing algorithm are used to design the indoor 3D scene reconstruction algorithm based on semantic understanding. Experiments show that the pixel accuracy and mean intersection over union of the indoor scene layout segmentation network segmentation results are 89.5\% and 60.9\%, respectively, that is, it has high accuracy. The VR real scene visit interactive tourism product can make tourists have a more immersive sense of interaction and experience before, during and after the tour. © 2020 Deng et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
	language = {English},
	number = {12 December},
	journal = {PLoS ONE},
	author = {Deng, Y. and Han, S.-Y. and Li, J. and Rong, J. and Fan, W. and Sun, T.},
	year = {2020},
	note = {Publisher: Public Library of Science},
	keywords = {Article, cloud computing, computer aided design, Computer-Aided Design, controlled study, convolutional neural network, economics, HEMDIG - SCOPUS, human, Humans, image display, marketing, Marketing, Neural Networks, Computer, procedures, product design, reconstruction algorithm, SCOPUS, segmentation algorithm, semantics, technology, three dimensional computer aided design, tourism, Tourism, virtual reality, Virtual Reality},
}

@article{chen_layout_2021,
	title = {Layout segmentation and description of tibetan document images based on adaptive run length smoothing algorithm},
	volume = {58},
	issn = {10064125 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115738399&doi=10.3788%2fLOP202158.1410006&partnerID=40&md5=112611898dc890dde08e23cf3a0bb56f},
	doi = {10.3788/LOP202158.1410006},
	abstract = {Layout segmentation is an important basic step in the process of document image analysis and recognition In order to explore a suitable method for layout segmentation and description of Tibetan document images a research method based on the adaptive run length smoothing algorithm is proposed Firstly according to the layout structure of Tibetan document images K-means clustering analysis is used to get the run length threshold suitable for the layout smooth the run length find the connected component and realize the layout segmentation Then according to the external contour characteristics of each layout element the text area and nontext area are simply distinguished Finally the text area is recognized by a Tibetan text recognizer and then the extensible markup language is used to record layout information and realize layout description Experiments on Tibetan primary and secondary school teaching materials and stereotyped Tibetan document images show that this method can achieve good layout analysis results. © 2021 Universitat zu Koln. All rights reserved.},
	language = {Chinese},
	number = {14},
	journal = {Laser and Optoelectronics Progress},
	author = {Chen, Y. and Wang, W. and Liu, H. and Cai, Z. and Zhao, P.},
	year = {2021},
	note = {Publisher: Universitat zu Koln},
	keywords = {Adaptive run length smoothin, HEMDIG - SCOPUS, Image processing, Layout description, Layout segmentation, SCOPUS, Tibetan document image},
}

@inproceedings{jang_large_2021,
	title = {Large area metalens for compact virtual reality optical systems},
	isbn = {9781557528209 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119367395&partnerID=40&md5=bf0d69d5d325af4b4b630538a1a46645},
	abstract = {We experimentally demonstrate compact optical imaging systems for virtual reality with wide field of view by using the metalens with centimeter-scale diameter. We present a method of layout segmentation for fabricating the large area metalens. © OSA 2021, © 2021 The Author(s)},
	language = {English},
	urldate = {2021-07-19},
	booktitle = {Opt. {InfoBase} {Conf}. {Pap}},
	publisher = {The Optical Society},
	author = {Jang, J. and Kim, K. and Kim, C. and Lee, B.},
	year = {2021},
	note = {Journal Abbreviation: Opt. InfoBase Conf. Pap},
	keywords = {Centimeter-scale, HEMDIG - SCOPUS, Imaging systems, MetaLens, Optical imaging system, Optical systems, SCOPUS, Virtual reality, Wide field-ofview},
}

@inproceedings{manandhar_magic_2021,
	title = {Magic layouts: {Structural} prior for component detection in user interface designs},
	isbn = {10636919 (ISSN); 9781665445092 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123216048&doi=10.1109%2fCVPR46437.2021.01555&partnerID=40&md5=bf5f35aa4d05861a7f71a89956ea30f5},
	doi = {10.1109/CVPR46437.2021.01555},
	abstract = {We present Magic Layouts; a method for parsing screenshots or hand-drawn sketches of user interface (UI) layouts. Our core contribution is to extend existing detectors to exploit a learned structural prior for UI designs, enabling robust detection of UI components; buttons, text boxes and similar. Specifically we learn a prior over mobile UI layouts, encoding common spatial co-occurrence relationships between different UI components. Conditioning region proposals using this prior leads to performance gains on UI layout parsing for both hand-drawn UIs and app screenshots, which we demonstrate within the context an interactive application for rapidly acquiring digital prototypes of user experience (UX) designs. © 2021 IEEE},
	language = {English},
	urldate = {2021-06-19},
	booktitle = {Proc {IEEE} {Comput} {Soc} {Conf} {Comput} {Vision} {Pattern} {Recognit}},
	publisher = {IEEE Computer Society},
	author = {Manandhar, D. and Jin, H. and Collomosse, J.},
	year = {2021},
	note = {Journal Abbreviation: Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
	keywords = {Co-occurrence relationships, Component detection, Digital devices, Drawing (graphics), Hand-drawn sketches, HEMDIG - SCOPUS, Learn+, Mobile user interface, Palmprint recognition, Robust detection, SCOPUS, Screenshots, User interface components, User interface designs, User interface layouts, User interfaces},
	pages = {15804--15813},
}

@inproceedings{zhang_room_2021,
	title = {Room {Layout} {Estimation} by {Learning} {Depth} {Maps} of {Planes} from {2D} {Layout} {Labels}},
	isbn = {9781665431538 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124793010&doi=10.1109%2fM2VIP49856.2021.9665046&partnerID=40&md5=ec6b34c581560641ff01edfde08c8012},
	doi = {10.1109/M2VIP49856.2021.9665046},
	abstract = {Room layout estimation is a basic mission in understanding the indoor scene and has a wide range of applications. Specifically, the task of room layout estimation is to segment an indoor RGB image with semantic surface labels, i.e., ceiling, floor, and walls. A straightforward solution for the problem is to estimate the geometry of the dominant indoor planes. However, since the ground truth depth maps of the layout is not easy to obtain, research in this direction is rare. In this paper, we focus on the cuboid rooms and propose an effective learning framework that can learn the depth maps of planes from 2D layout labels without ground truth depth maps. We employ a deep network to learn the surface parameters, which can be used to produce depth maps of planes and layout results. Then we propose stereo supervision mechanism that encourages the generated layout to be consistent with the ground truth layout segmentation along Z axis and layout edge on the image plane simultaneously, so that the learned surface parameters and layout results are reasonable. Experimental results show that our method can produce high-quality layout estimates and corresponding depth maps on benchmark datasets. © 2021 IEEE.},
	language = {English},
	urldate = {2021-11-26},
	booktitle = {Int. {Conf}. {Mechatronics} {Mach}. {Vis}. {Pract}., {M2VIP}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Zhang, W. and Liu, Y.},
	year = {2021},
	note = {Journal Abbreviation: Int. Conf. Mechatronics Mach. Vis. Pract., M2VIP},
	keywords = {Benchmarking, Computer vision, Depthmap, Effective learning, Ground truth, HEMDIG - SCOPUS, High quality, Image plane, Learn+, Learning frameworks, RGB images, SCOPUS, Semantic Segmentation, Semantics, Stereo image processing, Supervision mechanisms, Surface parameter},
	pages = {777--782},
}

@inproceedings{burgl_digitizing_2021,
	title = {Digitizing {Drilling} {Logs} - {Challenges} of typewritten forms},
	volume = {P-314},
	isbn = {16175468 (ISSN); 9783885797081 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127374207&partnerID=40&md5=ea10b9649aa0f328f0d62135447e4b2a},
	abstract = {In this work, we show prospects of how mining and geological documentation in the form of drilling reports can be digitized and further processed. Processing these typed and handwritten forms poses challenges for document management in renaturation projects. We highlight the structural problems of drilling reports and present three approaches for recognizing and processing the information documented in them. We use optical character recognition and document layout analysis techniques to approach the problem. Layout analysis was performed using a heuristic approach and a neural network for layout recognition. In detail, we show the approaches Form Processing (A), Table detection by line counting (B) and processing with Mask-R-CNN (C). A case study is used to show initial results and challenges. B and C are more robust than A to small changes in the form. C can recognize columns better with more training data than B in cases where table boundaries are not respected. B and C also allow other language models to be used for OCR and can thus also recognize handwriting with appropriate training data. © 2021 Gesellschaft fur Informatik (GI). All rights reserved.},
	language = {English},
	urldate = {2021-09-27},
	booktitle = {Lect. {Notes} {Informatics} ({LNI}), {Proc}. - {Series} {Ges}. {Inform}. ({GI})},
	publisher = {Gesellschaft fur Informatik (GI)},
	author = {Bürgl, K. and Reinhardt, L. and Binder, F. and Müller, L. and Niekler, A.},
	collaborator = {et al.; Google Cloud; Google Deutschland GmbH; Huawei Technologies Co., Ltd.; IBM Deutschland GmbH; SAP SE},
	year = {2021},
	note = {Journal Abbreviation: Lect. Notes Informatics (LNI), Proc. - Series Ges. Inform. (GI)},
	keywords = {Analysis techniques, C (programming language), Document layout analysis, Document management, Drilling log, drilling logs, Form processing, forms processing, HEMDIG - SCOPUS, Heuristic methods, Infill drilling, information extraction, Information retrieval, Information services, OCR, Optical character recognition, Renaturation, Renaturation project, renaturation projects, SCOPUS, Structural problems, table recognition, Table recognition, Training data},
	pages = {709--718},
}

@inproceedings{mostafa_ocformer_2021,
	title = {{OCFormer}: {A} {Transformer}-{Based} {Model} for {Arabic} {Handwritten} {Text} {Recognition}},
	isbn = {9781665412438 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111397951&doi=10.1109%2fMIUCC52538.2021.9447608&partnerID=40&md5=05deb94dcb721d55903acb7452cd0bf7},
	doi = {10.1109/MIUCC52538.2021.9447608},
	abstract = {The Optical Character Recognition (OCR) of Arabic historical documents is a challenging task. The reason being the complexity of the layout and the highly variant typography. Nonetheless, in recent years, with the rise of Deep learning, significant progress has been made in historical OCR; in both layout recognition and segmentation, and also in character recognition. The only downside is the limited advancements dedicated to the Arabic language, notably the handwritten text. In this paper, we present an OCR approach that utilizes state-of-theart Deep learning techniques for the Arabic language. We built a custom dataset of obfuscated and noisy images to imitate the noise in historical Arabic documents, with a collection of 30 million images paired with their ground truth. The model utilizes both page segmentation and line segmentation techniques to enhance the resultant transcription. The model is complex enough for transcribing handwritten manuscripts. In addition, the model can detect and transcribe documents that contain Arabic diacritics. The model attained a CER of 0.0727, a WER of 0.0829, and a SER of 0.10. © 2021 IEEE.},
	language = {English},
	urldate = {2021-05-26},
	booktitle = {Int. {Mob}., {Intell}., {Ubiquitous} {Comput}. {Conf}., {MIUCC}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Mostafa, A. and Mohamed, O. and Ashraf, A. and Elbehery, A. and Jamal, S. and Khoriba, G. and Ghoneim, A.S.},
	editor = {{Bahaa-Eldin A.} and {AbdelRaouf A.} and {Shorim N.A.M.} and {Osama Mohamed Rashad R.} and {Elbohy S.E.}},
	year = {2021},
	note = {Journal Abbreviation: Int. Mob., Intell., Ubiquitous Comput. Conf., MIUCC},
	keywords = {Arabic languages, Deep learning, Hand-written text recognition, handwritten text, Handwritten texts, HEMDIG - SCOPUS, Historical documents, Learning techniques, Line segmentation, OCR, Optical character recognition, Optical character recognition (OCR), Page segmentation, SCOPUS, Segmentation, Ubiquitous computing},
	pages = {182--186},
}

@article{zhang_dual_2021,
	title = {Dual {Recognition} {Method} of {Spatial} {Layout} {Fusion} for {Complex} {Architectural} {Plan} {Drawings}},
	volume = {46},
	issn = {16718860 (ISSN)},
	shorttitle = {复杂建筑平面图纸的空间布局融合对偶识别方法},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115889650&doi=10.13203%2fj.whugis20210323&partnerID=40&md5=5c5042bbc96d0a77b62513d0d186ac50},
	doi = {10.13203/j.whugis20210323},
	abstract = {Objectives: The spatial layout of vectorized complex architectural drawings is widely used in 5G base station construction, smart homes, and AR/VR(augmented reality/virtual reality). Methods: To solve the difficulty of identifying the spatial layout of drawings caused by various irregular drawing elements, a high⁃performance fusion recognition method combining raster image and vector representation is proposed. First, the rasterized representation of architectural plan drawings is used to extract the main development direction, and several enclosed spaces are constructed. Then, the vectorized representation of architectural plan drawings is used and according to the directional and adjacency characteristics of space recognition, the half⁃wall structure is innovatively proposed to calculate geometric position and topological relationship. The wall layout can be rebuilt as a whole with high precision according to the principle of space duality. Finally, the proposed method and the traditional methods are analyzed on the vector building plan data set of various wall layouts. Results: The experimental results show that the proposed fusion dual recognition algorithm is usable and effective for various types of building models. Conclusions: It has higher robustness and is less interfered by specific architectural drawing types. © 2021, Editorial Board of Geomatics and Information Science of Wuhan University. All right reserved.},
	language = {Chinese},
	number = {9},
	journal = {Wuhan Daxue Xuebao (Xinxi Kexue Ban)/Geomatics and Information Science of Wuhan University},
	author = {Zhang, H. and Fang, Y. and Liu, M. and Dong, B.},
	year = {2021},
	note = {Publisher: Editorial Board of Medical Journal of Wuhan University},
	keywords = {5G mobile communication systems, algorithm, Architectural design, Architectural plan drawing, Architectural plan drawings, architecture, Augmented reality, Automation, Debris removal, Drawing elements, Half⁃wall structure, HEMDIG - SCOPUS, image processing, Intelligent buildings, Performance, Rasterization, Recognition methods, SCOPUS, Smart homes, spatial analysis, Spatial layout, Spatial layout recognition, Tall buildings, wall, Wall structure, Walls (structural partitions)},
	pages = {1354--1361},
}

@book{jimeno_yepes_icdar_2021,
	series = {16th {International} {Conference} on {Document} {Analysis} and {Recognition}, {ICDAR} 2021},
	title = {{ICDAR} 2021 {Competition} on {Scientific} {Literature} {Parsing}},
	volume = {12824 LNCS},
	isbn = {03029743 (ISSN); 9783030863364 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115292731&doi=10.1007%2f978-3-030-86337-1_40&partnerID=40&md5=492336ed338001dcbaf1ae9ef1d699d5},
	abstract = {Scientific literature contain important information related to cutting-edge innovations in diverse domains. Advances in natural language processing have been driving the fast development in automated information extraction from scientific literature. However, scientific literature is often available in unstructured PDF format. While PDF is great for preserving basic visual elements, such as characters, lines, shapes, etc., on a canvas for presentation to humans, automatic processing of the PDF format by machines presents many challenges. With over 2.5 trillion PDF documents in existence, these issues are prevalent in many other important application domains as well. A critical challenge for automated information extraction from scientific literature is that documents often contain content that is not in natural language, such as figures and tables. Nevertheless, such content usually illustrates key results, messages, or summarizations of the research. To obtain a comprehensive understanding of scientific literature, the automated system must be able to recognize the layout of the documents and parse the non-natural-language content into a machine readable format. Our ICDAR 2021 Scientific Literature Parsing Competition (ICDAR2021-SLP) aims to drive the advances specifically in document understanding. ICDAR2021-SLP leverages the PubLayNet and PubTabNet datasets, which provide hundreds of thousands of training and evaluation examples. In Task A, Document Layout Recognition, submissions with the highest performance combine object detection and specialised solutions for the different categories. In Task B, Table Recognition, top submissions rely on methods to identify table components and post-processing methods to generate the table structure and content. Results from both tasks show an impressive performance and opens the possibility for high performance practical applications. © 2021, Springer Nature Switzerland AG.},
	language = {English},
	urldate = {2021-09-05},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Jimeno Yepes, A. and Zhong, P. and Burdick, D.},
	editor = {{Llados J.} and {Lopresti D.} and {Uchida S.}},
	year = {2021},
	doi = {10.1007/978-3-030-86337-1_40},
	note = {Journal Abbreviation: Lect. Notes Comput. Sci.
Pages: 617
Publication Title: Lect. Notes Comput. Sci.},
	keywords = {Automation, Cutting, Cutting edge innovation, Diverse domains, Document layout understanding, Document layouts, HEMDIG - SCOPUS, ICDAR competition, Information retrieval, Natural language processing systems, Natural languages, Object detection, PDF format, Performance, Scientific literature, SCOPUS, Table recognition},
}

@book{belagavi_docvisor_2021,
	series = {International {Workshops} co-located with the 16th {International} {Conference} on {Document} {Analysis} and {Recognition}, {ICDAR} 2021},
	title = {{DocVisor}: {A} {Multi}-purpose {Web}-{Based} {Interactive} {Visualizer} for {Document} {Image} {Analytics}},
	volume = {12917 LNCS},
	isbn = {03029743 (ISSN); 9783030861582 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115298932&doi=10.1007%2f978-3-030-86159-9_14&partnerID=40&md5=3c66def1a245cb0e3819a7e8ba278e23},
	abstract = {The performance for many document-based problems (OCR, Document Layout Segmentation, etc.) is typically studied in terms of a single aggregate performance measure (Intersection-Over-Union, Character Error Rate, etc.). While useful, the aggregation is a trade-off between instance-level analysis of predictions which may shed better light on a particular approach’s biases and performance characteristics. To enable a systematic understanding of instance-level predictions, we introduce DocVisor - a web-based multi-purpose visualization tool for analyzing the data and predictions related to various document image understanding problems. DocVisor provides support for visualizing data sorted using custom-specified performance metrics and display styles. It also supports the visualization of intermediate outputs (e.g., attention maps, coarse predictions) of the processing pipelines. This paper describes the appealing features of DocVisor and showcases its multi-purpose nature and general utility. We illustrate DocVisor’s functionality for four popular document understanding tasks – document region layout segmentation, tabular data detection, weakly-supervised document region segmentation and optical character recognition. DocVisor is available as a documented public repository for use by the community. © 2021, Springer Nature Switzerland AG.},
	language = {English},
	urldate = {2021-09-05},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Belagavi, K. and Tadimeti, P. and Sarvadevabhatla, R.K.},
	editor = {{Barney Smith E.H.} and {Pal U.}},
	year = {2021},
	doi = {10.1007/978-3-030-86159-9_14},
	note = {Journal Abbreviation: Lect. Notes Comput. Sci.
Pages: 219
Publication Title: Lect. Notes Comput. Sci.},
	keywords = {Aggregate performance, Character error rates, Computer vision, Data visualization, Document images, Document layouts, Document-based, Economic and social effects, Forecasting, HEMDIG - SCOPUS, Multi-purpose, Optical character recognition, Performance, Performance measure, SCOPUS, Visualization, Visualizers, Web based, Websites},
}

@book{cao_towards_2021,
	series = {16th {International} {Conference} on {Document} {Analysis} and {Recognition}, {ICDAR} 2021},
	title = {Towards {Document} {Panoptic} {Segmentation} with {Pinpoint} {Accuracy}: {Method} and {Evaluation}},
	volume = {12822 LNCS},
	isbn = {03029743 (ISSN); 9783030863302 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115333249&doi=10.1007%2f978-3-030-86331-9_1&partnerID=40&md5=824c36f44a417ea8e7a74d93764dc7b1},
	abstract = {In this paper we study the task of document layout recognition for digital documents, requiring that the model should detect the exact physical object region without missing any text or containing any redundant text outside objects. It is the vital step to support high-quality information extraction, table understanding and knowledge base construction over the documents from various vertical domains (e.g. financial, legal, and government fields). Here, we consider digital documents, where characters and graphic elements are given with their exact texts, positions inside document pages, compared with image documents. Towards document layout recognition with pinpoint accuracy, we consider this problem as a document panoptic segmentation task, that each token in the document page must be assigned a class label and an instance id. Considering that two predicted objects may intersect under traditional visual panoptic segmentation method, like Mask R-CNN, however, document objects never intersect because most document pages follow manhattan layout. Therefore, we propose a novel framework, named document panoptic segmentation (DPS) model. It first splits the document page into column regions and groups tokens into line regions, then extracts the textual and visual features, and finally assigns class label and instance id to each line region. Additionally, we propose a novel metric based on the intersection over union (IoU) between the tokens contained in predicted and the ground-truth object, which is more suitable than metric based on the area IoU between predicted and the ground-truth bounding box. Finally, the empirical experiments based on PubLayNet, ArXiv and Financial datasets show that the proposed DPS model obtains 0.8833, 0.9205 and 0.8530 mAP scores on three datasets. The proposed model obtains great improvement on mAP score compared with Faster R-CNN and Mask R-CNN models. © 2021, Springer Nature Switzerland AG.},
	language = {English},
	urldate = {2021-09-05},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Cao, R. and Li, H. and Zhou, G. and Luo, P.},
	editor = {{Llados J.} and {Lopresti D.} and {Uchida S.}},
	year = {2021},
	doi = {10.1007/978-3-030-86331-9_1},
	note = {Journal Abbreviation: Lect. Notes Comput. Sci.
Pages: 18
Publication Title: Lect. Notes Comput. Sci.},
	keywords = {Character recognition, Class labels, Digital Documents, Document layouts, Ground truth, HEMDIG - SCOPUS, High quality, Knowledge based systems, Knowledge-base construction, Object detection, Object region, Physical objects, SCOPUS, Segmentation models, Table understanding},
}

@inproceedings{lu_self-supervised_2022,
	title = {Self-{Supervised} {Road} {Layout} {Parsing} with {Graph} {Auto}-{Encoding}},
	volume = {2022-June},
	isbn = {9781665488211 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135377781&doi=10.1109%2fIV51971.2022.9827287&partnerID=40&md5=ecfd22d28f85636b2e179839e48793d5},
	doi = {10.1109/IV51971.2022.9827287},
	abstract = {Aiming for higher-level scene understanding, this work presents a neural network approach that takes a road-layout map in bird's-eye-view as input, and predicts a human-interpretable graph that represents the road's topological layout. Our approach elevates the understanding of road layouts from pixel level to the level of graphs. To achieve this goal, an image-graph-image auto-encoder is utilized. The network is designed to learn to regress the graph representation at its auto-encoder bottleneck. This learning is self-supervised by an image reconstruction loss, without needing any external manual annotations. We create a synthetic dataset containing common road layout patterns and use it for training of the auto-encoder in addition to the real-world Argoverse dataset. By using this additional synthetic dataset, which conceptually captures human knowledge of road layouts and makes this available to the network for training, we are able to stabilize and further improve the performance of topological road layout understanding on the real-world Argoverse dataset. The evaluation shows that our approach exhibits comparable performance to a strong fully-supervised baseline. © 2022 IEEE.},
	language = {English},
	urldate = {2022-06-05},
	booktitle = {{IEEE} {Intell} {Veh} {Symp} {Proc}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Lu, C. and Dubbelman, G.},
	year = {2022},
	note = {Journal Abbreviation: IEEE Intell Veh Symp Proc},
	keywords = {Auto encoders, Bird's eye view, Computer vision, Encoding (symbols), Encodings, HEMDIG - SCOPUS, Image reconstruction, Neural-networks, Performance, Pixel level, Real-world, Road layouts, Roads and streets, Scene understanding, SCOPUS, Signal encoding, Synthetic datasets, Topology},
	pages = {842--851},
}

@book{ravichandra_deep_2022,
	series = {3rd {International} {Conference} on {Advances} in {Distributed} {Computing} and {Machine} {Learning}, {ICADCML} 2022},
	title = {Deep {Learning} {Based} {Document} {Layout} {Analysis} on {Historical} {Documents}},
	volume = {427},
	isbn = {23673370 (ISSN); 9789811910173 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136083929&doi=10.1007%2f978-981-19-1018-0_23&partnerID=40&md5=b764df8df4cf91cb9c8c72a9b8fb0301},
	abstract = {Mining historical documents provide important information that assists scientists and researchers to gain new insights into their domain. Document Layout Analysis is the first step of the automatic capturing of information from documents. The layout recognition supports establishing relationships between components in the document, which allows extraction of information. A method to visually segment critical regions of Historical handwritten documents using an object detection technique augmented with contextual features are presented in this paper. Layout analysis of Historical documents is complicated due to complex document layout and degradation of documents due to age. The object-detection technique YOLOv3 has been adopted for document layout analysis by extracting the features at different levels by down-sampling the input and combining it using Feature Pyramid Network to detect smaller regions to improve the region detection performance. The experimental results on two distinct datasets that demonstrate the method’s potential and competitive results with respect to state-of-the-art approaches have been presented. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
	language = {English},
	urldate = {2022-01-15},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Ravichandra, S. and Siva Sathya, S. and Lourdu Marie Sophie, S.},
	editor = {{Rout R.R.} and {Ghosh S.K.} and {Jana P.K.} and {Tripathy A.K.} and {Sahoo J.P.} and {Li K.-C.}},
	year = {2022},
	doi = {10.1007/978-981-19-1018-0_23},
	note = {Journal Abbreviation: Lect. Notes Networks Syst.
Pages: 281
Publication Title: Lect. Notes Networks Syst.},
	keywords = {Deep learning, Document layout analysis, HEMDIG - SCOPUS, Object detection, SCOPUS},
}

@inproceedings{shekhar_opad_2022,
	title = {{OPAD}: {An} {Optimized} {Policy}-based {Active} {Learning} {Framework} for {Document} {Content} {Analysis}},
	volume = {2022-June},
	isbn = {21607508 (ISSN); 9781665487399 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137819057&doi=10.1109%2fCVPRW56347.2022.00320&partnerID=40&md5=4eceaffff6e78914a6437693887124d5},
	doi = {10.1109/CVPRW56347.2022.00320},
	abstract = {Documents are central to many business systems, and include forms, reports, contracts, invoices or purchase orders. The information in documents is typically in natural language, but can be organized in various layouts and formats. There have been recent spurt of interest in understanding document content with novel deep learning architectures. However, document understanding tasks need dense information annotations, which are costly to scale and generalize. Several active learning techniques have been proposed to reduce the overall budget of annotation while maintaining the performance of the underlying deep learning model. In this paper, we propose OPAD, a novel framework using reinforcement policy for active learning in content detection tasks for documents. The proposed framework learns the acquisition function to decide the samples to be selected while optimizing performance metrics that the tasks typically have. Furthermore, we extend to weak labelling scenarios to further reduce the cost of annotation significantly. We propose novel rewards to account for class imbalance and user feedback in the annotation interface, to improve the active learning method. We show superior performance of the proposed OPAD framework for active learning for various tasks related to document understanding like layout parsing, object detection and named entity recognition. Ablation studies for human feedback and class imbalance rewards are presented, along with a comparison of annotation times for different approaches. © 2022 IEEE.},
	language = {English},
	urldate = {2022-06-19},
	booktitle = {{IEEE} {Comput}. {Soc}. {Conf}. {Comput}. {Vis}. {Pattern} {Recogn}. {Workshops}},
	publisher = {IEEE Computer Society},
	author = {Shekhar, S. and Prakash Reddy Guda, B. and Chaubey, A. and Jindal, I. and Jain, A.},
	year = {2022},
	note = {Journal Abbreviation: IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn. Workshops},
	keywords = {Active Learning, Budget control, Business systems, Class imbalance, Content analysis, Deep learning, Document contents, Document understanding, HEMDIG - SCOPUS, Learning frameworks, Learning systems, Object detection, Optimized policies, Performance, Policy-based, Purchasing, SCOPUS},
	pages = {2825--2835},
}

@article{raman_synthetic_2022,
	title = {Synthetic document generator for annotation-free layout recognition},
	volume = {128},
	issn = {00313203 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127197928&doi=10.1016%2fj.patcog.2022.108660&partnerID=40&md5=fe083de6fdf8e607e20729011e7b4d5f},
	doi = {10.1016/j.patcog.2022.108660},
	abstract = {Analyzing the layout of a document to identify headers, sections, tables, figures etc. is critical to understanding its content. Deep learning based approaches for detecting the layout structure of document images have been promising. However, these methods require a large number of annotated examples during training, which are both expensive and time consuming to obtain. We describe here a synthetic document generator that automatically produces realistic documents with labels for spatial positions, extents and categories of the layout elements. The proposed generative process treats every physical component of a document as a random variable and models their intrinsic dependencies using a Bayesian Network graph. Our hierarchical formulation using stochastic templates allow parameter sharing between documents for retaining broad themes and yet the distributional characteristics produces visually unique samples, thereby capturing complex and diverse layouts. We empirically illustrate that a deep layout detection model trained purely on the synthetic documents can match the performance of a model that uses real documents. © 2022 Elsevier Ltd},
	language = {English},
	journal = {Pattern Recognition},
	author = {Raman, N. and Shah, S. and Veloso, M.},
	year = {2022},
	note = {Publisher: Elsevier Ltd},
	keywords = {Bayesia n networks, Bayesian network, Bayesian networks, Deep learning, Document images, Free layouts, Generative process, HEMDIG - SCOPUS, Image annotation, Layout analysis, Layout structure, Learning-based approach, Physical components, SCOPUS, Spatial positions, Stochastic systems, Synthetic image generation},
}

@inproceedings{pfitzmann_doclaynet_2022,
	title = {{DocLayNet}: {A} {Large} {Human}-{Annotated} {Dataset} for {Document}-{Layout} {Segmentation}},
	isbn = {9781450393850 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132251721&doi=10.1145%2f3534678.3539043&partnerID=40&md5=a69dc5f3ba95642bf94ee16e77b62f3c},
	doi = {10.1145/3534678.3539043},
	abstract = {Accurate document layout analysis is a key requirement for high-quality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we presentDocLayNet, a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10\% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNet-trained models are more robust and thus the preferred choice for general-purpose document-layout analysis. © 2022 Owner/Author.},
	language = {English},
	urldate = {2022-08-14},
	booktitle = {Proc. {ACM} {SIGKDD} {Int}. {Conf}. {Knowl}. {Discov}. {Data} {Min}.},
	publisher = {Association for Computing Machinery},
	author = {Pfitzmann, B. and Auer, C. and Dolfi, M. and Nassar, A.S. and Staar, P.},
	collaborator = {{ACM SIGKDD; ACM SIGMOD}},
	year = {2022},
	note = {Journal Abbreviation: Proc. ACM SIGKDD Int. Conf. Knowl. Discov. Data Min.},
	keywords = {Annotated datasets, computer vision, Computer vision, Deep learning, Document layout analysis, Document layouts, Document management, document management and text processing, Document management and text processing, HEMDIG - SCOPUS, High quality, Information retrieval systems, Information services, Large dataset, Learning systems, machine learning, Machine-learning, neural networks, Neural networks, Neural-networks, object detection, Object detection, Object recognition, Objects detection, Quality control, SCOPUS, Text processing, Text-processing},
	pages = {3743--3751},
}

@inproceedings{yao_complex_2021,
	title = {Complex document layout segmentation based on an encoder-decoder architecture},
	volume = {2010},
	isbn = {17426588 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115050254&doi=10.1088%2f1742-6596%2f2010%2f1%2f012024&partnerID=40&md5=f3d93c8da78cb97bd15c2d80fb14124d},
	doi = {10.1088/1742-6596/2010/1/012024},
	abstract = {In our work, we propose an end-to-end encoder-decoder network for complex document layout segmentation. The proposed multi-scale feature extraction network with two parallel branches is applied to further process the feature maps, where one branch enriches the multi-scale information of the feature maps by building feature pyramids, another branch is introduced to capture the dependencies between different locations and integrate long-range context information. Moreover, we merge the outputs of the two branches to enhance the feature representation so as to further improve segmentation accuracy. Experimental results on datasets of PubLayNet and DSSE-200 demonstrate the effectiveness of our proposed method, which yields pixel-wise accuracy of above 99\%. © Content from this work may be used under the terms of the Creative Commons Attribution 3.0 Licence.},
	language = {English},
	urldate = {2021-07-30},
	booktitle = {J. {Phys}. {Conf}. {Ser}.},
	publisher = {IOP Publishing Ltd},
	author = {Yao, J. and Huang, L.},
	year = {2021},
	note = {Issue: 1
Journal Abbreviation: J. Phys. Conf. Ser.},
	keywords = {Complex documents, Complex networks, Context information, Decoding, Encoder-decoder architecture, Feature representation, HEMDIG - SCOPUS, Multi-scale features, Multi-scale informations, Parallel branches, SCOPUS, Segmentation accuracy, Signal encoding},
}

@article{elanwar_extracting_2021,
	title = {Extracting text from scanned {Arabic} books: a large-scale benchmark dataset and a fine-tuned {Faster}-{R}-{CNN} model},
	volume = {24},
	issn = {14332833 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117632510&doi=10.1007%2fs10032-021-00382-4&partnerID=40&md5=0300f16ad30c3d96f79b825ffb3032bd},
	doi = {10.1007/s10032-021-00382-4},
	abstract = {Datasets of documents in Arabic are urgently needed to promote computer vision and natural language processing research that addresses the specifics of the language. Unfortunately, publicly available Arabic datasets are limited in size and restricted to certain document domains. This paper presents the release of BE-Arabic-9K, a dataset of more than 9000 high-quality scanned images from over 700 Arabic books. Among these, 1500 images have been manually segmented into regions and labeled by their functionality. BE-Arabic-9K includes book pages with a wide variety of complex layouts and page contents, making it suitable for various document layout analysis and text recognition research tasks. The paper also presents a page layout segmentation and text extraction baseline model based on fine-tuned Faster R-CNN structure (FFRA). This baseline model yields cross-validation results with an average accuracy of 99.4\% and F1 score of 99.1\% for text versus non-text block classification on 1500 annotated images of BE-Arabic-9K. These results are remarkably better than those of the state-of-the-art Arabic book page segmentation system ECDP. FFRA also outperforms three other prior systems when tested on a competition benchmark dataset, making it an outstanding baseline model to challenge. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.},
	language = {English},
	number = {4},
	journal = {International Journal on Document Analysis and Recognition},
	author = {Elanwar, R. and Qin, W. and Betke, M. and Wijaya, D.},
	year = {2021},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Benchmark datasets, Character recognition, Convolutional neural networks, Cross validation, Document layout analysis, HEMDIG - SCOPUS, Large dataset, NAtural language processing, Natural language processing systems, Page segmentation, SCOPUS, State of the art, Text block classification, Text processing, Text recognition},
	pages = {349--362},
}

@article{zhang_3d_2022,
	title = {{3D} {Layout} {Estimation} via {Weakly} {Supervised} {Learning} of {Plane} {Parameters} from {2D} {Segmentation}},
	volume = {31},
	issn = {10577149 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122074272&doi=10.1109%2fTIP.2021.3131025&partnerID=40&md5=594f7f3f1c04d419291928400b1b1d1d},
	doi = {10.1109/TIP.2021.3131025},
	abstract = {The task of 3D layout estimation in an indoor scene is to predict the holistic 3D structural information of the scene from an RGB image. It is costly to obtain the ground truth 3D layout, and this issue severely restricts the learning based 3D layout estimation approaches. In this paper, we present a novel weakly supervised learning framework that is able to learn the 3D layout effectively with 2D layout segmentation mask as supervision. We employ a deep neural network to predict the plane parameters and camera intrinsic parameters in the image. Based on the predicted plane instances, the 3D layout as well as the corresponding depth map and 2D segmentation can be generated. The key objectives for learning meaningful plane parameters are the label consistency of layout segmentation and depth consistency of border pixels from adjacent planes, with which the ground truth 2D layout segmentation is able to supervise the learning of the 3D layout. We further incorporate 3D geometric reasoning and prior knowledge in the learning process to ensure that the learned 3D layout is realistic and reasonable. Experimental results show that our method can produce accurate 3D layout estimates by weakly supervised learning. © 1992-2012 IEEE.},
	language = {English},
	journal = {IEEE Transactions on Image Processing},
	author = {Zhang, W. and Zhang, Y. and Song, R. and Liu, Y. and Zhang, W.},
	year = {2022},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {2D segmentation, 3d layout estimation, 3D layout estimation, 3D layouts, Cameras, Cognition, Computer vision, Deep neural networks, HEMDIG - SCOPUS, Image segmentation, Layout, monocular stereo vision, Monocular stereo vision, Parameter estimation, Plane parameter, plane parameters, SCOPUS, Solid modelling, Stereo image processing, Stereo vision, Supervised learning, Three dimensional computer graphics, Three dimensional displays, Three-dimensional display, weakly supervised learning, Weakly supervised learning},
	pages = {868--879},
}

@book{kawoosa_ncert5k-iitrpr_2022,
	series = {15th {IAPR} {International} {Workshop} on {Document} {Analysis} {Systems}, {DAS} 2022},
	title = {{NCERT5K}-{IITRPR}: {A} {Benchmark} {Dataset} for {Non}-textual {Component} {Detection} in {School} {Books}},
	volume = {13237 LNCS},
	isbn = {03029743 (ISSN); 9783031065545 (ISBN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131138195&doi=10.1007%2f978-3-031-06555-2_31&partnerID=40&md5=3983dea2096b10ea25fb066541532636},
	abstract = {The STEM subjects books heavily rely on Non-textual Components (NTCs) such as charts, geometric figures, and equations to demonstrate the underlying complex concepts. However, the accessibility of STEM subjects for Blind and Visually Impaired (BVIP) students is a primary concern, especially in developing countries such as India. BVIP uses assistive technologies (ATs) like optical character recognition (OCR) and screen readers for reading/writing purposes. While parsing, such ATs skip NTCs and mainly rely on alternative texts to describe these visualization components. Integration of effective and automated document layout parsing frameworks for extracting data from non-textual components of digital documents are required with existing ATs for making these NTCs accessible. Although, the primary concern is the absence of an adequately annotated textbook dataset on which layout recognition and other vision-based frameworks can be trained. To improve the accessibility and automated parsing of such books, we introduce a new NCERT5K-IITRPR dataset of National Council of Educational Research and Training (NCERT) school books. Twenty-three annotated books covering more than 5000 pages from the eighth to twelve standards have been considered. The NCERT label objects are structurally different from the existing document layout analysis (DLA) dataset objects and contain diverse label categories. We benchmark the NCERT5K-IITRPR dataset with multiple object detection methods. A systematic analysis of detectors shows the label complexity and fine-tuning necessity of the NCERT5K-IITRPR dataset. We hope that our dataset helps in improving the accessibility of NCERT Books for BVIP students. © 2022, Springer Nature Switzerland AG.},
	language = {English},
	urldate = {2022-05-22},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Kawoosa, H.S. and Singh, M. and Joshi, M.M. and Goyal, P.},
	editor = {{Uchida S.} and {Barney E.} and {Eglin V.}},
	year = {2022},
	doi = {10.1007/978-3-031-06555-2_31},
	note = {Journal Abbreviation: Lect. Notes Comput. Sci.
Pages: 475
Publication Title: Lect. Notes Comput. Sci.},
	keywords = {Assistive, Assistive reading, Blind and visually impaired, Developing countries, Document layout analysis, Educational research, Graphical object detection, HEMDIG - SCOPUS, National council of educational research and training book, National council of educational research and training dataset, NCERT books, NCERT dataset, Object detection, Object recognition, Optical character recognition, SCOPUS, Training dataset},
}

@article{jain_convolutional_2022,
	title = {Convolutional {Neural} {Network} {Based} {Intelligent} {Advertisement} {Search} {Framework} for {Online} {English} {Newspapers}},
	volume = {16},
	issn = {18722121 (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132580971&doi=10.2174%2f1872212115666210715163919&partnerID=40&md5=af86dcb5f330e867922f41b42fe69d73},
	doi = {10.2174/1872212115666210715163919},
	abstract = {Background: Instant access to desired information is the key element for building an intelligent environment creating value for people and steering towards society 5.0. Online newspapers are one such example which provide instant access to information anywhere and anytime on our mobiles, tablets, laptops, desktops, etc. But when it comes to searching for a specific advertisement in newspapers, online newspapers do not provide easy advertisement search options. Also, there are no specialized search portals which can provide for keyword-based advertisement search across multiple online newspapers. As a result, to find a specific advertisement in multiple newspapers, a sequential manual search is required across a range of online newspapers. Objective: This research paper proposes a keyword-based advertisement search framework to provide an instant access to the relevant advertisements from online English newspapers in a category of reader’s choice. Methods: First, an image extraction algorithm is proposed which can identify and extract the images from online newspapers without using any rules on advertisement placement and/or size. It is followed by a proposed deep learning Convolutional Neural Network (CNN) model named ‘Adv\_Recognizer’ which is used to separate the advertisement images from non-advertisement images. Another CNN Model, ‘Adv\_Classifier’, is proposed, which classifies the advertisement images into four pre-defined categories. Finally, Optical Character Recognition (OCR) technique is used to perform keyword-based advertisement searches in various categories across multiple newspapers. Results: The proposed image extraction algorithm can easily extract all types of well-bounded images from different online newspapers and this algorithm is used to create ‘English newspaper image dataset’ of 11,000 images, including advertisements and non-advertisements. The proposed ‘Adv\_Recognizer’ model separates advertisement and non-advertisement images with an accuracy of around 97.8\%. and the proposed ‘Adv\_Classifier’ model classifies the advertisements in four predefined categories exhibiting an accuracy of around 73.5\%. Conclusion: The proposed framework will help newspaper readers in performing exhaustive advertisement searches across a range of online English newspapers in a category of their own interest. It will also help in carrying out advertisement analysis and studies. © 2022 Bentham Science Publishers.},
	language = {English},
	number = {4},
	journal = {Recent Patents on Engineering},
	author = {Jain, P. and Taneja, K. and Taneja, H.},
	year = {2022},
	note = {Publisher: Bentham Science Publishers},
	keywords = {Advertisement image classification, Convolution, Convolutional neural network, Convolutional neural networks, convolutional neural networks (CNN), Deep learning, Extraction, HEMDIG - SCOPUS, Image classification, Image segmentation, Images classification, Neural network models, newspaper advertisements, Newspaper advertisements, newspaper layout segmentation, Newspaper layout segmentation, Newsprint, Online newspaper, Optical character recognition, optical character recognition (OCR), Residual network, residual networks (ResNet), SCOPUS, transfer learning, Transfer learning},
}

@inproceedings{bredthauer_64_1990,
	title = {64 output 1024×1024 pixel imager for high frame rate applications},
	volume = {1155},
	isbn = {0277786X (ISSN)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889547691&doi=10.1117%2f12.962407&partnerID=40&md5=6704c7eb8c716d9b024f81f21d1b9271},
	doi = {10.1117/12.962407},
	abstract = {A 1024x1024 imager suitable for high frame rate applications is described. Sixty-four parallel outputs are incorporated to achieve high resolution fast framing with reduced individual port data rate. Layout segmentation and a separate masking step provides for operation as either a frame store or full frame imager. This paper discusses the architecture, fabrication and packaging of the imager. © 1990 SPIE.},
	language = {English},
	booktitle = {Proc {SPIE} {Int} {Soc} {Opt} {Eng}},
	author = {Bredthauer, R.A.},
	year = {1990},
	note = {Journal Abbreviation: Proc SPIE Int Soc Opt Eng},
	keywords = {Data rates, Frame stores, HEMDIG - SCOPUS, High frame rate, High resolution, High speed photography, Parallel output, SCOPUS, Video recording},
	pages = {89--93},
}
